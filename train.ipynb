{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT and TOKENIZER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mv/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model class is      : <class 'transformers.models.bert.modeling_bert.BertModel'>\n",
      "\n",
      "model class is      : <class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel,AutoTokenizer\n",
    "\n",
    "auto_model = AutoModel.from_pretrained(\"bert-base-cased\",output_hidden_states=True)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nmodel class is      : {type(auto_model)}\")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "print(f\"\\nmodel class is      : {type(tokenizer)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Role Labelling Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import random\n",
    "from typing import Dict\n",
    "\n",
    "class SRL(Dataset):\n",
    " \n",
    "    def __init__(self,language,path,args_roles = None,pos_list = None) -> None:\n",
    "\n",
    "        self.path_root = 'data'\n",
    "        self.load_data(language,path)\n",
    "        if args_roles is None :\n",
    "            self.args_roles,self.list_broken_id = self.list_arg_roles()\n",
    "        else : \n",
    "            self.args_roles = args_roles\n",
    "            _,self.list_broken_id = self.list_arg_roles()\n",
    "        \n",
    "\n",
    "        if pos_list is None :\n",
    "            self.pos_list,_ = self.list_pos()\n",
    "            self.pos_list.append(\"Nothing\")\n",
    "        else : \n",
    "            self.pos_list = pos_list\n",
    "        \n",
    "\n",
    "\n",
    "        self.predicate_dis,_ = self.list_predicate_roles()\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(self.pos_list)\n",
    "\n",
    "        self.predicate_dis.append(\"Nothing\")\n",
    "\n",
    "    def load_data(self,language,mode):\n",
    "        \n",
    "        mode = mode+\".json\"\n",
    "        path = os.path.join(self.path_root,language,mode)\n",
    "        data_file = open(path)\n",
    "       \n",
    "        data_ = json.load(data_file)\n",
    "\n",
    "        list_data = []\n",
    "\n",
    "        for data in data_:\n",
    "            list_data.append(data_[data])\n",
    "        \n",
    "\n",
    "        self.data = list_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, id : int):\n",
    "\n",
    "        flag = False\n",
    "        if id in self.list_broken_id :\n",
    "            flag = True\n",
    "            while flag == True:\n",
    "\n",
    "                rand_id = random.randint(0, len(self.data)-1)\n",
    "                \n",
    "                if rand_id in self.list_broken_id :\n",
    "                    pass\n",
    "                else :\n",
    "                    flag = False\n",
    "                    id = rand_id        \n",
    "\n",
    "\n",
    "        data = self.pre_processing(self.data[id])\n",
    "        data = self.processig(data)\n",
    "        return data\n",
    "        \n",
    "    def pre_processing(self, data:dict):\n",
    "        data_list = []\n",
    "        for role in data[\"roles\"]:\n",
    "            dictionary = dict()\n",
    "            dictionary[\"words\"] = data[\"words\"]\n",
    "            dictionary[\"role\"] = data[\"roles\"][role]\n",
    "            dictionary[\"pre_idx\"] = role\n",
    "            dictionary[\"pos_tags\"] = data[\"pos_tags\"]\n",
    "            dictionary[\"predicate_meaning\"] = data[\"predicates\"]\n",
    "            data_list.append(dictionary)    \n",
    "        return data_list\n",
    "    \n",
    "    def processig(self,data_list:list):\n",
    "        \n",
    "        for dictionary in data_list:\n",
    "\n",
    "            #dictionary[\"words\"] = data[\"words\"]\n",
    "            dictionary[\"gt_arg_identification\"] = self.arg_id(dictionary[\"role\"])\n",
    "            dictionary[\"gt_arg_classification\"] = self.arg_class(dictionary[\"role\"])\n",
    "            dictionary[\"pos_idx\"] = self.pos_idx(dictionary[\"pos_tags\"])\n",
    "            dictionary[\"predicate_meaning_idx\"] = self.predicate_meaning_idx(dictionary[\"predicate_meaning\"])\n",
    "        \n",
    "        return data_list\n",
    "   \n",
    "    def list_arg_roles(self):\n",
    "        list_roles = []\n",
    "        list_broken_id = []\n",
    "        for i,element in enumerate(self.data):\n",
    "            flag = True\n",
    "            try : roles = element[\"roles\"]\n",
    "            except : flag = False\n",
    "            if flag :\n",
    "                for e in roles:\n",
    "                    sentence = element[\"roles\"][e]\n",
    "\n",
    "                    for word in sentence:\n",
    "                        \n",
    "                        list_roles.append(word)\n",
    "                list_roles = list(set(list_roles))\n",
    "            else : \n",
    "                list_broken_id.append(i)\n",
    "        return list_roles,list_broken_id\n",
    "\n",
    "    def list_predicate_roles(self):\n",
    "        list_predicate_roles = []\n",
    "        list_broken_id = []\n",
    "        for i,element in enumerate(self.data):\n",
    "            flag = True\n",
    "            try : predicates = element[\"predicates\"]\n",
    "            except : flag = False\n",
    "            if flag :\n",
    "                for pre in predicates:\n",
    "                    list_predicate_roles.append(pre)\n",
    "                list_predicate_roles = list(set(list_predicate_roles))\n",
    "            else : \n",
    "                list_broken_id.append(i)\n",
    "        return list_predicate_roles,list_broken_id\n",
    "\n",
    "    def list_pos(self):\n",
    "        list_pos = []\n",
    "        list_broken_id = []\n",
    "        for i,element in enumerate(self.data):\n",
    "            flag = True\n",
    "            try : pos = element[\"pos_tags\"]\n",
    "            except : flag = False\n",
    "            if flag :\n",
    "                for e in pos:\n",
    "                    list_pos.append(e)\n",
    "                list_pos = list(set(list_pos))\n",
    "            else : \n",
    "                list_broken_id.append(i)\n",
    "        return list_pos,list_broken_id\n",
    "  \n",
    "    def arg_class(self,role:list):\n",
    "        list_idxs = []\n",
    "        for element in role:\n",
    "            list_idxs.append(self.args_roles.index(element))\n",
    "        \n",
    "\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "\n",
    "    def arg_id(self,role:dict):\n",
    "        list_idxs = []\n",
    "        for element in role:\n",
    "            if element == \"_\":\n",
    "                list_idxs.append(0)\n",
    "            else :\n",
    "                list_idxs.append(1)\n",
    "\n",
    "        \n",
    "\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "\n",
    "    def pos_idx(self,pos_tags:dict):\n",
    "        list_idxs = []\n",
    "        list_idxs.append(self.pos_list.index(\"Nothing\"))\n",
    "\n",
    "        for element in pos_tags:\n",
    "            list_idxs.append(self.pos_list.index(element))\n",
    "        \n",
    "        list_idxs.append(self.pos_list.index(\"Nothing\"))\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "    \n",
    "    def predicate_meaning_idx(self,predicate_meaning_tags:dict):\n",
    "        list_idxs = []\n",
    "        list_idxs.append(self.predicate_dis.index(\"Nothing\"))\n",
    "\n",
    "        for element in predicate_meaning_tags:\n",
    "            list_idxs.append(self.predicate_dis.index(element))\n",
    "        \n",
    "        list_idxs.append(self.predicate_dis.index(\"Nothing\"))\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64) \n",
    "\n",
    "    \n",
    "# here we define our collate function\n",
    "def collate_fn(batch) -> Dict[str, torch.Tensor]:\n",
    "    #print(batch)\n",
    "    input = dict() \n",
    "    batch_sentence = [] \n",
    "    #print(len(batch))\n",
    "    for period in batch:\n",
    "        for sentence in period :\n",
    "        \n",
    "            #print(len(sentence[0][\"words\"]))\n",
    "            pre_idx = int(sentence[\"pre_idx\"])\n",
    "            \n",
    "\n",
    "            predicate = sentence[\"words\"][pre_idx]\n",
    "\n",
    "            text = \" \".join(sentence[\"words\"])\n",
    "            tokens: list[str] = text.split()\n",
    "            predicate: list[str] = predicate.split()\n",
    "\n",
    "            #text = sentence[0][\"words\"]\n",
    "            \n",
    "            t = (tokens,predicate)\n",
    "\n",
    "            batch_sentence.append(t)\n",
    "            #print(batch_sentence)\n",
    "\n",
    "    batch_output = tokenizer.batch_encode_plus(batch_sentence,padding=True,is_split_into_words=True, truncation=True,return_offsets_mapping=True, return_tensors=\"pt\")\n",
    "    #print(batch_output.keys())\n",
    "\n",
    "\n",
    "    gt = dict()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    for period in batch:\n",
    "\n",
    "        list_positional_predicate_encoding = []\n",
    "        list_arg_gt = []\n",
    "        list_predicate_index = [] \n",
    "        list_pos_index = [] \n",
    "        list_predicate_meaning_index = []\n",
    "\n",
    "        for sentence in period:\n",
    "            #positional_encoding\n",
    "            #+2 per il CLS iniziale ad SEP finale\n",
    "            sentence_words_lenght =  len(sentence[\"words\"])\n",
    "            positional_predicate_encoding = torch.zeros(1,sentence_words_lenght+2)\n",
    "            #+1 per il CLS iniziale\n",
    "            pre_idx = int(sentence[\"pre_idx\"])\n",
    "            positional_predicate_encoding[:,pre_idx+1] = 1\n",
    "            list_positional_predicate_encoding.append(positional_predicate_encoding)\n",
    "            #print(\"positional_prefix_encoding\",positional_predicate_encoding)\n",
    "            list_predicate_index.append(pre_idx)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            pos = torch.unsqueeze(sentence[\"pos_idx\"],dim = 0)\n",
    "            list_pos_index.append(pos)\n",
    "            predicate_meaning_idxs = torch.unsqueeze(sentence[\"predicate_meaning_idx\"],dim = 0)\n",
    "            list_predicate_meaning_index.append(predicate_meaning_idxs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #note CLS and SEP are discharder after Bi-LSTM, the Classifier takes in input only wokrds hidden state embedding\n",
    "            arg_gt = torch.unsqueeze(sentence[\"gt_arg_classification\"],dim = 0)\n",
    "            list_arg_gt.append(arg_gt)\n",
    "        \n",
    "\n",
    "    list_arg_gt = torch.cat(list_arg_gt,dim = 0)\n",
    "    list_pos_index = torch.cat(list_pos_index,dim = 0)\n",
    "    list_predicate_meaning_index = torch.cat(list_predicate_meaning_index,dim = 0)\n",
    "    list_positional_predicate_encoding = torch.cat(list_positional_predicate_encoding,dim = 0)\n",
    "    gt[\"arg_gt\"] = list_arg_gt\n",
    "    input[\"predicate_index\"] = list_predicate_index\n",
    "    input[\"pos_index\"] = list_pos_index.long()\n",
    "    input[\"predicate_meaning_idx\"] = list_predicate_meaning_index.long()\n",
    "    offset = batch_output.pop(\"offset_mapping\")\n",
    "    input[\"BERT_input\"] = batch_output\n",
    "    input[\"positional_encoding\"] = list_positional_predicate_encoding.long()\n",
    "    input[\"offset_mapping\"] = offset\n",
    "    input[\"gt\"] = gt\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    return input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SCONJ', 'ADV', 'PUNCT', 'NOUN', 'DET', 'NUM', 'PART', 'ADP', 'CCONJ', 'SYM', 'INTJ', 'PRON', 'X', 'ADJ', 'AUX', 'PROPN', 'VERB', 'Nothing']\n",
      "['SCONJ', 'ADV', 'PUNCT', 'NOUN', 'DET', 'NUM', 'PART', 'ADP', 'CCONJ', 'SYM', 'INTJ', 'PRON', 'X', 'ADJ', 'AUX', 'PROPN', 'VERB', 'Nothing']\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SRL(\"EN\",\"train\")\n",
    "#same mapping should be used in both the dataset\n",
    "dev_dataset = SRL(\"EN\",\"dev\",train_dataset.args_roles,train_dataset.pos_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "embeddings = dict()\n",
    "\n",
    "embeddings[\"predicate_flag_embedding_output_dim\"] = 32\n",
    "#defined in initial exploration of the dataset\n",
    "embeddings[\"pos_embedding_input_dim\"] = 0\n",
    "embeddings[\"pos_embedding_output_dim\"] = 100\n",
    "#-------------------------------------------------\n",
    "embeddings[\"predicate_embedding_input_dim\"] = 0\n",
    "embeddings[\"predicate_embedding_output_dim\"] = False\n",
    "#defined in initial exploration of the dataset\n",
    "n_classes = 0\n",
    "\n",
    "\n",
    "\n",
    "bilstm = dict()\n",
    "bilstm[\"n_layers\"] = 2\n",
    "bilstm[\"output_dim\"] = 50\n",
    "dropouts = [0.4,0.3,0.3]\n",
    "\n",
    "language_portable = True\n",
    "predicate_meaning = True\n",
    "pos = True\n",
    "\n",
    "cfg = dict()\n",
    "cfg[\"embeddings\"] = embeddings\n",
    "cfg[\"n_classes\"] = n_classes\n",
    "cfg[\"bilstm\"] = bilstm\n",
    "cfg[\"language_portable\"] = language_portable\n",
    "cfg[\"dropouts\"] = dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arg_Classifier(\n",
      "  (bi_lstm_portable): LSTM(132, 50, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "  (embedding_predicate_flag): Embedding(2, 32, max_norm=True)\n",
      "  (embedding_predicate): Embedding(304, False, max_norm=True)\n",
      "  (embedding_pos): Embedding(18, 100, max_norm=True)\n",
      "  (bi_lstm): LSTM(900, 50, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "  (dropout_language_constraint): Dropout(p=0.6, inplace=False)\n",
      "  (dropout_in_classifier): Dropout(p=0.4, inplace=False)\n",
      "  (Relu): ReLU()\n",
      "  (Sigmoid): Sigmoid()\n",
      "  (linear0): Linear(in_features=300, out_features=675, bias=True)\n",
      "  (linear1): Linear(in_features=675, out_features=135, bias=True)\n",
      "  (linear2): Linear(in_features=135, out_features=27, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#from mmcv import Config\n",
    "from hw2.stud.arg import Arg_Classifier, Arg_Classifier_from_paper\n",
    "\n",
    "\n",
    "\n",
    "#cfg = Config.fromfile('/home/francesco/Desktop/nlp2022-hw2-main/hw2/stud/configs/model.py')\n",
    "\n",
    "cfg[\"embeddings\"][\"pos_embedding_input_dim\"] = len(train_dataset.pos_list)\n",
    "cfg[\"embeddings\"][\"predicate_embedding_input_dim\"] = len(train_dataset.predicate_dis)\n",
    "cfg[\"n_classes\"] = len(train_dataset.args_roles)\n",
    "\n",
    "\n",
    "model = Arg_Classifier(\"EN\",cfg).cuda()\n",
    "#model = Arg_Classifier_from_paper(\"EN\",cfg).cuda()\n",
    "print(model)\n",
    "\n",
    "automodel = auto_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def metrics(gold,pred):\n",
    "    true_positives, false_positives, false_negatives = 0, 0, 0\n",
    "    null_tag = \"_\"\n",
    "\n",
    "    for i,r_g in  enumerate(gold):\n",
    "        r_p = pred[i]\n",
    "\n",
    "        if r_g != null_tag and r_p != null_tag:\n",
    "            true_positives += 1\n",
    "        elif r_g != null_tag and r_p == null_tag:\n",
    "            false_negatives += 1\n",
    "        elif r_g == null_tag and r_p != null_tag:\n",
    "            false_positives += 1\n",
    "\n",
    "    a = true_positives + false_positives\n",
    "    b = true_positives + false_negatives\n",
    "    if a == 0 and b == 0 :        \n",
    "        argument_identification = {\n",
    "            \"true_positives\": 0,\n",
    "            \"false_positives\": 0,\n",
    "            \"false_negatives\": 0,\n",
    "            \"precision\": 0,\n",
    "            \"recall\": 0,\n",
    "            \"f1\": 0,\n",
    "        } \n",
    "\n",
    "    else : \n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        argument_identification = {\n",
    "            \"true_positives\": true_positives,\n",
    "            \"false_positives\": false_positives,\n",
    "            \"false_negatives\": false_negatives,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    true_positives, false_positives, false_negatives = 0, 0, 0\n",
    "    for i,r_g in  enumerate(gold):\n",
    "        r_p = pred[i]\n",
    "\n",
    "        if r_g != null_tag and r_p != null_tag:\n",
    "            if r_g == r_p:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                false_positives += 1\n",
    "                false_negatives += 1\n",
    "        elif r_g != null_tag and r_p == null_tag:\n",
    "                false_negatives += 1\n",
    "        elif r_g == null_tag and r_p != null_tag:\n",
    "                false_positives += 1\n",
    "\n",
    "\n",
    "    a = true_positives + false_positives\n",
    "    b = true_positives + false_negatives\n",
    "    if a == 0 and b == 0 :\n",
    "        argument_classification = {\n",
    "            \"true_positives\": 0,\n",
    "            \"false_positives\": 0,\n",
    "            \"false_negatives\": 0,\n",
    "            \"precision\": 0,\n",
    "            \"recall\": 0,\n",
    "            \"f1\": 0,\n",
    "        } \n",
    "\n",
    "    else : \n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        argument_classification = {\n",
    "            \"true_positives\": true_positives,\n",
    "            \"false_positives\": false_positives,\n",
    "            \"false_negatives\": false_negatives,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "        }\n",
    "\n",
    "\n",
    "    return argument_identification,argument_classification\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "from [1,1,8,8,8,8,8,8,8,8,8,8,8,8,2,2,2,8,8,8......]\n",
    "to from [agent,agent,_,_,_........,]\n",
    "\"\"\"\n",
    "def mapping_args(g,p,mapping):\n",
    "    \n",
    "    \n",
    "    gt = [mapping[elem] for elem in g]\n",
    "    predictions = [mapping[elem] for elem in p]\n",
    "\n",
    "\n",
    "    return gt,predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Argument Identification and Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs n. 0\n",
      "F1 train: [0.         0.2183994  0.03934426 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.45015747 0.\n",
      " 0.07908429 0.98458388 0.         0.         0.         0.\n",
      " 0.         0.0777298  0.63938849 0.         0.         0.\n",
      " 0.         0.31025508 0.        ]\n",
      "F1 avg train: 0.9520681303532916\n",
      "identification {'true_positives': 12720, 'false_positives': 1721, 'false_negatives': 12148, 'precision': 0.8808254276019666, 'recall': 0.511500723821779, 'f1': 0.6471800351064642}\n",
      "classification_result {'true_positives': 8188, 'false_positives': 6253, 'false_negatives': 16680, 'precision': 0.5669967453777439, 'recall': 0.3292584847997426, 'f1': 0.41659670813299754}\n",
      "EPOCHS : 0\n",
      "F1 eval : [0.         0.22277847 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.52082691 0.\n",
      " 0.14814815 0.98893922 0.         0.         0.         0.\n",
      " 0.         0.26086957 0.72075092 0.         0.         0.\n",
      " 0.         0.26262626 0.        ]\n",
      "F1 avg eval : 0.9612485920642895\n",
      "identification {'true_positives': 3277, 'false_positives': 370, 'false_negatives': 1791, 'precision': 0.8985467507540444, 'recall': 0.6466061562746646, 'f1': 0.7520367183017787}\n",
      "classification_result {'true_positives': 2131, 'false_positives': 1516, 'false_negatives': 2937, 'precision': 0.5843158760625171, 'recall': 0.420481452249408, 'f1': 0.4890418818129661}\n",
      "SAVED : saved/model_2022_12_19_15_25_3.pth\n",
      "Epochs n. 1\n",
      "F1 train: [0.         0.41219853 0.11028316 0.00694444 0.         0.\n",
      " 0.         0.         0.         0.         0.53352053 0.\n",
      " 0.3322314  0.98834209 0.         0.         0.         0.\n",
      " 0.         0.22419187 0.74845346 0.         0.         0.\n",
      " 0.         0.46524823 0.        ]\n",
      "F1 avg train: 0.9613032359870648\n",
      "identification {'true_positives': 16159, 'false_positives': 1747, 'false_negatives': 8694, 'precision': 0.9024349380096057, 'recall': 0.6501830764897598, 'f1': 0.7558174887158259}\n",
      "classification_result {'true_positives': 11164, 'false_positives': 6742, 'false_negatives': 13689, 'precision': 0.6234781637439964, 'recall': 0.44920130366555344, 'f1': 0.5221824645103954}\n",
      "EPOCHS : 1\n",
      "F1 eval : [0.         0.49853372 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.56163481 0.\n",
      " 0.39166667 0.99028454 0.         0.         0.         0.\n",
      " 0.         0.34180139 0.76022244 0.         0.         0.\n",
      " 0.         0.56925996 0.        ]\n",
      "F1 avg eval : 0.9669867528655828\n",
      "identification {'true_positives': 3553, 'false_positives': 380, 'false_negatives': 1500, 'precision': 0.9033816425120773, 'recall': 0.7031466455570948, 'f1': 0.7907856665924772}\n",
      "classification_result {'true_positives': 2540, 'false_positives': 1393, 'false_negatives': 2513, 'precision': 0.6458174421561149, 'recall': 0.5026716801899861, 'f1': 0.5653238370799021}\n",
      "SAVED : saved/model_2022_12_19_15_25_3.pth\n",
      "Epochs n. 2\n",
      "F1 train: [0.         0.52178649 0.14141414 0.04487179 0.         0.\n",
      " 0.         0.         0.         0.         0.57364672 0.\n",
      " 0.46130952 0.98948055 0.         0.         0.02657807 0.\n",
      " 0.         0.31603774 0.7799877  0.         0.         0.\n",
      " 0.         0.51223935 0.        ]\n",
      "F1 avg train: 0.964858441421371\n",
      "identification {'true_positives': 17188, 'false_positives': 1729, 'false_negatives': 7654, 'precision': 0.9086007295025639, 'recall': 0.6918927622574672, 'f1': 0.785575538746315}\n",
      "classification_result {'true_positives': 12440, 'false_positives': 6477, 'false_negatives': 12402, 'precision': 0.6576095575408363, 'recall': 0.5007648337492956, 'f1': 0.5685687515711053}\n",
      "EPOCHS : 2\n",
      "F1 eval : [0.         0.61360124 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.61099215 0.\n",
      " 0.49242424 0.99103663 0.         0.         0.         0.\n",
      " 0.         0.49902534 0.77205409 0.         0.         0.\n",
      " 0.         0.59805825 0.        ]\n",
      "F1 avg eval : 0.9700422292499472\n",
      "identification {'true_positives': 3715, 'false_positives': 394, 'false_negatives': 1347, 'precision': 0.9041129228522755, 'recall': 0.7338996444093244, 'f1': 0.810162468651183}\n",
      "classification_result {'true_positives': 2799, 'false_positives': 1310, 'false_negatives': 2263, 'precision': 0.6811876368946216, 'recall': 0.5529435005926511, 'f1': 0.6104023552502452}\n",
      "SAVED : saved/model_2022_12_19_15_25_3.pth\n",
      "Epochs n. 3\n",
      "F1 train: [0.         0.58943599 0.21487603 0.12903226 0.         0.\n",
      " 0.         0.         0.00991736 0.         0.60499328 0.\n",
      " 0.55916955 0.99029352 0.         0.01509434 0.10559006 0.05597964\n",
      " 0.00615385 0.38297872 0.79746835 0.         0.         0.01687764\n",
      " 0.         0.56992547 0.        ]\n",
      "F1 avg train: 0.9676459141379585\n",
      "identification {'true_positives': 17863, 'false_positives': 1667, 'false_negatives': 7003, 'precision': 0.9146441372247824, 'recall': 0.7183704656961313, 'f1': 0.804712136228489}\n",
      "classification_result {'true_positives': 13395, 'false_positives': 6135, 'false_negatives': 11471, 'precision': 0.6858678955453149, 'recall': 0.5386873642725006, 'f1': 0.6034327416884404}\n",
      "EPOCHS : 3\n",
      "F1 eval : [0.         0.6269196  0.03960396 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.61716937 0.\n",
      " 0.55197133 0.99144342 0.         0.         0.13793103 0.\n",
      " 0.         0.49805447 0.79363016 0.         0.         0.\n",
      " 0.         0.57915058 0.        ]\n",
      "F1 avg eval : 0.9711115019211899\n",
      "identification {'true_positives': 3766, 'false_positives': 371, 'false_negatives': 1294, 'precision': 0.9103214890016921, 'recall': 0.7442687747035573, 'f1': 0.8189627052299663}\n",
      "classification_result {'true_positives': 2860, 'false_positives': 1277, 'false_negatives': 2200, 'precision': 0.6913222141648537, 'recall': 0.5652173913043478, 'f1': 0.6219419375883439}\n",
      "SAVED : saved/model_2022_12_19_15_25_3.pth\n",
      "Epochs n. 4\n",
      "F1 train: [0.         0.6258431  0.26954178 0.13333333 0.         0.01980198\n",
      " 0.         0.         0.04672897 0.03571429 0.62858767 0.\n",
      " 0.60394737 0.99091062 0.         0.08934708 0.18911175 0.18777293\n",
      " 0.10644258 0.4235399  0.81199132 0.         0.         0.10196078\n",
      " 0.         0.58589355 0.        ]\n",
      "F1 avg train: 0.9697889124724947\n",
      "identification {'true_positives': 18343, 'false_positives': 1606, 'false_negatives': 6533, 'precision': 0.9194947115143616, 'recall': 0.7373773918636437, 'f1': 0.8184272169548243}\n",
      "classification_result {'true_positives': 14048, 'false_positives': 5901, 'false_negatives': 10828, 'precision': 0.704195699032533, 'recall': 0.5647210162405532, 'f1': 0.6267930842163971}\n",
      "EPOCHS : 4\n",
      "F1 eval : [0.         0.66218487 0.04       0.04347826 0.         0.\n",
      " 0.         0.         0.05673759 0.         0.63201953 0.\n",
      " 0.66666667 0.99177438 0.         0.08       0.13793103 0.09230769\n",
      " 0.03174603 0.52290076 0.79734848 0.         0.         0.\n",
      " 0.         0.63537906 0.        ]\n",
      "F1 avg eval : 0.9725072875122209\n",
      "identification {'true_positives': 3856, 'false_positives': 383, 'false_negatives': 1213, 'precision': 0.9096485020051899, 'recall': 0.7607023081475637, 'f1': 0.8285345938977224}\n",
      "classification_result {'true_positives': 2995, 'false_positives': 1244, 'false_negatives': 2074, 'precision': 0.7065345600377447, 'recall': 0.5908463207733281, 'f1': 0.6435324452084228}\n",
      "SAVED : saved/model_2022_12_19_15_25_3.pth\n",
      "Epochs n. 5\n",
      "F1 train: [0.         0.65797706 0.3011889  0.21538462 0.03174603 0.01923077\n",
      " 0.         0.         0.13086771 0.03508772 0.64760533 0.\n",
      " 0.65762274 0.99125366 0.         0.07746479 0.22764228 0.25098039\n",
      " 0.2184466  0.46712212 0.81892112 0.         0.         0.08270677\n",
      " 0.10526316 0.61279461 0.        ]\n",
      "F1 avg train: 0.9712580155539285\n",
      "identification {'true_positives': 18705, 'false_positives': 1646, 'false_negatives': 6138, 'precision': 0.9191194535895042, 'recall': 0.7529283902910276, 'f1': 0.8277647475328583}\n",
      "classification_result {'true_positives': 14596, 'false_positives': 5755, 'false_negatives': 10247, 'precision': 0.7172129133703503, 'recall': 0.5875296864307853, 'f1': 0.6459264504137717}\n",
      "EPOCHS : 5\n",
      "F1 eval : [0.         0.63992707 0.18691589 0.08510638 0.         0.\n",
      " 0.         0.         0.17964072 0.         0.64892882 0.\n",
      " 0.64048338 0.99180273 0.         0.08510638 0.16949153 0.26666667\n",
      " 0.17142857 0.50299401 0.80426357 0.         0.         0.\n",
      " 0.         0.66666667 0.        ]\n",
      "F1 avg eval : 0.9732776453988776\n",
      "identification {'true_positives': 3820, 'false_positives': 349, 'false_negatives': 1246, 'precision': 0.9162868793475654, 'recall': 0.7540465850769839, 'f1': 0.8272874932322686}\n",
      "classification_result {'true_positives': 3006, 'false_positives': 1163, 'false_negatives': 2060, 'precision': 0.7210362197169585, 'recall': 0.5933675483616265, 'f1': 0.6510016242555495}\n",
      "SAVED : saved/model_2022_12_19_15_25_3.pth\n",
      "Epochs n. 6\n",
      "F1 train: [0.         0.68075445 0.30967742 0.26378897 0.03125    0.0657277\n",
      " 0.         0.         0.21510883 0.15384615 0.67160248 0.\n",
      " 0.68670077 0.99176659 0.         0.14189189 0.27586207 0.28125\n",
      " 0.32093023 0.49876543 0.8334576  0.         0.         0.14893617\n",
      " 0.05405405 0.63361345 0.        ]\n",
      "F1 avg train: 0.9730768032052187\n",
      "identification {'true_positives': 19148, 'false_positives': 1626, 'false_negatives': 5706, 'precision': 0.9217290844324637, 'recall': 0.7704192484107186, 'f1': 0.8393091961076533}\n",
      "classification_result {'true_positives': 15219, 'false_positives': 5555, 'false_negatives': 9635, 'precision': 0.73259844035814, 'recall': 0.6123360424881307, 'f1': 0.6670903830980977}\n",
      "EPOCHS : 6\n",
      "F1 eval : [0.         0.67362924 0.31932773 0.16666667 0.         0.\n",
      " 0.         0.         0.18300654 0.28571429 0.65330107 0.\n",
      " 0.67948718 0.99227931 0.         0.30508475 0.19354839 0.27027027\n",
      " 0.27027027 0.57350272 0.8101345  0.         0.         0.05128205\n",
      " 0.         0.67950963 0.        ]\n",
      "F1 avg eval : 0.9747952184459607\n",
      "identification {'true_positives': 3973, 'false_positives': 415, 'false_negatives': 1079, 'precision': 0.9054238833181404, 'recall': 0.7864212193190816, 'f1': 0.8417372881355932}\n",
      "classification_result {'true_positives': 3159, 'false_positives': 1229, 'false_negatives': 1893, 'precision': 0.7199179580674567, 'recall': 0.6252969121140143, 'f1': 0.6692796610169491}\n",
      "SAVED : saved/model_2022_12_19_15_25_3.pth\n",
      "Epochs n. 7\n",
      "F1 train: [0.         0.69989616 0.37       0.2815534  0.         0.05555556\n",
      " 0.         0.         0.26530612 0.125      0.69434667 0.\n",
      " 0.71609632 0.99216859 0.         0.22941176 0.34567901 0.39636364\n",
      " 0.43724696 0.53518822 0.84095831 0.         0.         0.24358974\n",
      " 0.         0.67321063 0.        ]\n",
      "F1 avg train: 0.9748174845493982\n",
      "identification {'true_positives': 19445, 'false_positives': 1557, 'false_negatives': 5445, 'precision': 0.9258642034091992, 'recall': 0.7812374447569305, 'f1': 0.8474243876928441}\n",
      "classification_result {'true_positives': 15794, 'false_positives': 5208, 'false_negatives': 9096, 'precision': 0.7520236167984001, 'recall': 0.63455202892728, 'f1': 0.6883116883116883}\n",
      "EPOCHS : 7\n",
      "F1 eval : [0.06060606 0.69775187 0.45454545 0.29508197 0.15384615 0.\n",
      " 0.         0.         0.39378238 0.25       0.6851312  0.\n",
      " 0.67846608 0.99247957 0.         0.08       0.27692308 0.2962963\n",
      " 0.32911392 0.56619145 0.8180973  0.         0.         0.1509434\n",
      " 0.         0.7008547  0.        ]\n",
      "F1 avg eval : 0.9762203211039172\n",
      "identification {'true_positives': 3995, 'false_positives': 393, 'false_negatives': 1060, 'precision': 0.9104375569735643, 'recall': 0.7903066271018794, 'f1': 0.8461294080271101}\n",
      "classification_result {'true_positives': 3258, 'false_positives': 1130, 'false_negatives': 1797, 'precision': 0.7424794895168642, 'recall': 0.6445103857566765, 'f1': 0.6900349465212327}\n",
      "SAVED : saved/model_2022_12_19_15_25_3.pth\n",
      "Epochs n. 8\n",
      "F1 train: [0.03896104 0.71898993 0.41277641 0.32679739 0.03174603 0.04608295\n",
      " 0.         0.         0.32019116 0.1369863  0.7067691  0.\n",
      " 0.72301691 0.99247633 0.         0.28969359 0.36057692 0.43205575\n",
      " 0.47177419 0.55317428 0.84660667 0.         0.         0.31268437\n",
      " 0.05555556 0.69996005 0.        ]\n",
      "F1 avg train: 0.9758908403358003\n",
      "identification {'true_positives': 19715, 'false_positives': 1575, 'false_negatives': 5115, 'precision': 0.9260216063879756, 'recall': 0.7939991945227547, 'f1': 0.8549436253252385}\n",
      "classification_result {'true_positives': 16168, 'false_positives': 5122, 'false_negatives': 8662, 'precision': 0.7594175669328324, 'recall': 0.6511478050745066, 'f1': 0.7011274934952298}\n",
      "EPOCHS : 8\n",
      "F1 eval : [0.35       0.70082645 0.44615385 0.33846154 0.13333333 0.\n",
      " 0.         0.         0.41489362 0.28571429 0.68509874 0.\n",
      " 0.69005848 0.99269321 0.         0.35294118 0.42666667 0.34146341\n",
      " 0.34567901 0.55198488 0.82549505 0.         0.         0.19230769\n",
      " 0.         0.71186441 0.        ]\n",
      "F1 avg eval : 0.9769870801524823\n",
      "identification {'true_positives': 4082, 'false_positives': 431, 'false_negatives': 987, 'precision': 0.9044981165521826, 'recall': 0.8052870388636812, 'f1': 0.852014193279065}\n",
      "classification_result {'true_positives': 3335, 'false_positives': 1178, 'false_negatives': 1734, 'precision': 0.7389762907157101, 'recall': 0.6579206944170448, 'f1': 0.6960968482571489}\n",
      "SAVED : saved/model_2022_12_19_15_25_3.pth\n",
      "Epochs n. 9\n",
      "F1 train: [0.15662651 0.73151348 0.43499392 0.36123348 0.03076923 0.14225941\n",
      " 0.         0.         0.39325843 0.24324324 0.71791521 0.\n",
      " 0.74695708 0.99286707 0.         0.31941032 0.41763341 0.48739496\n",
      " 0.53100775 0.5755511  0.85648773 0.         0.         0.36516854\n",
      " 0.09090909 0.70629921 0.        ]\n",
      "F1 avg train: 0.9771968015687583\n",
      "identification {'true_positives': 20013, 'false_positives': 1515, 'false_negatives': 4836, 'precision': 0.9296265328874025, 'recall': 0.8053845225159966, 'f1': 0.8630571188304548}\n",
      "classification_result {'true_positives': 16590, 'false_positives': 4938, 'false_negatives': 8259, 'precision': 0.7706243032329989, 'recall': 0.667632500301823, 'f1': 0.7154408435215731}\n",
      "EPOCHS : 9\n",
      "F1 eval : [0.34146341 0.71404682 0.48120301 0.25806452 0.15384615 0.125\n",
      " 0.         0.         0.40414508 0.28571429 0.68462644 0.\n",
      " 0.70186335 0.99268119 0.         0.2972973  0.38356164 0.32941176\n",
      " 0.41758242 0.57826888 0.82462328 0.         0.         0.18518519\n",
      " 0.         0.70526316 0.        ]\n",
      "F1 avg eval : 0.9770904795380656\n",
      "identification {'true_positives': 4020, 'false_positives': 389, 'false_negatives': 1022, 'precision': 0.9117713767294171, 'recall': 0.7973026576755256, 'f1': 0.8507036292455825}\n",
      "classification_result {'true_positives': 3290, 'false_positives': 1119, 'false_negatives': 1752, 'precision': 0.7462009525969607, 'recall': 0.6525188417294724, 'f1': 0.6962226219447677}\n",
      "SAVED : saved/model_2022_12_19_15_25_3.pth\n",
      "Epochs n. 10\n",
      "F1 train: [0.22099448 0.74745733 0.44336175 0.35448578 0.06060606 0.14468085\n",
      " 0.         0.         0.41311475 0.32911392 0.73438861 0.\n",
      " 0.77216749 0.99307259 0.         0.32923833 0.47302905 0.4958124\n",
      " 0.55909944 0.59881188 0.86118598 0.         0.         0.41899441\n",
      " 0.23255814 0.72705974 0.        ]\n",
      "F1 avg train: 0.9781587490657163\n",
      "identification {'true_positives': 20230, 'false_positives': 1516, 'false_negatives': 4655, 'precision': 0.9302860296146418, 'recall': 0.8129395218002813, 'f1': 0.8676631425446591}\n",
      "classification_result {'true_positives': 16991, 'false_positives': 4755, 'false_negatives': 7894, 'precision': 0.7813390968453968, 'recall': 0.6827807916415511, 'f1': 0.7287426819068861}\n",
      "EPOCHS : 10\n",
      "F1 eval : [0.38297872 0.73098937 0.46616541 0.34920635 0.15384615 0.15384615\n",
      " 0.         0.         0.45226131 0.25       0.69285447 0.\n",
      " 0.70552147 0.99268858 0.         0.34782609 0.43902439 0.35294118\n",
      " 0.45652174 0.60560748 0.82197943 0.         0.         0.34666667\n",
      " 0.         0.75120385 0.        ]\n",
      "F1 avg eval : 0.9778048210949208\n",
      "identification {'true_positives': 4021, 'false_positives': 383, 'false_negatives': 1031, 'precision': 0.9130336058128974, 'recall': 0.7959224069675376, 'f1': 0.8504653130287647}\n",
      "classification_result {'true_positives': 3343, 'false_positives': 1061, 'false_negatives': 1709, 'precision': 0.7590826521344233, 'recall': 0.6617181314330958, 'f1': 0.7070642978003383}\n",
      "SAVED : saved/model_2022_12_19_15_25_3.pth\n",
      "Epochs n. 11\n",
      "F1 train: [0.24210526 0.75103022 0.48251748 0.38410596 0.16438356 0.184\n",
      " 0.         0.         0.4279476  0.3908046  0.74974479 0.\n",
      " 0.7863354  0.9933121  0.         0.3690205  0.47540984 0.54028436\n",
      " 0.57570093 0.60627451 0.86558704 0.         0.         0.45405405\n",
      " 0.23255814 0.75218216 0.02040816]\n",
      "F1 avg train: 0.979068064642171\n",
      "identification {'true_positives': 20432, 'false_positives': 1533, 'false_negatives': 4425, 'precision': 0.930207147735033, 'recall': 0.8219817355272157, 'f1': 0.8727521250694118}\n",
      "classification_result {'true_positives': 17310, 'false_positives': 4655, 'false_negatives': 7547, 'precision': 0.7880719326200774, 'recall': 0.6963833125477733, 'f1': 0.7393960104224511}\n",
      "EPOCHS : 11\n",
      "F1 eval : [0.42553191 0.72997523 0.46808511 0.31168831 0.375      0.14634146\n",
      " 0.         0.         0.43192488 0.28571429 0.69970194 0.\n",
      " 0.70029674 0.99271235 0.         0.45454545 0.45       0.40425532\n",
      " 0.43564356 0.60550459 0.822187   0.         0.         0.33333333\n",
      " 0.         0.75483871 0.        ]\n",
      "F1 avg eval : 0.9779740778682788\n",
      "identification {'true_positives': 4086, 'false_positives': 438, 'false_negatives': 972, 'precision': 0.903183023872679, 'recall': 0.8078291814946619, 'f1': 0.8528490920475892}\n",
      "classification_result {'true_positives': 3393, 'false_positives': 1131, 'false_negatives': 1665, 'precision': 0.75, 'recall': 0.6708185053380783, 'f1': 0.7082028804007515}\n",
      "SAVED : saved/model_2022_12_19_15_25_3.pth\n",
      "Epochs n. 12\n",
      "F1 train: [0.28421053 0.76946721 0.52085682 0.44489796 0.21052632 0.25680934\n",
      " 0.         0.         0.48387097 0.43478261 0.75764224 0.\n",
      " 0.78589109 0.99356016 0.         0.42358079 0.49701789 0.57792208\n",
      " 0.64179104 0.62204724 0.87514257 0.         0.         0.48257373\n",
      " 0.17777778 0.77015373 0.04040404]\n",
      "F1 avg train: 0.980115651479097\n",
      "identification {'true_positives': 20662, 'false_positives': 1535, 'false_negatives': 4187, 'precision': 0.9308465107897463, 'recall': 0.8315022737333494, 'f1': 0.8783743570122858}\n",
      "classification_result {'true_positives': 17728, 'false_positives': 4469, 'false_negatives': 7121, 'precision': 0.7986664864621346, 'recall': 0.7134291118354863, 'f1': 0.7536453683628789}\n",
      "EPOCHS : 12\n",
      "F1 eval : [0.45833333 0.71496063 0.46478873 0.35820896 0.5        0.12121212\n",
      " 0.         0.         0.48101266 0.25       0.7092145  0.\n",
      " 0.72340426 0.99290999 0.         0.5        0.36842105 0.44210526\n",
      " 0.46601942 0.58348624 0.82869785 0.         0.         0.44776119\n",
      " 0.         0.77897991 0.        ]\n",
      "F1 avg eval : 0.9785637264335609\n",
      "identification {'true_positives': 4129, 'false_positives': 453, 'false_negatives': 923, 'precision': 0.9011348756001746, 'recall': 0.8173000791765638, 'f1': 0.8571725140128711}\n",
      "classification_result {'true_positives': 3447, 'false_positives': 1135, 'false_negatives': 1605, 'precision': 0.7522915757311218, 'recall': 0.6823040380047506, 'f1': 0.7155906165663277}\n",
      "SAVED : saved/model_2022_12_19_15_25_3.pth\n",
      "Epochs n. 13\n",
      "F1 train: [0.30541872 0.77758913 0.54545455 0.42978723 0.10666667 0.24354244\n",
      " 0.         0.         0.49790795 0.45652174 0.77048578 0.06666667\n",
      " 0.80621118 0.99370662 0.         0.45652174 0.51625239 0.6\n",
      " 0.67148014 0.61923378 0.87982804 0.         0.         0.49140049\n",
      " 0.13953488 0.77891543 0.01960784]\n",
      "F1 avg train: 0.9807294786113292\n",
      "identification {'true_positives': 20762, 'false_positives': 1509, 'false_negatives': 4085, 'precision': 0.9322437250235732, 'recall': 0.8355938342657061, 'f1': 0.8812767944310029}\n",
      "classification_result {'true_positives': 17944, 'false_positives': 4327, 'false_negatives': 6903, 'precision': 0.8057114633379732, 'recall': 0.7221797400088542, 'f1': 0.7616622097712128}\n",
      "Early stopping at epoch :  13\n",
      "F1 eval : [0.43137255 0.72283465 0.44755245 0.33802817 0.42105263 0.15384615\n",
      " 0.         0.         0.49565217 0.22222222 0.71138367 0.\n",
      " 0.73556231 0.99281997 0.         0.44444444 0.43373494 0.42222222\n",
      " 0.48       0.58500914 0.82663236 0.         0.         0.47761194\n",
      " 0.         0.76971609 0.        ]\n",
      "F1 avg eval : 0.9784439268230268\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "currentDateAndTime = datetime.now()\n",
    "_id = str(currentDateAndTime.year)+\"_\"+str(currentDateAndTime.month)+\"_\"+str(currentDateAndTime.day)+\"_\"+str(currentDateAndTime.hour)+\"_\"+str(currentDateAndTime.minute)+\"_\"+str(currentDateAndTime.second)\n",
    "\n",
    "\n",
    "\n",
    "#optimizer = torch.optim.Adam(model.parameters(),lr = 0.000005)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "\n",
    "logSotfMax = torch.nn.LogSoftmax(dim=1)\n",
    "nll_loss = torch.nn.NLLLoss()\n",
    "\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "dataloader_dev = DataLoader(dev_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "mapping = dataloader_train.dataset.args_roles\n",
    "\n",
    "auto_model.eval()\n",
    "\n",
    "EPOCHS = 200\n",
    "patience_counter = 0\n",
    "patience = 5\n",
    "max_val_loss = 9999\n",
    "f1_score_max = 0\n",
    "output_path = \"saved\"\n",
    "model_name = \"model_\"+_id+\".pth\"\n",
    "PATH = os.path.join(output_path,model_name)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    #TRAINING\n",
    "    p = []\n",
    "    g = []\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_train):\n",
    "        #print(sample_batched)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "       \n",
    "        #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "        #------------------FILTERING SUB-WORDS----------------------\n",
    "        subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "        word_emebedding = []\n",
    "        for i in range(n):\n",
    "            subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "            flag = subtoken_mask[0,i,0]\n",
    "            if flag :\n",
    "                continue\n",
    "            else :\n",
    "                word_emebedding.append(subwords_embedding)\n",
    "        word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "        #-------------------------FORWARD/BACKWARD----------------------------------\n",
    "        x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "            perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "            predicate_index = sample_batched[\"predicate_index\"],\n",
    "            pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "            predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])        \n",
    "        b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "        loss = nll_loss(logSotfMax(x),gt)\n",
    "        total_loss = total_loss + loss\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "        counter += 1 \n",
    "            \n",
    "\n",
    "        #-------------------------RESULT STORING----------------------------------\n",
    "        predicted = torch.argmax(x, dim=1)\n",
    "        p += predicted.tolist()\n",
    "        g += gt.tolist()\n",
    "    \n",
    "\n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    scheduler.step()\n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    print(\"Epochs n.\", epoch)\n",
    "    print(\"F1 train:\",f1)\n",
    "    print(\"F1 avg train:\",f1_avg)\n",
    "    \n",
    "    avg_train_loss = total_loss/counter\n",
    "    writer.add_scalar(\"Loss/train\", avg_train_loss, epoch)\n",
    "\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"Train_EN/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"Train_EN/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "    #EVALUATION\n",
    "    p = []\n",
    "    g = []\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_dev):\n",
    "    \n",
    "      #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "            #------------------FILTERING SUB-WORDS----------------------\n",
    "            subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "            word_emebedding = []\n",
    "            for i in range(n): \n",
    "                subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "                flag = subtoken_mask[0,i,0]\n",
    "                if flag :\n",
    "                    continue\n",
    "                else :\n",
    "                    word_emebedding.append(subwords_embedding)\n",
    "            word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "            #-------------------------FORWARD----------------------------------\n",
    "            x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "                        perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "                        predicate_index = sample_batched[\"predicate_index\"],\n",
    "                        pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "                        predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])   \n",
    "\n",
    "\n",
    "            b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "            loss = nll_loss(logSotfMax(x),gt)\n",
    "            total_loss = total_loss + loss\n",
    "            #-------------------------RESULT STORING----------------------------------\n",
    "            predicted = torch.argmax(x, dim=1)\n",
    "            p += predicted.tolist()\n",
    "            g += gt.tolist()\n",
    "            counter += 1 \n",
    "    \n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    avg_eval_loss = total_loss/counter\n",
    "\n",
    "    if avg_eval_loss < max_val_loss:\n",
    "        max_val_loss = avg_eval_loss\n",
    "    else :\n",
    "        patience_counter += 1\n",
    "    \n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    if patience_counter >= patience :  \n",
    "\n",
    "\n",
    "        print(\"Early stopping at epoch : \",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 avg eval :\",f1_avg)\n",
    "        break\n",
    "    else :\n",
    "        print(\"EPOCHS :\",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 avg eval :\",f1_avg)\n",
    "    \n",
    "\n",
    "    writer.add_scalar(\"Loss/validation\", avg_eval_loss, epoch)\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"Eval_EN/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"Eval_EN/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if f1_avg > f1_score_max:\n",
    "        f1_score_max = f1_avg\n",
    "        print(\"SAVED :\",PATH)\n",
    "        torch.save(model.state_dict(),PATH)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Transfert Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning over Structural Information over English dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading English Pretrained Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Arg_Classifier(\n",
       "  (bi_lstm_portable): LSTM(132, 50, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (embedding_predicate_flag): Embedding(2, 32, max_norm=True)\n",
       "  (embedding_predicate): Embedding(304, False, max_norm=True)\n",
       "  (embedding_pos): Embedding(18, 100, max_norm=True)\n",
       "  (bi_lstm): LSTM(900, 50, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (dropout_language_constraint): Dropout(p=0.6, inplace=False)\n",
       "  (dropout_in_classifier): Dropout(p=0.4, inplace=False)\n",
       "  (Relu): ReLU()\n",
       "  (Sigmoid): Sigmoid()\n",
       "  (linear0): Linear(in_features=300, out_features=675, bias=True)\n",
       "  (linear1): Linear(in_features=675, out_features=135, bias=True)\n",
       "  (linear2): Linear(in_features=135, out_features=27, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#note that parameter EN is only used for tracking on which dataset was/is trained\n",
    "model = Arg_Classifier(\"EN\",cfg)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language constained training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_language_constrains()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument indices in method wrapper__embedding_renorm_)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 91\u001b[0m\n\u001b[1;32m     89\u001b[0m word_emebedding \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(word_emebedding,dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m#-------------------------FORWARD/BACKWARD----------------------------------\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubwords_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mperdicate_positional_encoding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msample_batched\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpositional_encoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredicate_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msample_batched\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpredicate_index\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpos_index_encoding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msample_batched\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpos_index\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredicate_meaning_encoding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msample_batched\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpredicate_meaning_idx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m        \n\u001b[1;32m     96\u001b[0m b,n \u001b[38;5;241m=\u001b[39m sample_batched[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgt\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_gt\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m     97\u001b[0m loss \u001b[38;5;241m=\u001b[39m nll_loss(logSotfMax(x),gt)\n",
      "File \u001b[0;32m/media/mv/Volume/Download/TEST_EXP/nlp2022-hw2-main-master/hw2/stud/arg.py:109\u001b[0m, in \u001b[0;36mArg_Classifier.forward\u001b[0;34m(self, subwords_embeddings, perdicate_positional_encoding, predicate_index, pos_index_encoding, predicate_meaning_encoding)\u001b[0m\n\u001b[1;32m    <a href='file:///media/mv/Volume/Download/TEST_EXP/nlp2022-hw2-main-master/hw2/stud/arg.py?line=105'>106</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, subwords_embeddings :torch\u001b[39m.\u001b[39mtensor, perdicate_positional_encoding : torch\u001b[39m.\u001b[39mtensor, predicate_index:\u001b[39mlist\u001b[39m, pos_index_encoding:torch\u001b[39m.\u001b[39mtensor, predicate_meaning_encoding:torch\u001b[39m.\u001b[39mtensor):\n\u001b[1;32m    <a href='file:///media/mv/Volume/Download/TEST_EXP/nlp2022-hw2-main-master/hw2/stud/arg.py?line=106'>107</a>\u001b[0m     \n\u001b[1;32m    <a href='file:///media/mv/Volume/Download/TEST_EXP/nlp2022-hw2-main-master/hw2/stud/arg.py?line=107'>108</a>\u001b[0m     \u001b[39m#-------------------Emdedding and recombining----------------------------- \u001b[39;00m\n\u001b[0;32m--> <a href='file:///media/mv/Volume/Download/TEST_EXP/nlp2022-hw2-main-master/hw2/stud/arg.py?line=108'>109</a>\u001b[0m     flag_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding_predicate_flag(perdicate_positional_encoding)\n\u001b[1;32m    <a href='file:///media/mv/Volume/Download/TEST_EXP/nlp2022-hw2-main-master/hw2/stud/arg.py?line=109'>110</a>\u001b[0m     b,n,h \u001b[39m=\u001b[39m flag_embedding\u001b[39m.\u001b[39msize()\n\u001b[1;32m    <a href='file:///media/mv/Volume/Download/TEST_EXP/nlp2022-hw2-main-master/hw2/stud/arg.py?line=110'>111</a>\u001b[0m     subwords_embeddings \u001b[39m=\u001b[39m subwords_embeddings[:,:n,:]\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1131'>1132</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/sparse.py:158\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/sparse.py?line=156'>157</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/sparse.py?line=157'>158</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/sparse.py?line=158'>159</a>\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/sparse.py?line=159'>160</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/functional.py:2198\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/functional.py?line=2191'>2192</a>\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mcontiguous()\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/functional.py?line=2192'>2193</a>\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/functional.py?line=2193'>2194</a>\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/functional.py?line=2194'>2195</a>\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/functional.py?line=2195'>2196</a>\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/functional.py?line=2196'>2197</a>\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m-> <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/functional.py?line=2197'>2198</a>\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39;49m, max_norm, norm_type)\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/functional.py?line=2198'>2199</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39membedding(weight, \u001b[39minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/functional.py:2096\u001b[0m, in \u001b[0;36m_no_grad_embedding_renorm_\u001b[0;34m(weight, input, max_norm, norm_type)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/functional.py?line=2093'>2094</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_no_grad_embedding_renorm_\u001b[39m(weight: Tensor, \u001b[39minput\u001b[39m: Tensor, max_norm: \u001b[39mfloat\u001b[39m, norm_type: \u001b[39mfloat\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/functional.py?line=2094'>2095</a>\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m-> <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/functional.py?line=2095'>2096</a>\u001b[0m         torch\u001b[39m.\u001b[39;49membedding_renorm_(weight, \u001b[39minput\u001b[39;49m, max_norm, norm_type)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument indices in method wrapper__embedding_renorm_)"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "currentDateAndTime = datetime.now()\n",
    "_id = str(currentDateAndTime.year)+\"_\"+str(currentDateAndTime.month)+\"_\"+str(currentDateAndTime.day)+\"_\"+str(currentDateAndTime.hour)+\"_\"+str(currentDateAndTime.minute)+\"_\"+str(currentDateAndTime.second)\n",
    "_id =  _id +\"Language constained training\"\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 0.00000005)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "\n",
    "logSotfMax = torch.nn.LogSoftmax(dim=1)\n",
    "nll_loss = torch.nn.NLLLoss()\n",
    "\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "dataloader_dev = DataLoader(dev_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "mapping = dataloader_train.dataset.args_roles\n",
    "\n",
    "auto_model.eval()\n",
    "\n",
    "EPOCHS = 1\n",
    "patience_counter = 0\n",
    "patience = 5\n",
    "max_val_loss = 9999\n",
    "f1_score_max = 0\n",
    "output_path = \"saved\"\n",
    "model_name = \"model_\"+_id+\".pth\"\n",
    "PATH = os.path.join(output_path,model_name)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    #TRAINING\n",
    "    p = []\n",
    "    g = []\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_train):\n",
    "        #print(sample_batched)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "       \n",
    "        #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "        #------------------FILTERING SUB-WORDS----------------------\n",
    "        subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "        word_emebedding = []\n",
    "        for i in range(n):\n",
    "            subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "            flag = subtoken_mask[0,i,0]\n",
    "            if flag :\n",
    "                continue\n",
    "            else :\n",
    "                word_emebedding.append(subwords_embedding)\n",
    "        word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "        #-------------------------FORWARD/BACKWARD----------------------------------\n",
    "        x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "            perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "            predicate_index = sample_batched[\"predicate_index\"],\n",
    "            pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "            predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])        \n",
    "        b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "        loss = nll_loss(logSotfMax(x),gt)\n",
    "        total_loss = total_loss + loss\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "        counter += 1 \n",
    "            \n",
    "\n",
    "        #-------------------------RESULT STORING----------------------------------\n",
    "        predicted = torch.argmax(x, dim=1)\n",
    "        p += predicted.tolist()\n",
    "        g += gt.tolist()\n",
    "    \n",
    "\n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    print(\"Epochs n.\", epoch)\n",
    "    print(\"F1 train:\",f1_score(g, p, average=None))\n",
    "    scheduler.step()\n",
    "    avg_train_loss = total_loss/counter\n",
    "    writer.add_scalar(\"Loss/train\", avg_train_loss, epoch)\n",
    "\n",
    "\n",
    "\n",
    "    #EVALUATION\n",
    "    p = []\n",
    "    g = []\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_dev):\n",
    "    \n",
    "      #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "            #------------------FILTERING SUB-WORDS----------------------\n",
    "            subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "            word_emebedding = []\n",
    "            for i in range(n): \n",
    "                subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "                flag = subtoken_mask[0,i,0]\n",
    "                if flag :\n",
    "                    continue\n",
    "                else :\n",
    "                    word_emebedding.append(subwords_embedding)\n",
    "            word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "            #-------------------------FORWARD----------------------------------\n",
    "            x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "                        perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "                        predicate_index = sample_batched[\"predicate_index\"],\n",
    "                        pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "                        predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])   \n",
    "\n",
    "\n",
    "            b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "            loss = nll_loss(logSotfMax(x),gt)\n",
    "            total_loss = total_loss + loss\n",
    "            #-------------------------RESULT STORING----------------------------------\n",
    "            predicted = torch.argmax(x, dim=1)\n",
    "            p += predicted.tolist()\n",
    "            g += gt.tolist()\n",
    "            counter += 1 \n",
    "    \n",
    "    #-------------------------RESULTS----------------------------------\n",
    "\n",
    "    avg_eval_loss = total_loss/counter\n",
    "\n",
    "    if avg_eval_loss < max_val_loss:\n",
    "        max_val_loss = avg_eval_loss\n",
    "    else :\n",
    "        patience_counter += 1\n",
    "    \n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    if patience_counter >= patience :  \n",
    "\n",
    "\n",
    "        print(\"Early stopping at epoch : \",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 eval :\",f1_avg)\n",
    "        break\n",
    "    else :\n",
    "        print(\"EPOCHS :\",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 eval :\",f1_avg)\n",
    "    \n",
    "\n",
    "    writer.add_scalar(\"Loss/validation\", avg_eval_loss, epoch)\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"Eval_EN/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"Eval_EN/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if f1_avg > f1_score_max:\n",
    "        f1_score_max = f1_avg\n",
    "        torch.save(model.state_dict(),PATH)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spanish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Spanish Dataset\n",
    "bert-base-multilingual-cased\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 625/625 [00:00<00:00, 137kB/s]\n",
      "Downloading: 100%|██████████| 714M/714M [00:09<00:00, 73.2MB/s] \n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model class is      : <class 'transformers.models.bert.modeling_bert.BertModel'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 29.0/29.0 [00:00<00:00, 8.39kB/s]\n",
      "Downloading: 100%|██████████| 996k/996k [00:01<00:00, 810kB/s] \n",
      "Downloading: 100%|██████████| 1.96M/1.96M [00:01<00:00, 1.00MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model class is      : <class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import random\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "\n",
    "auto_model = AutoModel.from_pretrained(\"bert-base-multilingual-cased\",output_hidden_states=True)\n",
    "print(f\"\\nmodel class is      : {type(auto_model)}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "print(f\"\\nmodel class is      : {type(tokenizer)}\")\n",
    "\n",
    "\n",
    "\n",
    "class SRL(Dataset):\n",
    " \n",
    "    def __init__(self,language,path,args_roles = None,pos_list = None) -> None:\n",
    "\n",
    "        self.path_root = 'data'\n",
    "        self.load_data(language,path)\n",
    "        if args_roles is None :\n",
    "            self.args_roles,self.list_broken_id = self.list_arg_roles()\n",
    "        else : \n",
    "            self.args_roles = args_roles\n",
    "            _,self.list_broken_id = self.list_arg_roles()\n",
    "        \n",
    "\n",
    "        if pos_list is None :\n",
    "            self.pos_list,_ = self.list_pos()\n",
    "            self.pos_list.append(\"Nothing\")\n",
    "        else : \n",
    "            self.pos_list = pos_list\n",
    "        \n",
    "\n",
    "\n",
    "        self.predicate_dis,_ = self.list_predicate_roles()\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(self.pos_list)\n",
    "\n",
    "        self.predicate_dis.append(\"Nothing\")\n",
    "\n",
    "    def load_data(self,language,mode):\n",
    "        \n",
    "        mode = mode+\".json\"\n",
    "        path = os.path.join(self.path_root,language,mode)\n",
    "        data_file = open(path)\n",
    "       \n",
    "        data_ = json.load(data_file)\n",
    "\n",
    "        list_data = []\n",
    "\n",
    "        for data in data_:\n",
    "            list_data.append(data_[data])\n",
    "        \n",
    "\n",
    "        self.data = list_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, id : int):\n",
    "\n",
    "        flag = False\n",
    "        if id in self.list_broken_id :\n",
    "            flag = True\n",
    "            while flag == True:\n",
    "\n",
    "                rand_id = random.randint(0, len(self.data)-1)\n",
    "                \n",
    "                if rand_id in self.list_broken_id :\n",
    "                    pass\n",
    "                else :\n",
    "                    flag = False\n",
    "                    id = rand_id        \n",
    "\n",
    "\n",
    "        data = self.pre_processing(self.data[id])\n",
    "        data = self.processig(data)\n",
    "        return data\n",
    "        \n",
    "    def pre_processing(self, data:dict):\n",
    "        data_list = []\n",
    "        for role in data[\"roles\"]:\n",
    "            dictionary = dict()\n",
    "            dictionary[\"words\"] = data[\"words\"]\n",
    "            dictionary[\"role\"] = data[\"roles\"][role]\n",
    "            dictionary[\"pre_idx\"] = role\n",
    "            dictionary[\"pos_tags\"] = data[\"pos_tags\"]\n",
    "            dictionary[\"predicate_meaning\"] = data[\"predicates\"]\n",
    "            data_list.append(dictionary)    \n",
    "        return data_list\n",
    "    \n",
    "    def processig(self,data_list:list):\n",
    "        \n",
    "        for dictionary in data_list:\n",
    "\n",
    "            #dictionary[\"words\"] = data[\"words\"]\n",
    "            dictionary[\"gt_arg_identification\"] = self.arg_id(dictionary[\"role\"])\n",
    "            dictionary[\"gt_arg_classification\"] = self.arg_class(dictionary[\"role\"])\n",
    "            dictionary[\"pos_idx\"] = self.pos_idx(dictionary[\"pos_tags\"])\n",
    "            dictionary[\"predicate_meaning_idx\"] = self.predicate_meaning_idx(dictionary[\"predicate_meaning\"])\n",
    "        \n",
    "        return data_list\n",
    "   \n",
    "    def list_arg_roles(self):\n",
    "        list_roles = []\n",
    "        list_broken_id = []\n",
    "        for i,element in enumerate(self.data):\n",
    "            flag = True\n",
    "            try : roles = element[\"roles\"]\n",
    "            except : flag = False\n",
    "            if flag :\n",
    "                for e in roles:\n",
    "                    sentence = element[\"roles\"][e]\n",
    "\n",
    "                    for word in sentence:\n",
    "                        \n",
    "                        list_roles.append(word)\n",
    "                list_roles = list(set(list_roles))\n",
    "            else : \n",
    "                list_broken_id.append(i)\n",
    "        return list_roles,list_broken_id\n",
    "\n",
    "    def list_predicate_roles(self):\n",
    "        list_predicate_roles = []\n",
    "        list_broken_id = []\n",
    "        for i,element in enumerate(self.data):\n",
    "            flag = True\n",
    "            try : predicates = element[\"predicates\"]\n",
    "            except : flag = False\n",
    "            if flag :\n",
    "                for pre in predicates:\n",
    "                    list_predicate_roles.append(pre)\n",
    "                list_predicate_roles = list(set(list_predicate_roles))\n",
    "            else : \n",
    "                list_broken_id.append(i)\n",
    "        return list_predicate_roles,list_broken_id\n",
    "\n",
    "    def list_pos(self):\n",
    "        list_pos = []\n",
    "        list_broken_id = []\n",
    "        for i,element in enumerate(self.data):\n",
    "            flag = True\n",
    "            try : pos = element[\"pos_tags\"]\n",
    "            except : flag = False\n",
    "            if flag :\n",
    "                for e in pos:\n",
    "                    list_pos.append(e)\n",
    "                list_pos = list(set(list_pos))\n",
    "            else : \n",
    "                list_broken_id.append(i)\n",
    "        return list_pos,list_broken_id\n",
    "  \n",
    "    def arg_class(self,role:list):\n",
    "        list_idxs = []\n",
    "        for element in role:\n",
    "            list_idxs.append(self.args_roles.index(element))\n",
    "        \n",
    "\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "\n",
    "    def arg_id(self,role:dict):\n",
    "        list_idxs = []\n",
    "        for element in role:\n",
    "            if element == \"_\":\n",
    "                list_idxs.append(0)\n",
    "            else :\n",
    "                list_idxs.append(1)\n",
    "\n",
    "        \n",
    "\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "\n",
    "    def pos_idx(self,pos_tags:dict):\n",
    "        list_idxs = []\n",
    "        list_idxs.append(self.pos_list.index(\"Nothing\"))\n",
    "\n",
    "        for element in pos_tags:\n",
    "            list_idxs.append(self.pos_list.index(element))\n",
    "        \n",
    "        list_idxs.append(self.pos_list.index(\"Nothing\"))\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "    \n",
    "    def predicate_meaning_idx(self,predicate_meaning_tags:dict):\n",
    "        list_idxs = []\n",
    "        list_idxs.append(self.predicate_dis.index(\"Nothing\"))\n",
    "\n",
    "        for element in predicate_meaning_tags:\n",
    "            list_idxs.append(self.predicate_dis.index(element))\n",
    "        \n",
    "        list_idxs.append(self.predicate_dis.index(\"Nothing\"))\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64) \n",
    "\n",
    "    \n",
    "# here we define our collate function\n",
    "def collate_fn(batch) -> Dict[str, torch.Tensor]:\n",
    "    #print(batch)\n",
    "    input = dict() \n",
    "    batch_sentence = [] \n",
    "    #print(len(batch))\n",
    "    for period in batch:\n",
    "        for sentence in period :\n",
    "        \n",
    "            #print(len(sentence[0][\"words\"]))\n",
    "            pre_idx = int(sentence[\"pre_idx\"])\n",
    "            \n",
    "\n",
    "            predicate = sentence[\"words\"][pre_idx]\n",
    "\n",
    "            text = \" \".join(sentence[\"words\"])\n",
    "            tokens: list[str] = text.split()\n",
    "            predicate: list[str] = predicate.split()\n",
    "\n",
    "            #text = sentence[0][\"words\"]\n",
    "            \n",
    "            t = (tokens,predicate)\n",
    "\n",
    "            batch_sentence.append(t)\n",
    "            #print(batch_sentence)\n",
    "\n",
    "    batch_output = tokenizer.batch_encode_plus(batch_sentence,padding=True,is_split_into_words=True, truncation=True,return_offsets_mapping=True, return_tensors=\"pt\")\n",
    "    #print(batch_output.keys())\n",
    "\n",
    "\n",
    "    gt = dict()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    for period in batch:\n",
    "\n",
    "        list_positional_predicate_encoding = []\n",
    "        list_arg_gt = []\n",
    "        list_predicate_index = [] \n",
    "        list_pos_index = [] \n",
    "        list_predicate_meaning_index = []\n",
    "\n",
    "        for sentence in period:\n",
    "            #positional_encoding\n",
    "            #+2 per il CLS iniziale ad SEP finale\n",
    "            sentence_words_lenght =  len(sentence[\"words\"])\n",
    "            positional_predicate_encoding = torch.zeros(1,sentence_words_lenght+2)\n",
    "            #+1 per il CLS iniziale\n",
    "            pre_idx = int(sentence[\"pre_idx\"])\n",
    "            positional_predicate_encoding[:,pre_idx+1] = 1\n",
    "            list_positional_predicate_encoding.append(positional_predicate_encoding)\n",
    "            #print(\"positional_prefix_encoding\",positional_predicate_encoding)\n",
    "            list_predicate_index.append(pre_idx)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            pos = torch.unsqueeze(sentence[\"pos_idx\"],dim = 0)\n",
    "            list_pos_index.append(pos)\n",
    "            predicate_meaning_idxs = torch.unsqueeze(sentence[\"predicate_meaning_idx\"],dim = 0)\n",
    "            list_predicate_meaning_index.append(predicate_meaning_idxs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #note CLS and SEP are discharder after Bi-LSTM, the Classifier takes in input only wokrds hidden state embedding\n",
    "            arg_gt = torch.unsqueeze(sentence[\"gt_arg_classification\"],dim = 0)\n",
    "            list_arg_gt.append(arg_gt)\n",
    "        \n",
    "\n",
    "    list_arg_gt = torch.cat(list_arg_gt,dim = 0)\n",
    "    list_pos_index = torch.cat(list_pos_index,dim = 0)\n",
    "    list_predicate_meaning_index = torch.cat(list_predicate_meaning_index,dim = 0)\n",
    "    list_positional_predicate_encoding = torch.cat(list_positional_predicate_encoding,dim = 0)\n",
    "    gt[\"arg_gt\"] = list_arg_gt\n",
    "    input[\"predicate_index\"] = list_predicate_index\n",
    "    input[\"pos_index\"] = list_pos_index.long()\n",
    "    input[\"predicate_meaning_idx\"] = list_predicate_meaning_index.long()\n",
    "    offset = batch_output.pop(\"offset_mapping\")\n",
    "    input[\"BERT_input\"] = batch_output\n",
    "    input[\"positional_encoding\"] = list_positional_predicate_encoding.long()\n",
    "    input[\"offset_mapping\"] = offset\n",
    "    input[\"gt\"] = gt\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    return input\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SCONJ', 'ADV', 'PUNCT', 'NOUN', 'DET', 'NUM', 'PART', 'ADP', 'CCONJ', 'SYM', 'INTJ', 'PRON', 'X', 'ADJ', 'AUX', 'PROPN', 'VERB', 'Nothing']\n",
      "['SCONJ', 'ADV', 'PUNCT', 'NOUN', 'DET', 'NUM', 'PART', 'ADP', 'CCONJ', 'SYM', 'INTJ', 'PRON', 'X', 'ADJ', 'AUX', 'PROPN', 'VERB', 'Nothing']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#train_dataset = SRL(\"EN\",\"train\")\n",
    "\n",
    "#note here we are directly loading args_roles mapping as computed before the the dasetet where we have perfomerd \n",
    "#EN and ES dataset should have the same consistency in generating \n",
    "train_dataset = SRL(\"ES\",\"train\",train_dataset.args_roles,train_dataset.pos_list)\n",
    "#same mapping should be used in both the dataset\n",
    "dev_dataset = SRL(\"ES\",\"dev\",train_dataset.args_roles,train_dataset.pos_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English-Spanish attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Arg_Classifier(\n",
       "  (bi_lstm_portable): LSTM(132, 50, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (embedding_predicate_flag): Embedding(2, 32, max_norm=True)\n",
       "  (embedding_predicate): Embedding(304, False, max_norm=True)\n",
       "  (embedding_pos): Embedding(18, 100, max_norm=True)\n",
       "  (bi_lstm): LSTM(900, 50, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (dropout_language_constraint): Dropout(p=0.6, inplace=False)\n",
       "  (dropout_in_classifier): Dropout(p=0.4, inplace=False)\n",
       "  (Relu): ReLU()\n",
       "  (Sigmoid): Sigmoid()\n",
       "  (linear0): Linear(in_features=300, out_features=675, bias=True)\n",
       "  (linear1): Linear(in_features=675, out_features=135, bias=True)\n",
       "  (linear2): Linear(in_features=135, out_features=27, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#note that parameter EN is only used for tracking on which dataset was/is trained, and activate loading of the pretrained head\n",
    "#load the fine-tuned model over english\n",
    "PATH = \"/media/mv/Volume/Download/TEST_EXP/nlp2022-hw2-main-master/saved/model_2022_12_19_15_25_3.pth\"\n",
    "model = Arg_Classifier(\"ES\",cfg)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.train().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs n. 0\n",
      "F1 train: [0.         0.06857143 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.43322476 0.\n",
      " 0.         0.9834875  0.         0.         0.         0.04166667\n",
      " 0.         0.         0.44676806 0.         0.         0.\n",
      " 0.04545455 0.        ]\n",
      "F1 avg train: 0.9492945394245353\n",
      "identification {'true_positives': 932, 'false_positives': 224, 'false_negatives': 1222, 'precision': 0.8062283737024222, 'recall': 0.43268337975858867, 'f1': 0.5631419939577039}\n",
      "classification_result {'true_positives': 517, 'false_positives': 639, 'false_negatives': 1637, 'precision': 0.4472318339100346, 'recall': 0.24001857010213556, 'f1': 0.31238670694864046}\n",
      "EPOCHS : 0\n",
      "F1 eval : [0.         0.01474926 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.51171393 0.         0.\n",
      " 0.98792859 0.         0.         0.         0.         0.\n",
      " 0.         0.56926316 0.         0.         0.         0.\n",
      " 0.03058104 0.        ]\n",
      "F1 avg eval : 0.9593153965026872\n",
      "identification {'true_positives': 2520, 'false_positives': 304, 'false_negatives': 2377, 'precision': 0.8923512747875354, 'recall': 0.5146007759852971, 'f1': 0.6527651858567541}\n",
      "classification_result {'true_positives': 1516, 'false_positives': 1308, 'false_negatives': 3381, 'precision': 0.5368271954674221, 'recall': 0.30957729221972635, 'f1': 0.39269524672969824}\n",
      "SAVED : saved/model_2022_12_19_15_43_47.pth\n",
      "Epochs n. 1\n",
      "F1 train: [0.         0.04487179 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.49515287 0.\n",
      " 0.03960396 0.98669401 0.         0.         0.         0.\n",
      " 0.05405405 0.         0.62367491 0.         0.         0.\n",
      " 0.16296296 0.        ]\n",
      "F1 avg train: 0.9560521205196473\n",
      "identification {'true_positives': 1142, 'false_positives': 153, 'false_negatives': 1015, 'precision': 0.8818532818532818, 'recall': 0.5294390356977283, 'f1': 0.6616454229432214}\n",
      "classification_result {'true_positives': 706, 'false_positives': 589, 'false_negatives': 1451, 'precision': 0.5451737451737452, 'recall': 0.3273064441353732, 'f1': 0.40903823870220163}\n",
      "EPOCHS : 1\n",
      "F1 eval : [0.         0.05633803 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.53375196 0.         0.23893805\n",
      " 0.98906397 0.         0.         0.         0.         0.\n",
      " 0.         0.66339992 0.         0.         0.         0.\n",
      " 0.23661972 0.        ]\n",
      "F1 avg eval : 0.9630999535978669\n",
      "identification {'true_positives': 2802, 'false_positives': 342, 'false_negatives': 2076, 'precision': 0.8912213740458015, 'recall': 0.5744157441574416, 'f1': 0.6985789080029918}\n",
      "classification_result {'true_positives': 1819, 'false_positives': 1325, 'false_negatives': 3059, 'precision': 0.5785623409669212, 'recall': 0.3728987289872899, 'f1': 0.45350286711543264}\n",
      "SAVED : saved/model_2022_12_19_15_43_47.pth\n",
      "Epochs n. 2\n",
      "F1 train: [0.         0.1260745  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.51880276 0.\n",
      " 0.18018018 0.9877483  0.         0.         0.06896552 0.08695652\n",
      " 0.         0.01639344 0.66898349 0.         0.         0.\n",
      " 0.16438356 0.        ]\n",
      "F1 avg train: 0.9587598124977363\n",
      "identification {'true_positives': 1220, 'false_positives': 145, 'false_negatives': 914, 'precision': 0.8937728937728938, 'recall': 0.5716963448922212, 'f1': 0.6973420977422121}\n",
      "classification_result {'true_positives': 771, 'false_positives': 594, 'false_negatives': 1363, 'precision': 0.5648351648351648, 'recall': 0.3612933458294283, 'f1': 0.4406973420977422}\n",
      "EPOCHS : 2\n",
      "F1 eval : [0.         0.18030842 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.51496218 0.         0.23684211\n",
      " 0.98977735 0.         0.         0.         0.         0.\n",
      " 0.12048193 0.67692308 0.         0.         0.         0.\n",
      " 0.44343891 0.        ]\n",
      "F1 avg eval : 0.9653840914463983\n",
      "identification {'true_positives': 2913, 'false_positives': 299, 'false_negatives': 1976, 'precision': 0.9069115815691158, 'recall': 0.5958273675598282, 'f1': 0.7191704727811382}\n",
      "classification_result {'true_positives': 1884, 'false_positives': 1328, 'false_negatives': 3005, 'precision': 0.5865504358655044, 'recall': 0.3853548782982205, 'f1': 0.46512776200469086}\n",
      "SAVED : saved/model_2022_12_19_15_43_47.pth\n",
      "Epochs n. 3\n",
      "F1 train: [0.         0.12707182 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.52977099 0.\n",
      " 0.14678899 0.98865647 0.         0.         0.         0.05\n",
      " 0.05263158 0.23776224 0.69881557 0.         0.         0.\n",
      " 0.33532934 0.        ]\n",
      "F1 avg train: 0.9614860164480786\n",
      "identification {'true_positives': 1291, 'false_positives': 143, 'false_negatives': 857, 'precision': 0.900278940027894, 'recall': 0.601024208566108, 'f1': 0.7208263539921831}\n",
      "classification_result {'true_positives': 838, 'false_positives': 596, 'false_negatives': 1310, 'precision': 0.5843793584379359, 'recall': 0.39013035381750466, 'f1': 0.4678950307091011}\n",
      "EPOCHS : 3\n",
      "F1 eval : [0.         0.09269663 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.54641089 0.         0.41134752\n",
      " 0.99001122 0.         0.         0.         0.         0.\n",
      " 0.17714286 0.69167905 0.         0.         0.         0.\n",
      " 0.30526316 0.        ]\n",
      "F1 avg eval : 0.9656290042429252\n",
      "identification {'true_positives': 3022, 'false_positives': 333, 'false_negatives': 1884, 'precision': 0.9007451564828614, 'recall': 0.6159804321239298, 'f1': 0.7316305532017915}\n",
      "classification_result {'true_positives': 1994, 'false_positives': 1361, 'false_negatives': 2912, 'precision': 0.5943368107302534, 'recall': 0.40644109253974725, 'f1': 0.48275027236412055}\n",
      "SAVED : saved/model_2022_12_19_15_43_47.pth\n",
      "Epochs n. 4\n",
      "F1 train: [0.         0.26506024 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.54602675 0.\n",
      " 0.33587786 0.98898928 0.         0.         0.33333333 0.18604651\n",
      " 0.17777778 0.32098765 0.71099744 0.         0.         0.\n",
      " 0.28571429 0.        ]\n",
      "F1 avg train: 0.9638150633855087\n",
      "identification {'true_positives': 1331, 'false_positives': 145, 'false_negatives': 825, 'precision': 0.9017615176151762, 'recall': 0.6173469387755102, 'f1': 0.7329295154185022}\n",
      "classification_result {'true_positives': 902, 'false_positives': 574, 'false_negatives': 1254, 'precision': 0.6111111111111112, 'recall': 0.41836734693877553, 'f1': 0.49669603524229083}\n",
      "EPOCHS : 4\n",
      "F1 eval : [0.         0.23195266 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.53813968 0.         0.43835616\n",
      " 0.99035729 0.         0.         0.08       0.02777778 0.\n",
      " 0.29230769 0.70888974 0.         0.         0.         0.\n",
      " 0.39702233 0.        ]\n",
      "F1 avg eval : 0.9674899849998956\n",
      "identification {'true_positives': 3074, 'false_positives': 316, 'false_negatives': 1808, 'precision': 0.9067846607669616, 'recall': 0.6296599754199099, 'f1': 0.7432301740812379}\n",
      "classification_result {'true_positives': 2067, 'false_positives': 1323, 'false_negatives': 2815, 'precision': 0.6097345132743363, 'recall': 0.4233920524375256, 'f1': 0.49975822050290136}\n",
      "SAVED : saved/model_2022_12_19_15_43_47.pth\n",
      "Epochs n. 5\n",
      "F1 train: [0.         0.35652174 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.53135314 0.\n",
      " 0.32061069 0.98921559 0.         0.         0.25       0.35714286\n",
      " 0.37209302 0.28742515 0.74049028 0.         0.         0.\n",
      " 0.3583815  0.        ]\n",
      "F1 avg train: 0.9650298208947482\n",
      "identification {'true_positives': 1362, 'false_positives': 150, 'false_negatives': 793, 'precision': 0.9007936507936508, 'recall': 0.6320185614849188, 'f1': 0.7428415598581946}\n",
      "classification_result {'true_positives': 940, 'false_positives': 572, 'false_negatives': 1215, 'precision': 0.6216931216931217, 'recall': 0.4361948955916473, 'f1': 0.5126806653940551}\n",
      "EPOCHS : 5\n",
      "F1 eval : [0.         0.28806584 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.53364738 0.         0.43278689\n",
      " 0.99034448 0.         0.         0.04       0.02666667 0.\n",
      " 0.25520833 0.7063922  0.         0.         0.         0.\n",
      " 0.4195122  0.        ]\n",
      "F1 avg eval : 0.9677381084824558\n",
      "identification {'true_positives': 3147, 'false_positives': 394, 'false_negatives': 1748, 'precision': 0.8887319966111268, 'recall': 0.6429009193054137, 'f1': 0.7460881934566145}\n",
      "classification_result {'true_positives': 2114, 'false_positives': 1427, 'false_negatives': 2781, 'precision': 0.5970064953402994, 'recall': 0.43186925434116447, 'f1': 0.5011853959222381}\n",
      "SAVED : saved/model_2022_12_19_15_43_47.pth\n",
      "Epochs n. 6\n",
      "F1 train: [0.         0.39308855 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.57708162 0.\n",
      " 0.37313433 0.98982221 0.         0.         0.35294118 0.26923077\n",
      " 0.18867925 0.3699422  0.74936709 0.         0.         0.\n",
      " 0.41176471 0.        ]\n",
      "F1 avg train: 0.9669054810260753\n",
      "identification {'true_positives': 1402, 'false_positives': 153, 'false_negatives': 728, 'precision': 0.9016077170418006, 'recall': 0.6582159624413145, 'f1': 0.760922659430122}\n",
      "classification_result {'true_positives': 995, 'false_positives': 560, 'false_negatives': 1135, 'precision': 0.639871382636656, 'recall': 0.4671361502347418, 'f1': 0.5400271370420624}\n",
      "EPOCHS : 6\n",
      "F1 eval : [0.         0.28819068 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.55555556 0.         0.44936709\n",
      " 0.99056446 0.         0.         0.07843137 0.05194805 0.\n",
      " 0.30695444 0.70076726 0.         0.         0.         0.\n",
      " 0.41849148 0.        ]\n",
      "F1 avg eval : 0.9682914580165815\n",
      "identification {'true_positives': 3221, 'false_positives': 421, 'false_negatives': 1662, 'precision': 0.884404173531027, 'recall': 0.6596354699979521, 'f1': 0.7556598240469208}\n",
      "classification_result {'true_positives': 2177, 'false_positives': 1465, 'false_negatives': 2706, 'precision': 0.5977484898407468, 'recall': 0.44583248003276676, 'f1': 0.5107331378299121}\n",
      "SAVED : saved/model_2022_12_19_15_43_47.pth\n",
      "Epochs n. 7\n",
      "F1 train: [0.         0.4371134  0.04166667 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.58512397 0.\n",
      " 0.48684211 0.99065827 0.         0.         0.26666667 0.41935484\n",
      " 0.37288136 0.44565217 0.78093645 0.         0.         0.\n",
      " 0.42352941 0.        ]\n",
      "F1 avg train: 0.9692693539843206\n",
      "identification {'true_positives': 1472, 'false_positives': 140, 'false_negatives': 676, 'precision': 0.913151364764268, 'recall': 0.6852886405959032, 'f1': 0.7829787234042553}\n",
      "classification_result {'true_positives': 1070, 'false_positives': 542, 'false_negatives': 1078, 'precision': 0.6637717121588089, 'recall': 0.49813780260707635, 'f1': 0.5691489361702128}\n",
      "Early stopping at epoch :  7\n",
      "F1 eval : [0.         0.40221694 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.52036199 0.         0.43558282\n",
      " 0.99021366 0.         0.         0.07017544 0.02739726 0.05194805\n",
      " 0.32618026 0.70820896 0.         0.         0.         0.\n",
      " 0.50107991 0.        ]\n",
      "F1 avg eval : 0.9684451747560112\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "currentDateAndTime = datetime.now()\n",
    "_id = str(currentDateAndTime.year)+\"_\"+str(currentDateAndTime.month)+\"_\"+str(currentDateAndTime.day)+\"_\"+str(currentDateAndTime.hour)+\"_\"+str(currentDateAndTime.minute)+\"_\"+str(currentDateAndTime.second)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#optimizer = torch.optim.Adam(model.parameters(),lr = 0.000005)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "\n",
    "logSotfMax = torch.nn.LogSoftmax(dim=1)\n",
    "nll_loss = torch.nn.NLLLoss()\n",
    "\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "dataloader_dev = DataLoader(dev_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "mapping = dataloader_train.dataset.args_roles\n",
    "\n",
    "auto_model.eval()\n",
    "auto_model.cuda()\n",
    "\n",
    "EPOCHS = 200\n",
    "patience_counter = 0\n",
    "patience = 5\n",
    "max_val_loss = 9999\n",
    "f1_score_max = 0\n",
    "output_path = \"saved\"\n",
    "model_name = \"model_\"+_id+\".pth\"\n",
    "PATH = os.path.join(output_path,model_name)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    #TRAINING\n",
    "    p = []\n",
    "    g = []\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_train):\n",
    "        #print(sample_batched)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "       \n",
    "        #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "        #------------------FILTERING SUB-WORDS----------------------\n",
    "        subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "        word_emebedding = []\n",
    "        for i in range(n):\n",
    "            subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "            flag = subtoken_mask[0,i,0]\n",
    "            if flag :\n",
    "                continue\n",
    "            else :\n",
    "                word_emebedding.append(subwords_embedding)\n",
    "        word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "        #-------------------------FORWARD/BACKWARD----------------------------------\n",
    "        x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "            perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "            predicate_index = sample_batched[\"predicate_index\"],\n",
    "            pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "            predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])        \n",
    "        b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "        loss = nll_loss(logSotfMax(x),gt)\n",
    "        total_loss = total_loss + loss\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "        counter += 1 \n",
    "            \n",
    "\n",
    "        #-------------------------RESULT STORING----------------------------------\n",
    "        predicted = torch.argmax(x, dim=1)\n",
    "        p += predicted.tolist()\n",
    "        g += gt.tolist()\n",
    "    \n",
    "\n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    scheduler.step()\n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    print(\"Epochs n.\", epoch)\n",
    "    print(\"F1 train:\",f1)\n",
    "    print(\"F1 avg train:\",f1_avg)\n",
    "    \n",
    "    avg_train_loss = total_loss/counter\n",
    "    writer.add_scalar(\"EN_Loss_ES/train\", avg_train_loss, epoch)\n",
    "\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"EN_Train_ES/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"EN_Train_ES/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "    #EVALUATION\n",
    "    p = []\n",
    "    g = []\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_dev):\n",
    "    \n",
    "      #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "            #------------------FILTERING SUB-WORDS----------------------\n",
    "            subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "            word_emebedding = []\n",
    "            for i in range(n): \n",
    "                subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "                flag = subtoken_mask[0,i,0]\n",
    "                if flag :\n",
    "                    continue\n",
    "                else :\n",
    "                    word_emebedding.append(subwords_embedding)\n",
    "            word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "            #-------------------------FORWARD----------------------------------\n",
    "            x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "                        perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "                        predicate_index = sample_batched[\"predicate_index\"],\n",
    "                        pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "                        predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])   \n",
    "\n",
    "\n",
    "            b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "            loss = nll_loss(logSotfMax(x),gt)\n",
    "            total_loss = total_loss + loss\n",
    "            #-------------------------RESULT STORING----------------------------------\n",
    "            predicted = torch.argmax(x, dim=1)\n",
    "            p += predicted.tolist()\n",
    "            g += gt.tolist()\n",
    "            counter += 1 \n",
    "    \n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    avg_eval_loss = total_loss/counter\n",
    "\n",
    "    if avg_eval_loss < max_val_loss:\n",
    "        max_val_loss = avg_eval_loss\n",
    "    else :\n",
    "        patience_counter += 1\n",
    "    \n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    if patience_counter >= patience :  \n",
    "\n",
    "\n",
    "        print(\"Early stopping at epoch : \",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 avg eval :\",f1_avg)\n",
    "        break\n",
    "    else :\n",
    "        print(\"EPOCHS :\",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 avg eval :\",f1_avg)\n",
    "    \n",
    "\n",
    "    writer.add_scalar(\"EN_Loss_ES/validation\", avg_eval_loss, epoch)\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"EN_Eval_ES/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"EN_Eval_ES/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if f1_avg > f1_score_max:\n",
    "        f1_score_max = f1_avg\n",
    "        print(\"SAVED :\",PATH)\n",
    "        torch.save(model.state_dict(),PATH)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare without Transfert Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs n. 0\n",
      "F1 train: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.17606602 0.\n",
      " 0.         0.97629743 0.         0.         0.         0.\n",
      " 0.         0.         0.020558   0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9326034676010221\n",
      "identification {'true_positives': 157, 'false_positives': 106, 'false_negatives': 1994, 'precision': 0.596958174904943, 'recall': 0.07298930729893073, 'f1': 0.1300745650372825}\n",
      "classification_result {'true_positives': 71, 'false_positives': 192, 'false_negatives': 2080, 'precision': 0.26996197718631176, 'recall': 0.03300790330079033, 'f1': 0.0588235294117647}\n",
      "EPOCHS : 0\n",
      "F1 eval : [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.4334126  0.         0.\n",
      " 0.98445385 0.         0.         0.         0.         0.\n",
      " 0.         0.12718204 0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9491122608744599\n",
      "identification {'true_positives': 1795, 'false_positives': 361, 'false_negatives': 3080, 'precision': 0.8325602968460112, 'recall': 0.3682051282051282, 'f1': 0.5105959322998151}\n",
      "classification_result {'true_positives': 831, 'false_positives': 1325, 'false_negatives': 4044, 'precision': 0.3854359925788497, 'recall': 0.17046153846153847, 'f1': 0.2363817380173517}\n",
      "SAVED : saved/model_2022_12_19_15_46_45WT.pth\n",
      "Epochs n. 1\n",
      "F1 train: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.37237762 0.\n",
      " 0.         0.98162789 0.         0.         0.         0.\n",
      " 0.         0.         0.45136922 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9464986838059342\n",
      "identification {'true_positives': 757, 'false_positives': 228, 'false_negatives': 1394, 'precision': 0.7685279187817259, 'recall': 0.3519293351929335, 'f1': 0.48278061224489804}\n",
      "classification_result {'true_positives': 452, 'false_positives': 533, 'false_negatives': 1699, 'precision': 0.4588832487309645, 'recall': 0.2101348210134821, 'f1': 0.288265306122449}\n",
      "EPOCHS : 1\n",
      "F1 eval : [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.44625316 0.         0.\n",
      " 0.98687232 0.         0.         0.         0.         0.\n",
      " 0.         0.5125399  0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.956798159637657\n",
      "identification {'true_positives': 2450, 'false_positives': 487, 'false_negatives': 2434, 'precision': 0.8341845420497106, 'recall': 0.5016380016380017, 'f1': 0.6265183480373354}\n",
      "classification_result {'true_positives': 1357, 'false_positives': 1580, 'false_negatives': 3527, 'precision': 0.4620360912495744, 'recall': 0.27784602784602785, 'f1': 0.34701444828027106}\n",
      "SAVED : saved/model_2022_12_19_15_46_45WT.pth\n",
      "Epochs n. 2\n",
      "F1 train: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.42304657 0.\n",
      " 0.         0.98405717 0.         0.         0.         0.\n",
      " 0.         0.         0.59271523 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9511966030647121\n",
      "identification {'true_positives': 1009, 'false_positives': 251, 'false_negatives': 1141, 'precision': 0.8007936507936508, 'recall': 0.46930232558139534, 'f1': 0.5917888563049853}\n",
      "classification_result {'true_positives': 626, 'false_positives': 634, 'false_negatives': 1524, 'precision': 0.49682539682539684, 'recall': 0.2911627906976744, 'f1': 0.3671554252199414}\n",
      "EPOCHS : 2\n",
      "F1 eval : [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.50244886 0.         0.\n",
      " 0.98871696 0.         0.         0.         0.         0.\n",
      " 0.         0.62439418 0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9607667700722805\n",
      "identification {'true_positives': 2745, 'false_positives': 376, 'false_negatives': 2144, 'precision': 0.8795257930150593, 'recall': 0.5614645121701779, 'f1': 0.6853932584269662}\n",
      "classification_result {'true_positives': 1645, 'false_positives': 1476, 'false_negatives': 3244, 'precision': 0.5270746555591157, 'recall': 0.33646962569032524, 'f1': 0.4107365792759052}\n",
      "SAVED : saved/model_2022_12_19_15_46_45WT.pth\n",
      "Epochs n. 3\n",
      "F1 train: [0.         0.00673401 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.47250194 0.\n",
      " 0.         0.98621672 0.         0.         0.         0.\n",
      " 0.         0.         0.65463495 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9552013584210328\n",
      "identification {'true_positives': 1115, 'false_positives': 189, 'false_negatives': 1020, 'precision': 0.8550613496932515, 'recall': 0.522248243559719, 'f1': 0.6484443152079092}\n",
      "classification_result {'true_positives': 705, 'false_positives': 599, 'false_negatives': 1430, 'precision': 0.5406441717791411, 'recall': 0.33021077283372363, 'f1': 0.4100029078220413}\n",
      "EPOCHS : 3\n",
      "F1 eval : [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.52492492 0.         0.\n",
      " 0.98865671 0.         0.         0.         0.         0.\n",
      " 0.         0.65524781 0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.961047227094713\n",
      "identification {'true_positives': 2820, 'false_positives': 434, 'false_negatives': 2069, 'precision': 0.8666256914566687, 'recall': 0.5768050726119861, 'f1': 0.692619427729338}\n",
      "classification_result {'true_positives': 1773, 'false_positives': 1481, 'false_negatives': 3116, 'precision': 0.5448678549477566, 'recall': 0.36265084884434445, 'f1': 0.4354660444553604}\n",
      "SAVED : saved/model_2022_12_19_15_46_45WT.pth\n",
      "Epochs n. 4\n",
      "F1 train: [0.         0.04458599 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.496997   0.\n",
      " 0.         0.98690944 0.         0.         0.         0.\n",
      " 0.         0.         0.69745223 0.         0.         0.\n",
      " 0.03278689 0.        ]\n",
      "F1 avg train: 0.9568220490755146\n",
      "identification {'true_positives': 1199, 'false_positives': 190, 'false_negatives': 956, 'precision': 0.86321094312455, 'recall': 0.5563805104408353, 'f1': 0.6766365688487584}\n",
      "classification_result {'true_positives': 778, 'false_positives': 611, 'false_negatives': 1377, 'precision': 0.5601151907847373, 'recall': 0.36102088167053364, 'f1': 0.43905191873589167}\n",
      "EPOCHS : 4\n",
      "F1 eval : [0.         0.12724758 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.52681091 0.         0.\n",
      " 0.98877624 0.         0.         0.         0.         0.\n",
      " 0.1994302  0.62095238 0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9620486244446943\n",
      "identification {'true_positives': 2761, 'false_positives': 368, 'false_negatives': 2114, 'precision': 0.8823905401086609, 'recall': 0.5663589743589743, 'f1': 0.6899050474762618}\n",
      "classification_result {'true_positives': 1736, 'false_positives': 1393, 'false_negatives': 3139, 'precision': 0.5548098434004475, 'recall': 0.3561025641025641, 'f1': 0.43378310844577717}\n",
      "SAVED : saved/model_2022_12_19_15_46_45WT.pth\n",
      "Epochs n. 5\n",
      "F1 train: [0.         0.10465116 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.50687023 0.\n",
      " 0.13333333 0.9878918  0.         0.         0.         0.\n",
      " 0.         0.22360248 0.73070866 0.         0.         0.\n",
      " 0.07874016 0.        ]\n",
      "F1 avg train: 0.9598061645717502\n",
      "identification {'true_positives': 1282, 'false_positives': 197, 'false_negatives': 858, 'precision': 0.8668018931710615, 'recall': 0.5990654205607476, 'f1': 0.7084830063553468}\n",
      "classification_result {'true_positives': 844, 'false_positives': 635, 'false_negatives': 1296, 'precision': 0.5706558485463151, 'recall': 0.394392523364486, 'f1': 0.4664271898314451}\n",
      "EPOCHS : 5\n",
      "F1 eval : [0.         0.13746631 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.53320743 0.         0.17757009\n",
      " 0.98958928 0.         0.         0.         0.         0.\n",
      " 0.30170316 0.63988095 0.         0.         0.         0.\n",
      " 0.14857143 0.        ]\n",
      "F1 avg eval : 0.9642106494627098\n",
      "identification {'true_positives': 2946, 'false_positives': 377, 'false_negatives': 1919, 'precision': 0.8865482997291604, 'recall': 0.6055498458376156, 'f1': 0.719589643380557}\n",
      "classification_result {'true_positives': 1865, 'false_positives': 1458, 'false_negatives': 3000, 'precision': 0.5612398435148962, 'recall': 0.38335046248715315, 'f1': 0.4555446995603322}\n",
      "SAVED : saved/model_2022_12_19_15_46_45WT.pth\n",
      "Epochs n. 6\n",
      "F1 train: [0.         0.14130435 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.52245509 0.\n",
      " 0.12173913 0.98921672 0.         0.         0.         0.\n",
      " 0.         0.36871508 0.7476489  0.         0.         0.\n",
      " 0.2027027  0.        ]\n",
      "F1 avg train: 0.962424113115842\n",
      "identification {'true_positives': 1391, 'false_positives': 188, 'false_negatives': 752, 'precision': 0.8809373020899304, 'recall': 0.6490900606626225, 'f1': 0.7474476088124664}\n",
      "classification_result {'true_positives': 907, 'false_positives': 672, 'false_negatives': 1236, 'precision': 0.5744141861937936, 'recall': 0.42323845076994865, 'f1': 0.48737238044062337}\n",
      "EPOCHS : 6\n",
      "F1 eval : [0.         0.12980132 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.53291536 0.         0.22624434\n",
      " 0.99017995 0.         0.         0.         0.         0.\n",
      " 0.32727273 0.67338241 0.         0.         0.         0.\n",
      " 0.17241379 0.        ]\n",
      "F1 avg eval : 0.9653579129244246\n",
      "identification {'true_positives': 3147, 'false_positives': 435, 'false_negatives': 1732, 'precision': 0.8785594639865997, 'recall': 0.6450092232014757, 'f1': 0.7438837016901076}\n",
      "classification_result {'true_positives': 1980, 'false_positives': 1602, 'false_negatives': 2899, 'precision': 0.5527638190954773, 'recall': 0.4058208649313384, 'f1': 0.468029783713509}\n",
      "SAVED : saved/model_2022_12_19_15_46_45WT.pth\n",
      "Epochs n. 7\n",
      "F1 train: [0.         0.25358852 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.54813665 0.\n",
      " 0.21666667 0.98998556 0.         0.         0.         0.\n",
      " 0.         0.36548223 0.78713629 0.         0.         0.\n",
      " 0.29761905 0.        ]\n",
      "F1 avg train: 0.9651852556529101\n",
      "identification {'true_positives': 1464, 'false_positives': 180, 'false_negatives': 701, 'precision': 0.8905109489051095, 'recall': 0.6762124711316397, 'f1': 0.768705697033342}\n",
      "classification_result {'true_positives': 994, 'false_positives': 650, 'false_negatives': 1171, 'precision': 0.6046228710462287, 'recall': 0.4591224018475751, 'f1': 0.5219217642425834}\n",
      "EPOCHS : 7\n",
      "F1 eval : [0.         0.18411097 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.53446115 0.         0.25514403\n",
      " 0.98985055 0.         0.         0.         0.         0.\n",
      " 0.3411215  0.66666667 0.         0.         0.         0.\n",
      " 0.23463687 0.        ]\n",
      "F1 avg eval : 0.9655376391929702\n",
      "identification {'true_positives': 2987, 'false_positives': 356, 'false_negatives': 1883, 'precision': 0.8935088244092133, 'recall': 0.613347022587269, 'f1': 0.7273834165347619}\n",
      "classification_result {'true_positives': 1937, 'false_positives': 1406, 'false_negatives': 2933, 'precision': 0.5794196829195334, 'recall': 0.397741273100616, 'f1': 0.4716912212346281}\n",
      "SAVED : saved/model_2022_12_19_15_46_45WT.pth\n",
      "Epochs n. 8\n",
      "F1 train: [0.         0.3359375  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.55980472 0.\n",
      " 0.37762238 0.99119752 0.         0.         0.         0.24\n",
      " 0.         0.41025641 0.81418658 0.         0.         0.\n",
      " 0.40437158 0.        ]\n",
      "F1 avg train: 0.9684429583499209\n",
      "identification {'true_positives': 1551, 'false_positives': 169, 'false_negatives': 602, 'precision': 0.9017441860465116, 'recall': 0.7203901532745007, 'f1': 0.8009295120061968}\n",
      "classification_result {'true_positives': 1068, 'false_positives': 652, 'false_negatives': 1085, 'precision': 0.6209302325581395, 'recall': 0.49605202043660007, 'f1': 0.5515104570100696}\n",
      "Early stopping at epoch :  8\n",
      "F1 eval : [0.         0.18858561 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.54355401 0.         0.38513514\n",
      " 0.98989465 0.         0.         0.         0.         0.\n",
      " 0.30523918 0.67118886 0.         0.         0.         0.\n",
      " 0.325      0.        ]\n",
      "F1 avg eval : 0.9662967881783213\n"
     ]
    }
   ],
   "source": [
    "model = Arg_Classifier(\"ES\",cfg).cuda()\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "currentDateAndTime = datetime.now()\n",
    "_id = str(currentDateAndTime.year)+\"_\"+str(currentDateAndTime.month)+\"_\"+str(currentDateAndTime.day)+\"_\"+str(currentDateAndTime.hour)+\"_\"+str(currentDateAndTime.minute)+\"_\"+str(currentDateAndTime.second)+\"WT\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#optimizer = torch.optim.Adam(model.parameters(),lr = 0.000005)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "\n",
    "logSotfMax = torch.nn.LogSoftmax(dim=1)\n",
    "nll_loss = torch.nn.NLLLoss()\n",
    "\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "dataloader_dev = DataLoader(dev_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "mapping = dataloader_train.dataset.args_roles\n",
    "\n",
    "auto_model.eval()\n",
    "auto_model.cuda()\n",
    "\n",
    "EPOCHS = 200\n",
    "patience_counter = 0\n",
    "patience = 5\n",
    "max_val_loss = 9999\n",
    "f1_score_max = 0\n",
    "output_path = \"saved\"\n",
    "model_name = \"model_\"+_id+\".pth\"\n",
    "PATH = os.path.join(output_path,model_name)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    #TRAINING\n",
    "    p = []\n",
    "    g = []\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_train):\n",
    "        #print(sample_batched)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "       \n",
    "        #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "        #------------------FILTERING SUB-WORDS----------------------\n",
    "        subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "        word_emebedding = []\n",
    "        for i in range(n):\n",
    "            subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "            flag = subtoken_mask[0,i,0]\n",
    "            if flag :\n",
    "                continue\n",
    "            else :\n",
    "                word_emebedding.append(subwords_embedding)\n",
    "        word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "        #-------------------------FORWARD/BACKWARD----------------------------------\n",
    "        x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "            perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "            predicate_index = sample_batched[\"predicate_index\"],\n",
    "            pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "            predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])        \n",
    "        b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "        loss = nll_loss(logSotfMax(x),gt)\n",
    "        total_loss = total_loss + loss\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "        counter += 1 \n",
    "            \n",
    "\n",
    "        #-------------------------RESULT STORING----------------------------------\n",
    "        predicted = torch.argmax(x, dim=1)\n",
    "        p += predicted.tolist()\n",
    "        g += gt.tolist()\n",
    "    \n",
    "\n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    scheduler.step()\n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    print(\"Epochs n.\", epoch)\n",
    "    print(\"F1 train:\",f1)\n",
    "    print(\"F1 avg train:\",f1_avg)\n",
    "    \n",
    "    avg_train_loss = total_loss/counter\n",
    "    writer.add_scalar(\"Loss_ES/train\", avg_train_loss, epoch)\n",
    "\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"Train_ES/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"Train_ES/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "    #EVALUATION\n",
    "    p = []\n",
    "    g = []\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_dev):\n",
    "    \n",
    "      #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "            #------------------FILTERING SUB-WORDS----------------------\n",
    "            subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "            word_emebedding = []\n",
    "            for i in range(n): \n",
    "                subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "                flag = subtoken_mask[0,i,0]\n",
    "                if flag :\n",
    "                    continue\n",
    "                else :\n",
    "                    word_emebedding.append(subwords_embedding)\n",
    "            word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "            #-------------------------FORWARD----------------------------------\n",
    "            x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "                        perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "                        predicate_index = sample_batched[\"predicate_index\"],\n",
    "                        pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "                        predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])   \n",
    "\n",
    "\n",
    "            b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "            loss = nll_loss(logSotfMax(x),gt)\n",
    "            total_loss = total_loss + loss\n",
    "            #-------------------------RESULT STORING----------------------------------\n",
    "            predicted = torch.argmax(x, dim=1)\n",
    "            p += predicted.tolist()\n",
    "            g += gt.tolist()\n",
    "            counter += 1 \n",
    "    \n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    avg_eval_loss = total_loss/counter\n",
    "\n",
    "    if avg_eval_loss < max_val_loss:\n",
    "        max_val_loss = avg_eval_loss\n",
    "    else :\n",
    "        patience_counter += 1\n",
    "    \n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    if patience_counter >= patience :  \n",
    "\n",
    "\n",
    "        print(\"Early stopping at epoch : \",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 avg eval :\",f1_avg)\n",
    "        break\n",
    "    else :\n",
    "        print(\"EPOCHS :\",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 avg eval :\",f1_avg)\n",
    "    \n",
    "\n",
    "    writer.add_scalar(\"Loss_ES/validation\", avg_eval_loss, epoch)\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"Eval_ES/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"Eval_ES/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if f1_avg > f1_score_max:\n",
    "        f1_score_max = f1_avg\n",
    "        print(\"SAVED :\",PATH)\n",
    "        torch.save(model.state_dict(),PATH)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### French"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New French Dataset\n",
    "bert-base-multilingual-cased\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load the configuration of 'bert-base-multili ngual-cased'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'bert-base-multili ngual-cased' is the correct path to a directory containing a config.json file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py:614\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=611'>612</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=612'>613</a>\u001b[0m     \u001b[39m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=613'>614</a>\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=614'>615</a>\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=615'>616</a>\u001b[0m         configuration_file,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=616'>617</a>\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=617'>618</a>\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=618'>619</a>\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=619'>620</a>\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=620'>621</a>\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=621'>622</a>\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=622'>623</a>\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=623'>624</a>\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=624'>625</a>\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=625'>626</a>\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=626'>627</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=627'>628</a>\u001b[0m     commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/utils/hub.py:409\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/utils/hub.py?line=406'>407</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/utils/hub.py?line=407'>408</a>\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/utils/hub.py?line=408'>409</a>\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/utils/hub.py?line=409'>410</a>\u001b[0m         path_or_repo_id,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/utils/hub.py?line=410'>411</a>\u001b[0m         filename,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/utils/hub.py?line=411'>412</a>\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/utils/hub.py?line=412'>413</a>\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/utils/hub.py?line=413'>414</a>\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/utils/hub.py?line=414'>415</a>\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/utils/hub.py?line=415'>416</a>\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/utils/hub.py?line=416'>417</a>\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/utils/hub.py?line=417'>418</a>\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/utils/hub.py?line=418'>419</a>\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/utils/hub.py?line=419'>420</a>\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/utils/hub.py?line=420'>421</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/utils/hub.py?line=422'>423</a>\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py?line=112'>113</a>\u001b[0m \u001b[39mif\u001b[39;00m arg_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrepo_id\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py?line=113'>114</a>\u001b[0m     validate_repo_id(arg_value)\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py?line=115'>116</a>\u001b[0m \u001b[39melif\u001b[39;00m arg_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtoken\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m arg_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:172\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py?line=170'>171</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m REPO_ID_REGEX\u001b[39m.\u001b[39mmatch(repo_id):\n\u001b[0;32m--> <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py?line=171'>172</a>\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py?line=172'>173</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRepo id must use alphanumeric chars or \u001b[39m\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m--\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39m..\u001b[39m\u001b[39m'\u001b[39m\u001b[39m are\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py?line=173'>174</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m forbidden, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m cannot start or end the name, max length is 96:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py?line=174'>175</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py?line=175'>176</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py?line=177'>178</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m--\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m repo_id \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m..\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m repo_id:\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: 'bert-base-multili ngual-cased'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 12\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict\n\u001b[0;32m---> 12\u001b[0m auto_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbert-base-multili ngual-cased\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mmodel class is      : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(auto_model)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-multilingual-cased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:434\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py?line=431'>432</a>\u001b[0m hub_kwargs \u001b[39m=\u001b[39m {name: kwargs\u001b[39m.\u001b[39mpop(name) \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m hub_kwargs_names \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m kwargs}\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py?line=432'>433</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[0;32m--> <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py?line=433'>434</a>\u001b[0m     config, kwargs \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py?line=434'>435</a>\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py?line=435'>436</a>\u001b[0m         return_unused_kwargs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py?line=436'>437</a>\u001b[0m         trust_remote_code\u001b[39m=\u001b[39;49mtrust_remote_code,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py?line=437'>438</a>\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py?line=438'>439</a>\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py?line=439'>440</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py?line=440'>441</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(config, \u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m config\u001b[39m.\u001b[39mauto_map:\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py?line=441'>442</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py:776\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py?line=773'>774</a>\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mname_or_path\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m pretrained_model_name_or_path\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py?line=774'>775</a>\u001b[0m trust_remote_code \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtrust_remote_code\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py?line=775'>776</a>\u001b[0m config_dict, unused_kwargs \u001b[39m=\u001b[39m PretrainedConfig\u001b[39m.\u001b[39;49mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py?line=776'>777</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mAutoConfig\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py?line=777'>778</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py:559\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=556'>557</a>\u001b[0m original_kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=557'>558</a>\u001b[0m \u001b[39m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=558'>559</a>\u001b[0m config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=559'>560</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=560'>561</a>\u001b[0m     original_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config_dict[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py:635\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=631'>632</a>\u001b[0m         \u001b[39mraise\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=632'>633</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=633'>634</a>\u001b[0m         \u001b[39m# For any other exception, we throw a generic error.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=634'>635</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=635'>636</a>\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load the configuration of \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. If you were trying to load it\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=636'>637</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m from \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, make sure you don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt have a local directory with the same\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=637'>638</a>\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m name. Otherwise, make sure \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is the correct path to a directory\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=638'>639</a>\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m containing a \u001b[39m\u001b[39m{\u001b[39;00mconfiguration_file\u001b[39m}\u001b[39;00m\u001b[39m file\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=639'>640</a>\u001b[0m         )\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=641'>642</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=642'>643</a>\u001b[0m     \u001b[39m# Load config dict\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=643'>644</a>\u001b[0m     config_dict \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_dict_from_json_file(resolved_config_file)\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load the configuration of 'bert-base-multili ngual-cased'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'bert-base-multili ngual-cased' is the correct path to a directory containing a config.json file"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import random\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SRL(Dataset):\n",
    " \n",
    "    def __init__(self,language,path,args_roles = None) -> None:\n",
    "\n",
    "        self.path_root = 'data'\n",
    "        self.load_data(language,path)\n",
    "        if args_roles is None :\n",
    "            self.args_roles,self.list_broken_id = self.list_arg_roles()\n",
    "        else : \n",
    "            self.args_roles = args_roles\n",
    "            _,self.list_broken_id = self.list_arg_roles()\n",
    "\n",
    "        self.pos_list,_ = self.list_pos()\n",
    "        print(self.args_roles)\n",
    "        self.predicate_dis,_ = self.list_predicate_roles()\n",
    "        self.pos_list.append(\"Nothing\")\n",
    "        self.predicate_dis.append(\"Nothing\")\n",
    "\n",
    "    def load_data(self,language,mode):\n",
    "        \n",
    "        mode = mode+\".json\"\n",
    "        path = os.path.join(self.path_root,language,mode)\n",
    "        data_file = open(path)\n",
    "       \n",
    "        data_ = json.load(data_file)\n",
    "\n",
    "        list_data = []\n",
    "\n",
    "        for data in data_:\n",
    "            list_data.append(data_[data])\n",
    "        \n",
    "\n",
    "        self.data = list_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, id : int):\n",
    "\n",
    "        flag = False\n",
    "        if id in self.list_broken_id :\n",
    "            flag = True\n",
    "            while flag == True:\n",
    "\n",
    "                rand_id = random.randint(0, len(self.data)-1)\n",
    "                \n",
    "                if rand_id in self.list_broken_id :\n",
    "                    pass\n",
    "                else :\n",
    "                    flag = False\n",
    "                    id = rand_id        \n",
    "\n",
    "\n",
    "        data = self.pre_processing(self.data[id])\n",
    "        data = self.processig(data)\n",
    "        return data\n",
    "        \n",
    "    def pre_processing(self, data:dict):\n",
    "        data_list = []\n",
    "        for role in data[\"roles\"]:\n",
    "            dictionary = dict()\n",
    "            dictionary[\"words\"] = data[\"words\"]\n",
    "            dictionary[\"role\"] = data[\"roles\"][role]\n",
    "            dictionary[\"pre_idx\"] = role\n",
    "            dictionary[\"pos_tags\"] = data[\"pos_tags\"]\n",
    "            dictionary[\"predicate_meaning\"] = data[\"predicates\"]\n",
    "            data_list.append(dictionary)    \n",
    "        return data_list\n",
    "    \n",
    "    def processig(self,data_list:list):\n",
    "        \n",
    "        for dictionary in data_list:\n",
    "\n",
    "            #dictionary[\"words\"] = data[\"words\"]\n",
    "            dictionary[\"gt_arg_identification\"] = self.arg_id(dictionary[\"role\"])\n",
    "            dictionary[\"gt_arg_classification\"] = self.arg_class(dictionary[\"role\"])\n",
    "            dictionary[\"pos_idx\"] = self.pos_idx(dictionary[\"pos_tags\"])\n",
    "            dictionary[\"predicate_meaning_idx\"] = self.predicate_meaning_idx(dictionary[\"predicate_meaning\"])\n",
    "        \n",
    "        return data_list\n",
    "   \n",
    "    def list_arg_roles(self):\n",
    "        list_roles = []\n",
    "        list_broken_id = []\n",
    "        for i,element in enumerate(self.data):\n",
    "            flag = True\n",
    "            try : roles = element[\"roles\"]\n",
    "            except : flag = False\n",
    "            if flag :\n",
    "                for e in roles:\n",
    "                    sentence = element[\"roles\"][e]\n",
    "\n",
    "                    for word in sentence:\n",
    "                        \n",
    "                        list_roles.append(word)\n",
    "                list_roles = list(set(list_roles))\n",
    "            else : \n",
    "                list_broken_id.append(i)\n",
    "        return list_roles,list_broken_id\n",
    "\n",
    "    def list_predicate_roles(self):\n",
    "        list_predicate_roles = []\n",
    "        list_broken_id = []\n",
    "        for i,element in enumerate(self.data):\n",
    "            flag = True\n",
    "            try : predicates = element[\"predicates\"]\n",
    "            except : flag = False\n",
    "            if flag :\n",
    "                for pre in predicates:\n",
    "                    list_predicate_roles.append(pre)\n",
    "                list_predicate_roles = list(set(list_predicate_roles))\n",
    "            else : \n",
    "                list_broken_id.append(i)\n",
    "        return list_predicate_roles,list_broken_id\n",
    "\n",
    "    def list_pos(self):\n",
    "        list_pos = []\n",
    "        list_broken_id = []\n",
    "        for i,element in enumerate(self.data):\n",
    "            flag = True\n",
    "            try : pos = element[\"pos_tags\"]\n",
    "            except : flag = False\n",
    "            if flag :\n",
    "                for e in pos:\n",
    "                    list_pos.append(e)\n",
    "                list_pos = list(set(list_pos))\n",
    "            else : \n",
    "                list_broken_id.append(i)\n",
    "        return list_pos,list_broken_id\n",
    "  \n",
    "    def arg_class(self,role:list):\n",
    "        list_idxs = []\n",
    "        for element in role:\n",
    "            list_idxs.append(self.args_roles.index(element))\n",
    "        \n",
    "\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "\n",
    "    def arg_id(self,role:dict):\n",
    "        list_idxs = []\n",
    "        for element in role:\n",
    "            if element == \"_\":\n",
    "                list_idxs.append(0)\n",
    "            else :\n",
    "                list_idxs.append(1)\n",
    "\n",
    "        \n",
    "\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "\n",
    "    def pos_idx(self,pos_tags:dict):\n",
    "        list_idxs = []\n",
    "        list_idxs.append(self.pos_list.index(\"Nothing\"))\n",
    "\n",
    "        for element in pos_tags:\n",
    "            list_idxs.append(self.pos_list.index(element))\n",
    "        \n",
    "        list_idxs.append(self.pos_list.index(\"Nothing\"))\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "    \n",
    "    def predicate_meaning_idx(self,predicate_meaning_tags:dict):\n",
    "        list_idxs = []\n",
    "        list_idxs.append(self.predicate_dis.index(\"Nothing\"))\n",
    "\n",
    "        for element in predicate_meaning_tags:\n",
    "            list_idxs.append(self.predicate_dis.index(element))\n",
    "        \n",
    "        list_idxs.append(self.predicate_dis.index(\"Nothing\"))\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64) \n",
    "\n",
    "    \n",
    "# here we define our collate function\n",
    "def collate_fn(batch) -> Dict[str, torch.Tensor]:\n",
    "    #print(batch)\n",
    "    input = dict() \n",
    "    batch_sentence = [] \n",
    "    #print(len(batch))\n",
    "    for period in batch:\n",
    "        for sentence in period :\n",
    "        \n",
    "            #print(len(sentence[0][\"words\"]))\n",
    "            pre_idx = int(sentence[\"pre_idx\"])\n",
    "            \n",
    "\n",
    "            predicate = sentence[\"words\"][pre_idx]\n",
    "\n",
    "            text = \" \".join(sentence[\"words\"])\n",
    "            tokens: list[str] = text.split()\n",
    "            predicate: list[str] = predicate.split()\n",
    "\n",
    "            #text = sentence[0][\"words\"]\n",
    "            \n",
    "            t = (tokens,predicate)\n",
    "\n",
    "            batch_sentence.append(t)\n",
    "            #print(batch_sentence)\n",
    "\n",
    "    batch_output = tokenizer.batch_encode_plus(batch_sentence,padding=True,is_split_into_words=True, truncation=True,return_offsets_mapping=True, return_tensors=\"pt\")\n",
    "    #print(batch_output.keys())\n",
    "\n",
    "\n",
    "    gt = dict()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    for period in batch:\n",
    "\n",
    "        list_positional_predicate_encoding = []\n",
    "        list_arg_gt = []\n",
    "        list_predicate_index = [] \n",
    "        list_pos_index = [] \n",
    "        list_predicate_meaning_index = []\n",
    "\n",
    "        for sentence in period:\n",
    "            #positional_encoding\n",
    "            #+2 per il CLS iniziale ad SEP finale\n",
    "            sentence_words_lenght =  len(sentence[\"words\"])\n",
    "            positional_predicate_encoding = torch.zeros(1,sentence_words_lenght+2)\n",
    "            #+1 per il CLS iniziale\n",
    "            pre_idx = int(sentence[\"pre_idx\"])\n",
    "            positional_predicate_encoding[:,pre_idx+1] = 1\n",
    "            list_positional_predicate_encoding.append(positional_predicate_encoding)\n",
    "            #print(\"positional_prefix_encoding\",positional_predicate_encoding)\n",
    "            list_predicate_index.append(pre_idx)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            pos = torch.unsqueeze(sentence[\"pos_idx\"],dim = 0)\n",
    "            list_pos_index.append(pos)\n",
    "            predicate_meaning_idxs = torch.unsqueeze(sentence[\"predicate_meaning_idx\"],dim = 0)\n",
    "            list_predicate_meaning_index.append(predicate_meaning_idxs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #note CLS and SEP are discharder after Bi-LSTM, the Classifier takes in input only wokrds hidden state embedding\n",
    "            arg_gt = torch.unsqueeze(sentence[\"gt_arg_classification\"],dim = 0)\n",
    "            list_arg_gt.append(arg_gt)\n",
    "        \n",
    "\n",
    "    list_arg_gt = torch.cat(list_arg_gt,dim = 0)\n",
    "    list_pos_index = torch.cat(list_pos_index,dim = 0)\n",
    "    list_predicate_meaning_index = torch.cat(list_predicate_meaning_index,dim = 0)\n",
    "    list_positional_predicate_encoding = torch.cat(list_positional_predicate_encoding,dim = 0)\n",
    "    gt[\"arg_gt\"] = list_arg_gt\n",
    "    input[\"predicate_index\"] = list_predicate_index\n",
    "    input[\"pos_index\"] = list_pos_index.long()\n",
    "    input[\"predicate_meaning_idx\"] = list_predicate_meaning_index.long()\n",
    "    offset = batch_output.pop(\"offset_mapping\")\n",
    "    input[\"BERT_input\"] = batch_output\n",
    "    input[\"positional_encoding\"] = list_positional_predicate_encoding.long()\n",
    "    input[\"offset_mapping\"] = offset\n",
    "    input[\"gt\"] = gt\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    return\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes from 3 to 4 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#train_dataset = SRL(\"EN\",\"train\")\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#note here we are directly loading args_roles mapping as computed before the the dasetet where we have perfomerd \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#EN and ES dataset should have the same consistency in generating \u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mSRL\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFR\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs_roles\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#same mapping should be used in both the dataset\u001b[39;00m\n\u001b[1;32m      7\u001b[0m dev_dataset \u001b[38;5;241m=\u001b[39m SRL(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdev\u001b[39m\u001b[38;5;124m\"\u001b[39m,train_dataset\u001b[38;5;241m.\u001b[39margs_roles,train_dataset\u001b[38;5;241m.\u001b[39mpos_list)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes from 3 to 4 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "\n",
    "#train_dataset = SRL(\"EN\",\"train\")\n",
    "\n",
    "#note here we are directly loading args_roles mapping as computed before the the dasetet where we have perfomerd \n",
    "#EN and ES dataset should have the same consistency in generating \n",
    "train_dataset = SRL(\"FR\",\"train\",train_dataset.args_roles,train_dataset.pos_list)\n",
    "#same mapping should be used in both the dataset\n",
    "dev_dataset = SRL(\"FR\",\"dev\",train_dataset.args_roles,train_dataset.pos_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English-French attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Arg_Classifier(\n",
       "  (bi_lstm_portable): LSTM(132, 50, num_layers=2, batch_first=True, dropout=0.4, bidirectional=True)\n",
       "  (embedding_predicate_flag): Embedding(2, 32, max_norm=True)\n",
       "  (embedding_predicate): Embedding(304, False, max_norm=True)\n",
       "  (embedding_pos): Embedding(18, 100, max_norm=True)\n",
       "  (bi_lstm): LSTM(900, 50, num_layers=2, batch_first=True, dropout=0.4, bidirectional=True)\n",
       "  (dropout_language_constraint): Dropout(p=0.6, inplace=False)\n",
       "  (dropout_in_classifier): Dropout(p=0.4, inplace=False)\n",
       "  (Relu): ReLU()\n",
       "  (Sigmoid): Sigmoid()\n",
       "  (linear0): Linear(in_features=300, out_features=675, bias=True)\n",
       "  (linear1): Linear(in_features=675, out_features=135, bias=True)\n",
       "  (linear2): Linear(in_features=135, out_features=27, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#note that parameter EN is only used for tracking on which dataset was/is trained, and activate loading of the pretrained head\n",
    "#load the fine-tuned model over english\n",
    "PATH = \"saved/model_2022_12_18_21_33_45.pth\"\n",
    "model = Arg_Classifier(\"ES\",cfg)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.train().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs n. 0\n",
      "F1 train: [0.         0.41519926 0.00668896 0.98159286 0.         0.38427948\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9458893102533754\n",
      "identification {'true_positives': 657, 'false_positives': 127, 'false_negatives': 1495, 'precision': 0.8380102040816326, 'recall': 0.30529739776951675, 'f1': 0.4475476839237057}\n",
      "classification_result {'true_positives': 401, 'false_positives': 383, 'false_negatives': 1751, 'precision': 0.5114795918367347, 'recall': 0.18633828996282528, 'f1': 0.2731607629427793}\n",
      "EPOCHS : 0\n",
      "F1 eval : [0.         0.50540098 0.         0.98476308 0.25635359 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.951819105330271\n",
      "identification {'true_positives': 1784, 'false_positives': 270, 'false_negatives': 3098, 'precision': 0.8685491723466408, 'recall': 0.3654240065546907, 'f1': 0.5144175317185699}\n",
      "classification_result {'true_positives': 1004, 'false_positives': 1050, 'false_negatives': 3878, 'precision': 0.48880233690360275, 'recall': 0.20565342072920934, 'f1': 0.2895040369088812}\n",
      "SAVED : saved/model_2022_12_18_21_55_17.pth\n",
      "Epochs n. 1\n",
      "F1 train: [0.         0.4831553  0.07228916 0.98556403 0.         0.58646617\n",
      " 0.         0.         0.         0.         0.01639344 0.\n",
      " 0.         0.         0.         0.         0.14814815 0.\n",
      " 0.         0.         0.11940299 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9544334404786856\n",
      "identification {'true_positives': 1012, 'false_positives': 127, 'false_negatives': 1133, 'precision': 0.8884986830553117, 'recall': 0.4717948717948718, 'f1': 0.6163215590742995}\n",
      "classification_result {'true_positives': 635, 'false_positives': 504, 'false_negatives': 1510, 'precision': 0.5575065847234416, 'recall': 0.29603729603729606, 'f1': 0.38672350791717425}\n",
      "EPOCHS : 1\n",
      "F1 eval : [0.         0.51134548 0.         0.98473388 0.23535511 0.\n",
      " 0.         0.         0.         0.18390805 0.         0.\n",
      " 0.         0.         0.         0.         0.03448276 0.\n",
      " 0.         0.         0.01286174 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9524853334057606\n",
      "identification {'true_positives': 1901, 'false_positives': 415, 'false_negatives': 3022, 'precision': 0.8208117443868739, 'recall': 0.38614665854153973, 'f1': 0.5252106644564167}\n",
      "classification_result {'true_positives': 1065, 'false_positives': 1251, 'false_negatives': 3858, 'precision': 0.45984455958549225, 'recall': 0.21633150517976843, 'f1': 0.29423953584749274}\n",
      "SAVED : saved/model_2022_12_18_21_55_17.pth\n",
      "Epochs n. 2\n",
      "F1 train: [0.         0.49960349 0.07902736 0.98612618 0.         0.6159292\n",
      " 0.         0.         0.         0.         0.20689655 0.\n",
      " 0.         0.         0.         0.         0.19354839 0.\n",
      " 0.         0.         0.25333333 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.956746441350736\n",
      "identification {'true_positives': 1113, 'false_positives': 182, 'false_negatives': 1033, 'precision': 0.8594594594594595, 'recall': 0.5186393289841565, 'f1': 0.6469049694856146}\n",
      "classification_result {'true_positives': 722, 'false_positives': 573, 'false_negatives': 1424, 'precision': 0.5575289575289575, 'recall': 0.3364398881640261, 'f1': 0.41964545190351643}\n",
      "EPOCHS : 2\n",
      "F1 eval : [0.         0.51954603 0.09116809 0.98472378 0.14238411 0.\n",
      " 0.         0.         0.         0.23268698 0.         0.\n",
      " 0.         0.         0.         0.         0.0361991  0.\n",
      " 0.         0.         0.16374269 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9521032155241269\n",
      "identification {'true_positives': 1926, 'false_positives': 421, 'false_negatives': 2944, 'precision': 0.8206220707285897, 'recall': 0.395482546201232, 'f1': 0.5337397810724678}\n",
      "classification_result {'true_positives': 1059, 'false_positives': 1288, 'false_negatives': 3811, 'precision': 0.4512143161482744, 'recall': 0.21745379876796714, 'f1': 0.293473742552307}\n",
      "Epochs n. 3\n",
      "F1 train: [0.         0.4892772  0.16853933 0.98702312 0.         0.66314864\n",
      " 0.         0.         0.         0.         0.27329193 0.\n",
      " 0.         0.         0.         0.         0.19512195 0.\n",
      " 0.         0.         0.28187919 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9589274068393385\n",
      "identification {'true_positives': 1178, 'false_positives': 168, 'false_negatives': 963, 'precision': 0.87518573551263, 'recall': 0.5502101821578702, 'f1': 0.6756524232864928}\n",
      "classification_result {'true_positives': 770, 'false_positives': 576, 'false_negatives': 1371, 'precision': 0.5720653789004457, 'recall': 0.3596450256889304, 'f1': 0.4416403785488959}\n",
      "EPOCHS : 3\n",
      "F1 eval : [0.         0.52654867 0.12273361 0.98440412 0.1816387  0.\n",
      " 0.         0.         0.         0.25464191 0.         0.\n",
      " 0.         0.         0.         0.         0.18587361 0.\n",
      " 0.         0.         0.33248082 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9535452873154773\n",
      "identification {'true_positives': 2046, 'false_positives': 625, 'false_negatives': 2839, 'precision': 0.7660052414825907, 'recall': 0.41883316274309107, 'f1': 0.5415563790365272}\n",
      "classification_result {'true_positives': 1199, 'false_positives': 1472, 'false_negatives': 3686, 'precision': 0.4488955447397978, 'recall': 0.24544524053224157, 'f1': 0.31736368448914776}\n",
      "SAVED : saved/model_2022_12_18_21_55_17.pth\n",
      "Epochs n. 4\n",
      "F1 train: [0.         0.49594814 0.22051282 0.98784859 0.         0.70686767\n",
      " 0.         0.         0.         0.         0.34285714 0.\n",
      " 0.         0.         0.         0.         0.17241379 0.\n",
      " 0.         0.         0.32051282 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9609269395326534\n",
      "identification {'true_positives': 1255, 'false_positives': 162, 'false_negatives': 899, 'precision': 0.8856739590684545, 'recall': 0.5826369545032498, 'f1': 0.7028843461215346}\n",
      "classification_result {'true_positives': 836, 'false_positives': 581, 'false_negatives': 1318, 'precision': 0.5899788285109386, 'recall': 0.3881151346332405, 'f1': 0.46821618594231307}\n",
      "EPOCHS : 4\n",
      "F1 eval : [0.         0.51295681 0.14188267 0.98365281 0.14747859 0.\n",
      " 0.         0.         0.         0.23561644 0.         0.\n",
      " 0.         0.         0.         0.         0.06278027 0.\n",
      " 0.         0.         0.38834951 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9521465915526715\n",
      "identification {'true_positives': 1914, 'false_positives': 649, 'false_negatives': 2978, 'precision': 0.7467811158798283, 'recall': 0.3912510220768602, 'f1': 0.5134808853118713}\n",
      "classification_result {'true_positives': 1109, 'false_positives': 1454, 'false_negatives': 3783, 'precision': 0.43269605930550137, 'recall': 0.22669664758789862, 'f1': 0.29751844399731725}\n",
      "Epochs n. 5\n",
      "F1 train: [0.         0.52560873 0.3531746  0.98907995 0.         0.73452769\n",
      " 0.         0.         0.         0.         0.3583815  0.\n",
      " 0.         0.         0.         0.         0.32061069 0.\n",
      " 0.         0.         0.38323353 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.964453422178617\n",
      "identification {'true_positives': 1364, 'false_positives': 157, 'false_negatives': 814, 'precision': 0.8967784352399737, 'recall': 0.6262626262626263, 'f1': 0.7374966207082995}\n",
      "classification_result {'true_positives': 937, 'false_positives': 584, 'false_negatives': 1241, 'precision': 0.6160420775805391, 'recall': 0.43021120293847565, 'f1': 0.5066234117329007}\n",
      "Early stopping at epoch :  5\n",
      "F1 eval : [0.         0.51528998 0.23322332 0.98259752 0.04374703 0.\n",
      " 0.         0.         0.         0.25668449 0.         0.\n",
      " 0.         0.         0.         0.         0.21201413 0.\n",
      " 0.         0.         0.45248869 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9508463436472356\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "currentDateAndTime = datetime.now()\n",
    "_id = str(currentDateAndTime.year)+\"_\"+str(currentDateAndTime.month)+\"_\"+str(currentDateAndTime.day)+\"_\"+str(currentDateAndTime.hour)+\"_\"+str(currentDateAndTime.minute)+\"_\"+str(currentDateAndTime.second)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#optimizer = torch.optim.Adam(model.parameters(),lr = 0.000005)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "\n",
    "logSotfMax = torch.nn.LogSoftmax(dim=1)\n",
    "nll_loss = torch.nn.NLLLoss()\n",
    "\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "dataloader_dev = DataLoader(dev_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "mapping = dataloader_train.dataset.args_roles\n",
    "\n",
    "auto_model.eval()\n",
    "auto_model.cuda()\n",
    "\n",
    "EPOCHS = 200\n",
    "patience_counter = 0\n",
    "patience = 5\n",
    "max_val_loss = 9999\n",
    "f1_score_max = 0\n",
    "output_path = \"saved\"\n",
    "model_name = \"model_\"+_id+\".pth\"\n",
    "PATH = os.path.join(output_path,model_name)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    #TRAINING\n",
    "    p = []\n",
    "    g = []\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_train):\n",
    "        #print(sample_batched)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "       \n",
    "        #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "        #------------------FILTERING SUB-WORDS----------------------\n",
    "        subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "        word_emebedding = []\n",
    "        for i in range(n):\n",
    "            subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "            flag = subtoken_mask[0,i,0]\n",
    "            if flag :\n",
    "                continue\n",
    "            else :\n",
    "                word_emebedding.append(subwords_embedding)\n",
    "        word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "        #-------------------------FORWARD/BACKWARD----------------------------------\n",
    "        x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "            perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "            predicate_index = sample_batched[\"predicate_index\"],\n",
    "            pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "            predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])        \n",
    "        b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "        loss = nll_loss(logSotfMax(x),gt)\n",
    "        total_loss = total_loss + loss\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "        counter += 1 \n",
    "            \n",
    "\n",
    "        #-------------------------RESULT STORING----------------------------------\n",
    "        predicted = torch.argmax(x, dim=1)\n",
    "        p += predicted.tolist()\n",
    "        g += gt.tolist()\n",
    "    \n",
    "\n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    scheduler.step()\n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    print(\"Epochs n.\", epoch)\n",
    "    print(\"F1 train:\",f1)\n",
    "    print(\"F1 avg train:\",f1_avg)\n",
    "    \n",
    "    avg_train_loss = total_loss/counter\n",
    "    writer.add_scalar(\"EN_Loss_ES/train\", avg_train_loss, epoch)\n",
    "\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"EN_Train_ES/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"EN_Train_ES/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "    #EVALUATION\n",
    "    p = []\n",
    "    g = []\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_dev):\n",
    "    \n",
    "      #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "            #------------------FILTERING SUB-WORDS----------------------\n",
    "            subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "            word_emebedding = []\n",
    "            for i in range(n): \n",
    "                subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "                flag = subtoken_mask[0,i,0]\n",
    "                if flag :\n",
    "                    continue\n",
    "                else :\n",
    "                    word_emebedding.append(subwords_embedding)\n",
    "            word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "            #-------------------------FORWARD----------------------------------\n",
    "            x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "                        perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "                        predicate_index = sample_batched[\"predicate_index\"],\n",
    "                        pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "                        predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])   \n",
    "\n",
    "\n",
    "            b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "            loss = nll_loss(logSotfMax(x),gt)\n",
    "            total_loss = total_loss + loss\n",
    "            #-------------------------RESULT STORING----------------------------------\n",
    "            predicted = torch.argmax(x, dim=1)\n",
    "            p += predicted.tolist()\n",
    "            g += gt.tolist()\n",
    "            counter += 1 \n",
    "    \n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    avg_eval_loss = total_loss/counter\n",
    "\n",
    "    if avg_eval_loss < max_val_loss:\n",
    "        max_val_loss = avg_eval_loss\n",
    "    else :\n",
    "        patience_counter += 1\n",
    "    \n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    if patience_counter >= patience :  \n",
    "\n",
    "\n",
    "        print(\"Early stopping at epoch : \",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 avg eval :\",f1_avg)\n",
    "        break\n",
    "    else :\n",
    "        print(\"EPOCHS :\",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 avg eval :\",f1_avg)\n",
    "    \n",
    "\n",
    "    writer.add_scalar(\"EN_Loss_ES/validation\", avg_eval_loss, epoch)\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"EN_Eval_ES/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"EN_Eval_ES/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if f1_avg > f1_score_max:\n",
    "        f1_score_max = f1_avg\n",
    "        print(\"SAVED :\",PATH)\n",
    "        torch.save(model.state_dict(),PATH)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare without Transfert Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs n. 0\n",
      "F1 train: [0.         0.19946809 0.         0.9768764  0.         0.0295421\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9336015019908596\n",
      "identification {'true_positives': 172, 'false_positives': 71, 'false_negatives': 1987, 'precision': 0.7078189300411523, 'recall': 0.07966651227420102, 'f1': 0.14321398834304747}\n",
      "classification_result {'true_positives': 85, 'false_positives': 158, 'false_negatives': 2074, 'precision': 0.3497942386831276, 'recall': 0.03937007874015748, 'f1': 0.070774354704413}\n",
      "EPOCHS : 0\n",
      "F1 eval : [0.         0.23416618 0.         0.9777718  0.00265252 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9389493973815146\n",
      "identification {'true_positives': 1030, 'false_positives': 1096, 'false_negatives': 3858, 'precision': 0.48447789275634995, 'recall': 0.2107201309328969, 'f1': 0.2936983176504135}\n",
      "classification_result {'true_positives': 405, 'false_positives': 1721, 'false_negatives': 4483, 'precision': 0.1904985888993415, 'recall': 0.08285597381342062, 'f1': 0.11548331907613345}\n",
      "SAVED : saved/model_2022_12_18_22_2_58.pth\n",
      "Epochs n. 1\n",
      "F1 train: [0.         0.3872549  0.         0.98172752 0.         0.42610365\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9464882649310143\n",
      "identification {'true_positives': 798, 'false_positives': 268, 'false_negatives': 1337, 'precision': 0.7485928705440901, 'recall': 0.3737704918032787, 'f1': 0.4985941893158388}\n",
      "classification_result {'true_positives': 459, 'false_positives': 607, 'false_negatives': 1676, 'precision': 0.43058161350844276, 'recall': 0.21498829039812647, 'f1': 0.2867853795688848}\n",
      "EPOCHS : 1\n",
      "F1 eval : [0.         0.23666396 0.         0.97658545 0.00478469 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9376621220958927\n",
      "identification {'true_positives': 1127, 'false_positives': 1421, 'false_negatives': 3759, 'precision': 0.4423076923076923, 'recall': 0.23065902578796563, 'f1': 0.3032015065913371}\n",
      "classification_result {'true_positives': 441, 'false_positives': 2107, 'false_negatives': 4445, 'precision': 0.17307692307692307, 'recall': 0.09025787965616046, 'f1': 0.11864406779661017}\n",
      "Epochs n. 2\n",
      "F1 train: [0.         0.44530046 0.         0.98438053 0.         0.60050463\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.952145225902215\n",
      "identification {'true_positives': 1022, 'false_positives': 243, 'false_negatives': 1124, 'precision': 0.807905138339921, 'recall': 0.4762348555452004, 'f1': 0.5992377601876282}\n",
      "classification_result {'true_positives': 646, 'false_positives': 619, 'false_negatives': 1500, 'precision': 0.5106719367588933, 'recall': 0.30102516309412863, 'f1': 0.37877455291703316}\n",
      "EPOCHS : 2\n",
      "F1 eval : [0.         0.26013195 0.         0.9780147  0.0071599  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9395047264553521\n",
      "identification {'true_positives': 1022, 'false_positives': 1031, 'false_negatives': 3848, 'precision': 0.4978080857282026, 'recall': 0.20985626283367556, 'f1': 0.2952477249747219}\n",
      "classification_result {'true_positives': 420, 'false_positives': 1633, 'false_negatives': 4450, 'precision': 0.20457866536775451, 'recall': 0.08624229979466119, 'f1': 0.12133468149646107}\n",
      "SAVED : saved/model_2022_12_18_22_2_58.pth\n",
      "Epochs n. 3\n",
      "F1 train: [0.         0.47011952 0.02555911 0.98541674 0.         0.63305785\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9541615111910255\n",
      "identification {'true_positives': 1065, 'false_positives': 200, 'false_negatives': 1089, 'precision': 0.841897233201581, 'recall': 0.49442896935933145, 'f1': 0.6229891781222578}\n",
      "classification_result {'true_positives': 682, 'false_positives': 583, 'false_negatives': 1472, 'precision': 0.5391304347826087, 'recall': 0.3166202414113278, 'f1': 0.39894706054401874}\n",
      "EPOCHS : 3\n",
      "F1 eval : [0.         0.28145455 0.02816901 0.97774314 0.00762112 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9396751709187108\n",
      "identification {'true_positives': 871, 'false_positives': 950, 'false_negatives': 4019, 'precision': 0.4783086216364635, 'recall': 0.17811860940695295, 'f1': 0.2595738340038742}\n",
      "classification_result {'true_positives': 404, 'false_positives': 1417, 'false_negatives': 4486, 'precision': 0.22185612300933552, 'recall': 0.08261758691206544, 'f1': 0.12039934436000595}\n",
      "SAVED : saved/model_2022_12_18_22_2_58.pth\n",
      "Epochs n. 4\n",
      "F1 train: [0.         0.51370399 0.05590062 0.98651971 0.         0.69414674\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.07843137 0.\n",
      " 0.         0.         0.03333333 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9570333876174468\n",
      "identification {'true_positives': 1139, 'false_positives': 185, 'false_negatives': 994, 'precision': 0.8602719033232629, 'recall': 0.5339896858884201, 'f1': 0.658952849291293}\n",
      "classification_result {'true_positives': 764, 'false_positives': 560, 'false_negatives': 1369, 'precision': 0.5770392749244713, 'recall': 0.35818096577590247, 'f1': 0.4420017356089094}\n",
      "EPOCHS : 4\n",
      "F1 eval : [0.         0.27561328 0.01457726 0.9769951  0.00616333 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9388430347981687\n",
      "identification {'true_positives': 843, 'false_positives': 1092, 'false_negatives': 4038, 'precision': 0.4356589147286822, 'recall': 0.1727105101413645, 'f1': 0.24735915492957747}\n",
      "classification_result {'true_positives': 393, 'false_positives': 1542, 'false_negatives': 4488, 'precision': 0.20310077519379846, 'recall': 0.08051628764597418, 'f1': 0.11531690140845072}\n",
      "Epochs n. 5\n",
      "F1 train: [0.         0.51257862 0.12105263 0.98804271 0.         0.72871917\n",
      " 0.         0.         0.         0.         0.0620155  0.\n",
      " 0.         0.         0.         0.         0.12612613 0.\n",
      " 0.         0.         0.01652893 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9595282512313569\n",
      "identification {'true_positives': 1263, 'false_positives': 167, 'false_negatives': 879, 'precision': 0.8832167832167832, 'recall': 0.5896358543417367, 'f1': 0.7071668533034715}\n",
      "classification_result {'true_positives': 819, 'false_positives': 611, 'false_negatives': 1323, 'precision': 0.5727272727272728, 'recall': 0.38235294117647056, 'f1': 0.4585666293393057}\n",
      "Early stopping at epoch :  5\n",
      "F1 eval : [0.         0.30775194 0.03133903 0.97723285 0.00687623 0.\n",
      " 0.         0.         0.         0.04590164 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9394300496737596\n"
     ]
    }
   ],
   "source": [
    "model = Arg_Classifier(\"FR\",cfg).cuda()\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "currentDateAndTime = datetime.now()\n",
    "_id = str(currentDateAndTime.year)+\"_\"+str(currentDateAndTime.month)+\"_\"+str(currentDateAndTime.day)+\"_\"+str(currentDateAndTime.hour)+\"_\"+str(currentDateAndTime.minute)+\"_\"+str(currentDateAndTime.second)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#optimizer = torch.optim.Adam(model.parameters(),lr = 0.000005)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "\n",
    "logSotfMax = torch.nn.LogSoftmax(dim=1)\n",
    "nll_loss = torch.nn.NLLLoss()\n",
    "\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "dataloader_dev = DataLoader(dev_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "mapping = dataloader_train.dataset.args_roles\n",
    "\n",
    "auto_model.eval()\n",
    "auto_model.cuda()\n",
    "\n",
    "EPOCHS = 200\n",
    "patience_counter = 0\n",
    "patience = 5\n",
    "max_val_loss = 9999\n",
    "f1_score_max = 0\n",
    "output_path = \"saved\"\n",
    "model_name = \"model_\"+_id+\".pth\"\n",
    "PATH = os.path.join(output_path,model_name)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    #TRAINING\n",
    "    p = []\n",
    "    g = []\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_train):\n",
    "        #print(sample_batched)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "       \n",
    "        #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "        #------------------FILTERING SUB-WORDS----------------------\n",
    "        subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "        word_emebedding = []\n",
    "        for i in range(n):\n",
    "            subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "            flag = subtoken_mask[0,i,0]\n",
    "            if flag :\n",
    "                continue\n",
    "            else :\n",
    "                word_emebedding.append(subwords_embedding)\n",
    "        word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "        #-------------------------FORWARD/BACKWARD----------------------------------\n",
    "        x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "            perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "            predicate_index = sample_batched[\"predicate_index\"],\n",
    "            pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "            predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])        \n",
    "        b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "        loss = nll_loss(logSotfMax(x),gt)\n",
    "        total_loss = total_loss + loss\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "        counter += 1 \n",
    "            \n",
    "\n",
    "        #-------------------------RESULT STORING----------------------------------\n",
    "        predicted = torch.argmax(x, dim=1)\n",
    "        p += predicted.tolist()\n",
    "        g += gt.tolist()\n",
    "    \n",
    "\n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    scheduler.step()\n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    print(\"Epochs n.\", epoch)\n",
    "    print(\"F1 train:\",f1)\n",
    "    print(\"F1 avg train:\",f1_avg)\n",
    "    \n",
    "    avg_train_loss = total_loss/counter\n",
    "    writer.add_scalar(\"Loss_ES/train\", avg_train_loss, epoch)\n",
    "\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"Train_ES/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"Train_ES/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "    #EVALUATION\n",
    "    p = []\n",
    "    g = []\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_dev):\n",
    "    \n",
    "      #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "            #------------------FILTERING SUB-WORDS----------------------\n",
    "            subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "            word_emebedding = []\n",
    "            for i in range(n): \n",
    "                subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "                flag = subtoken_mask[0,i,0]\n",
    "                if flag :\n",
    "                    continue\n",
    "                else :\n",
    "                    word_emebedding.append(subwords_embedding)\n",
    "            word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "            #-------------------------FORWARD----------------------------------\n",
    "            x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "                        perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "                        predicate_index = sample_batched[\"predicate_index\"],\n",
    "                        pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "                        predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])   \n",
    "\n",
    "\n",
    "            b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "            loss = nll_loss(logSotfMax(x),gt)\n",
    "            total_loss = total_loss + loss\n",
    "            #-------------------------RESULT STORING----------------------------------\n",
    "            predicted = torch.argmax(x, dim=1)\n",
    "            p += predicted.tolist()\n",
    "            g += gt.tolist()\n",
    "            counter += 1 \n",
    "    \n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    avg_eval_loss = total_loss/counter\n",
    "\n",
    "    if avg_eval_loss < max_val_loss:\n",
    "        max_val_loss = avg_eval_loss\n",
    "    else :\n",
    "        patience_counter += 1\n",
    "    \n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    if patience_counter >= patience :  \n",
    "\n",
    "\n",
    "        print(\"Early stopping at epoch : \",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 avg eval :\",f1_avg)\n",
    "        break\n",
    "    else :\n",
    "        print(\"EPOCHS :\",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 avg eval :\",f1_avg)\n",
    "    \n",
    "\n",
    "    writer.add_scalar(\"Loss_ES/validation\", avg_eval_loss, epoch)\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"Eval_ES/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"Eval_ES/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if f1_avg > f1_score_max:\n",
    "        f1_score_max = f1_avg\n",
    "        print(\"SAVED :\",PATH)\n",
    "        torch.save(model.state_dict(),PATH)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('nlp2022-hw3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ea0fd027ae2f380e05c4e3293ede0f85a80d4d466dd9309da3a748502c958571"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
