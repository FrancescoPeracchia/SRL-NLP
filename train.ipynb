{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT and TOKENIZER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model class is      : <class 'transformers.models.bert.modeling_bert.BertModel'>\n",
      "\n",
      "model class is      : <class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel,AutoTokenizer\n",
    "\n",
    "auto_model = AutoModel.from_pretrained(\"bert-base-cased\",output_hidden_states=True)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nmodel class is      : {type(auto_model)}\")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "print(f\"\\nmodel class is      : {type(tokenizer)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Role Labelling Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import random\n",
    "from typing import Dict\n",
    "\n",
    "class SRL(Dataset):\n",
    " \n",
    "    def __init__(self,language,path,args_roles = None,pos_list = None) -> None:\n",
    "\n",
    "        self.path_root = 'data'\n",
    "        self.load_data(language,path)\n",
    "        if args_roles is None :\n",
    "            self.args_roles,self.list_broken_id = self.list_arg_roles()\n",
    "        else : \n",
    "            self.args_roles = args_roles\n",
    "            _,self.list_broken_id = self.list_arg_roles()\n",
    "        \n",
    "\n",
    "        if pos_list is None :\n",
    "            self.pos_list,_ = self.list_pos()\n",
    "            self.pos_list.append(\"Nothing\")\n",
    "        else : \n",
    "            self.pos_list = pos_list\n",
    "        \n",
    "\n",
    "\n",
    "        self.predicate_dis,_ = self.list_predicate_roles()\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(self.pos_list)\n",
    "\n",
    "        self.predicate_dis.append(\"Nothing\")\n",
    "\n",
    "    def load_data(self,language,mode):\n",
    "        \n",
    "        mode = mode+\".json\"\n",
    "        path = os.path.join(self.path_root,language,mode)\n",
    "        data_file = open(path)\n",
    "       \n",
    "        data_ = json.load(data_file)\n",
    "\n",
    "        list_data = []\n",
    "\n",
    "        for data in data_:\n",
    "            list_data.append(data_[data])\n",
    "        \n",
    "\n",
    "        self.data = list_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, id : int):\n",
    "\n",
    "        flag = False\n",
    "        if id in self.list_broken_id :\n",
    "            flag = True\n",
    "            while flag == True:\n",
    "\n",
    "                rand_id = random.randint(0, len(self.data)-1)\n",
    "                \n",
    "                if rand_id in self.list_broken_id :\n",
    "                    pass\n",
    "                else :\n",
    "                    flag = False\n",
    "                    id = rand_id        \n",
    "\n",
    "\n",
    "        data = self.pre_processing(self.data[id])\n",
    "        data = self.processig(data)\n",
    "        return data\n",
    "        \n",
    "    def pre_processing(self, data:dict):\n",
    "        data_list = []\n",
    "        for role in data[\"roles\"]:\n",
    "            dictionary = dict()\n",
    "            dictionary[\"words\"] = data[\"words\"]\n",
    "            dictionary[\"role\"] = data[\"roles\"][role]\n",
    "            dictionary[\"pre_idx\"] = role\n",
    "            dictionary[\"pos_tags\"] = data[\"pos_tags\"]\n",
    "            dictionary[\"predicate_meaning\"] = data[\"predicates\"]\n",
    "            data_list.append(dictionary)    \n",
    "        return data_list\n",
    "    \n",
    "    def processig(self,data_list:list):\n",
    "        \n",
    "        for dictionary in data_list:\n",
    "\n",
    "            #dictionary[\"words\"] = data[\"words\"]\n",
    "            dictionary[\"gt_arg_identification\"] = self.arg_id(dictionary[\"role\"])\n",
    "            dictionary[\"gt_arg_classification\"] = self.arg_class(dictionary[\"role\"])\n",
    "            dictionary[\"pos_idx\"] = self.pos_idx(dictionary[\"pos_tags\"])\n",
    "            dictionary[\"predicate_meaning_idx\"] = self.predicate_meaning_idx(dictionary[\"predicate_meaning\"])\n",
    "        \n",
    "        return data_list\n",
    "   \n",
    "    def list_arg_roles(self):\n",
    "        list_roles = []\n",
    "        list_broken_id = []\n",
    "        for i,element in enumerate(self.data):\n",
    "            flag = True\n",
    "            try : roles = element[\"roles\"]\n",
    "            except : flag = False\n",
    "            if flag :\n",
    "                for e in roles:\n",
    "                    sentence = element[\"roles\"][e]\n",
    "\n",
    "                    for word in sentence:\n",
    "                        \n",
    "                        list_roles.append(word)\n",
    "                list_roles = list(set(list_roles))\n",
    "            else : \n",
    "                list_broken_id.append(i)\n",
    "        return list_roles,list_broken_id\n",
    "\n",
    "    def list_predicate_roles(self):\n",
    "        list_predicate_roles = []\n",
    "        list_broken_id = []\n",
    "        for i,element in enumerate(self.data):\n",
    "            flag = True\n",
    "            try : predicates = element[\"predicates\"]\n",
    "            except : flag = False\n",
    "            if flag :\n",
    "                for pre in predicates:\n",
    "                    list_predicate_roles.append(pre)\n",
    "                list_predicate_roles = list(set(list_predicate_roles))\n",
    "            else : \n",
    "                list_broken_id.append(i)\n",
    "        return list_predicate_roles,list_broken_id\n",
    "\n",
    "    def list_pos(self):\n",
    "        list_pos = []\n",
    "        list_broken_id = []\n",
    "        for i,element in enumerate(self.data):\n",
    "            flag = True\n",
    "            try : pos = element[\"pos_tags\"]\n",
    "            except : flag = False\n",
    "            if flag :\n",
    "                for e in pos:\n",
    "                    list_pos.append(e)\n",
    "                list_pos = list(set(list_pos))\n",
    "            else : \n",
    "                list_broken_id.append(i)\n",
    "        return list_pos,list_broken_id\n",
    "  \n",
    "    def arg_class(self,role:list):\n",
    "        list_idxs = []\n",
    "        for element in role:\n",
    "            list_idxs.append(self.args_roles.index(element))\n",
    "        \n",
    "\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "\n",
    "    def arg_id(self,role:dict):\n",
    "        list_idxs = []\n",
    "        for element in role:\n",
    "            if element == \"_\":\n",
    "                list_idxs.append(0)\n",
    "            else :\n",
    "                list_idxs.append(1)\n",
    "\n",
    "        \n",
    "\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "\n",
    "    def pos_idx(self,pos_tags:dict):\n",
    "        list_idxs = []\n",
    "        list_idxs.append(self.pos_list.index(\"Nothing\"))\n",
    "\n",
    "        for element in pos_tags:\n",
    "            list_idxs.append(self.pos_list.index(element))\n",
    "        \n",
    "        list_idxs.append(self.pos_list.index(\"Nothing\"))\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "    \n",
    "    def predicate_meaning_idx(self,predicate_meaning_tags:dict):\n",
    "        list_idxs = []\n",
    "        list_idxs.append(self.predicate_dis.index(\"Nothing\"))\n",
    "\n",
    "        for element in predicate_meaning_tags:\n",
    "            list_idxs.append(self.predicate_dis.index(element))\n",
    "        \n",
    "        list_idxs.append(self.predicate_dis.index(\"Nothing\"))\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64) \n",
    "\n",
    "    \n",
    "# here we define our collate function\n",
    "def collate_fn(batch) -> Dict[str, torch.Tensor]:\n",
    "    #print(batch)\n",
    "    input = dict() \n",
    "    batch_sentence = [] \n",
    "    #print(len(batch))\n",
    "    for period in batch:\n",
    "        for sentence in period :\n",
    "        \n",
    "            #print(len(sentence[0][\"words\"]))\n",
    "            pre_idx = int(sentence[\"pre_idx\"])\n",
    "            \n",
    "\n",
    "            predicate = sentence[\"words\"][pre_idx]\n",
    "\n",
    "            text = \" \".join(sentence[\"words\"])\n",
    "            tokens: list[str] = text.split()\n",
    "            predicate: list[str] = predicate.split()\n",
    "\n",
    "            #text = sentence[0][\"words\"]\n",
    "            \n",
    "            t = (tokens,predicate)\n",
    "\n",
    "            batch_sentence.append(t)\n",
    "            #print(batch_sentence)\n",
    "\n",
    "    batch_output = tokenizer.batch_encode_plus(batch_sentence,padding=True,is_split_into_words=True, truncation=True,return_offsets_mapping=True, return_tensors=\"pt\")\n",
    "    #print(batch_output.keys())\n",
    "\n",
    "\n",
    "    gt = dict()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    for period in batch:\n",
    "\n",
    "        list_positional_predicate_encoding = []\n",
    "        list_arg_gt = []\n",
    "        list_predicate_index = [] \n",
    "        list_pos_index = [] \n",
    "        list_predicate_meaning_index = []\n",
    "\n",
    "        for sentence in period:\n",
    "            #positional_encoding\n",
    "            #+2 per il CLS iniziale ad SEP finale\n",
    "            sentence_words_lenght =  len(sentence[\"words\"])\n",
    "            positional_predicate_encoding = torch.zeros(1,sentence_words_lenght+2)\n",
    "            #+1 per il CLS iniziale\n",
    "            pre_idx = int(sentence[\"pre_idx\"])\n",
    "            positional_predicate_encoding[:,pre_idx+1] = 1\n",
    "            list_positional_predicate_encoding.append(positional_predicate_encoding)\n",
    "            #print(\"positional_prefix_encoding\",positional_predicate_encoding)\n",
    "            list_predicate_index.append(pre_idx)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            pos = torch.unsqueeze(sentence[\"pos_idx\"],dim = 0)\n",
    "            list_pos_index.append(pos)\n",
    "            predicate_meaning_idxs = torch.unsqueeze(sentence[\"predicate_meaning_idx\"],dim = 0)\n",
    "            list_predicate_meaning_index.append(predicate_meaning_idxs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #note CLS and SEP are discharder after Bi-LSTM, the Classifier takes in input only wokrds hidden state embedding\n",
    "            arg_gt = torch.unsqueeze(sentence[\"gt_arg_classification\"],dim = 0)\n",
    "            list_arg_gt.append(arg_gt)\n",
    "        \n",
    "\n",
    "    list_arg_gt = torch.cat(list_arg_gt,dim = 0)\n",
    "    list_pos_index = torch.cat(list_pos_index,dim = 0)\n",
    "    list_predicate_meaning_index = torch.cat(list_predicate_meaning_index,dim = 0)\n",
    "    list_positional_predicate_encoding = torch.cat(list_positional_predicate_encoding,dim = 0)\n",
    "    gt[\"arg_gt\"] = list_arg_gt\n",
    "    input[\"predicate_index\"] = list_predicate_index\n",
    "    input[\"pos_index\"] = list_pos_index.long()\n",
    "    input[\"predicate_meaning_idx\"] = list_predicate_meaning_index.long()\n",
    "    offset = batch_output.pop(\"offset_mapping\")\n",
    "    input[\"BERT_input\"] = batch_output\n",
    "    input[\"positional_encoding\"] = list_positional_predicate_encoding.long()\n",
    "    input[\"offset_mapping\"] = offset\n",
    "    input[\"gt\"] = gt\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    return input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADP', 'PUNCT', 'SYM', 'ADV', 'X', 'PART', 'AUX', 'NUM', 'INTJ', 'SCONJ', 'VERB', 'NOUN', 'DET', 'PROPN', 'PRON', 'CCONJ', 'ADJ', 'Nothing']\n",
      "['ADP', 'PUNCT', 'SYM', 'ADV', 'X', 'PART', 'AUX', 'NUM', 'INTJ', 'SCONJ', 'VERB', 'NOUN', 'DET', 'PROPN', 'PRON', 'CCONJ', 'ADJ', 'Nothing']\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SRL(\"EN\",\"train\")\n",
    "#same mapping should be used in both the dataset\n",
    "dev_dataset = SRL(\"EN\",\"dev\",train_dataset.args_roles,train_dataset.pos_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "embeddings = dict()\n",
    "\n",
    "embeddings[\"predicate_flag_embedding_output_dim\"] = 32\n",
    "#defined in initial exploration of the dataset\n",
    "embeddings[\"pos_embedding_input_dim\"] = 0\n",
    "embeddings[\"pos_embedding_output_dim\"] = 100\n",
    "#-------------------------------------------------\n",
    "embeddings[\"predicate_embedding_input_dim\"] = 0\n",
    "embeddings[\"predicate_embedding_output_dim\"] = False\n",
    "#defined in initial exploration of the dataset\n",
    "n_classes = 0\n",
    "\n",
    "\n",
    "\n",
    "bilstm = dict()\n",
    "bilstm[\"n_layers\"] = 2\n",
    "bilstm[\"output_dim\"] = 50\n",
    "dropouts = [0.4,0.3,0.3]\n",
    "\n",
    "language_portable = True\n",
    "predicate_meaning = True\n",
    "pos = True\n",
    "\n",
    "cfg = dict()\n",
    "cfg[\"embeddings\"] = embeddings\n",
    "cfg[\"n_classes\"] = n_classes\n",
    "cfg[\"bilstm\"] = bilstm\n",
    "cfg[\"language_portable\"] = language_portable\n",
    "cfg[\"dropouts\"] = dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arg_Classifier(\n",
      "  (bi_lstm_portable): LSTM(132, 50, num_layers=2, batch_first=True, dropout=0.4, bidirectional=True)\n",
      "  (embedding_predicate_flag): Embedding(2, 32, max_norm=True)\n",
      "  (embedding_predicate): Embedding(304, False, max_norm=True)\n",
      "  (embedding_pos): Embedding(18, 100, max_norm=True)\n",
      "  (bi_lstm): LSTM(900, 50, num_layers=2, batch_first=True, dropout=0.4, bidirectional=True)\n",
      "  (dropout_language_constraint): Dropout(p=0.6, inplace=False)\n",
      "  (dropout_in_classifier): Dropout(p=0.4, inplace=False)\n",
      "  (Relu): ReLU()\n",
      "  (Sigmoid): Sigmoid()\n",
      "  (linear0): Linear(in_features=300, out_features=675, bias=True)\n",
      "  (linear1): Linear(in_features=675, out_features=135, bias=True)\n",
      "  (linear2): Linear(in_features=135, out_features=27, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#from mmcv import Config\n",
    "from hw2.stud.arg import Arg_Classifier, Arg_Classifier_from_paper\n",
    "\n",
    "\n",
    "\n",
    "#cfg = Config.fromfile('/home/francesco/Desktop/nlp2022-hw2-main/hw2/stud/configs/model.py')\n",
    "\n",
    "cfg[\"embeddings\"][\"pos_embedding_input_dim\"] = len(train_dataset.pos_list)\n",
    "cfg[\"embeddings\"][\"predicate_embedding_input_dim\"] = len(train_dataset.predicate_dis)\n",
    "cfg[\"n_classes\"] = len(train_dataset.args_roles)\n",
    "\n",
    "\n",
    "model = Arg_Classifier(\"EN\",cfg).cuda()\n",
    "#model = Arg_Classifier_from_paper(\"EN\",cfg).cuda()\n",
    "print(model)\n",
    "\n",
    "automodel = auto_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def metrics(gold,pred):\n",
    "    true_positives, false_positives, false_negatives = 0, 0, 0\n",
    "    null_tag = \"_\"\n",
    "\n",
    "    for i,r_g in  enumerate(gold):\n",
    "        r_p = pred[i]\n",
    "\n",
    "        if r_g != null_tag and r_p != null_tag:\n",
    "            true_positives += 1\n",
    "        elif r_g != null_tag and r_p == null_tag:\n",
    "            false_negatives += 1\n",
    "        elif r_g == null_tag and r_p != null_tag:\n",
    "            false_positives += 1\n",
    "\n",
    "    a = true_positives + false_positives\n",
    "    b = true_positives + false_negatives\n",
    "    if a == 0 and b == 0 :        \n",
    "        argument_identification = {\n",
    "            \"true_positives\": 0,\n",
    "            \"false_positives\": 0,\n",
    "            \"false_negatives\": 0,\n",
    "            \"precision\": 0,\n",
    "            \"recall\": 0,\n",
    "            \"f1\": 0,\n",
    "        } \n",
    "\n",
    "    else : \n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        argument_identification = {\n",
    "            \"true_positives\": true_positives,\n",
    "            \"false_positives\": false_positives,\n",
    "            \"false_negatives\": false_negatives,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    true_positives, false_positives, false_negatives = 0, 0, 0\n",
    "    for i,r_g in  enumerate(gold):\n",
    "        r_p = pred[i]\n",
    "\n",
    "        if r_g != null_tag and r_p != null_tag:\n",
    "            if r_g == r_p:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                false_positives += 1\n",
    "                false_negatives += 1\n",
    "        elif r_g != null_tag and r_p == null_tag:\n",
    "                false_negatives += 1\n",
    "        elif r_g == null_tag and r_p != null_tag:\n",
    "                false_positives += 1\n",
    "\n",
    "\n",
    "    a = true_positives + false_positives\n",
    "    b = true_positives + false_negatives\n",
    "    if a == 0 and b == 0 :\n",
    "        argument_classification = {\n",
    "            \"true_positives\": 0,\n",
    "            \"false_positives\": 0,\n",
    "            \"false_negatives\": 0,\n",
    "            \"precision\": 0,\n",
    "            \"recall\": 0,\n",
    "            \"f1\": 0,\n",
    "        } \n",
    "\n",
    "    else : \n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        argument_classification = {\n",
    "            \"true_positives\": true_positives,\n",
    "            \"false_positives\": false_positives,\n",
    "            \"false_negatives\": false_negatives,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "        }\n",
    "\n",
    "\n",
    "    return argument_identification,argument_classification\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "from [1,1,8,8,8,8,8,8,8,8,8,8,8,8,2,2,2,8,8,8......]\n",
    "to from [agent,agent,_,_,_........,]\n",
    "\"\"\"\n",
    "def mapping_args(g,p,mapping):\n",
    "    \n",
    "    \n",
    "    gt = [mapping[elem] for elem in g]\n",
    "    predictions = [mapping[elem] for elem in p]\n",
    "\n",
    "\n",
    "    return gt,predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Argument Identification and Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs n. 0\n",
      "F1 train: [0.         0.54511996 0.38440348 0.98901032 0.         0.76087107\n",
      " 0.         0.         0.02047782 0.         0.29840657 0.00754717\n",
      " 0.         0.         0.         0.         0.         0.40305344\n",
      " 0.         0.00666667 0.         0.49908088 0.         0.\n",
      " 0.         0.         0.12041116]\n",
      "F1 avg train: 0.9625738232757398\n",
      "identification {'true_positives': 16748, 'false_positives': 1711, 'false_negatives': 8107, 'precision': 0.9073080881954602, 'recall': 0.6738282035807684, 'f1': 0.7733296393775685}\n",
      "classification_result {'true_positives': 11571, 'false_positives': 6888, 'false_negatives': 13284, 'precision': 0.626848691695108, 'recall': 0.4655401327700664, 'f1': 0.5342845269427898}\n",
      "EPOCHS : 0\n",
      "F1 eval : [0.         0.57418244 0.53695837 0.99054861 0.         0.76754536\n",
      " 0.         0.         0.         0.         0.44265594 0.\n",
      " 0.         0.         0.         0.         0.         0.54482759\n",
      " 0.         0.         0.         0.40092166 0.         0.\n",
      " 0.         0.         0.        ]\n",
      "F1 avg eval : 0.967779064957195\n",
      "identification {'true_positives': 3532, 'false_positives': 306, 'false_negatives': 1536, 'precision': 0.9202709744658676, 'recall': 0.696921862667719, 'f1': 0.7931731417022232}\n",
      "classification_result {'true_positives': 2547, 'false_positives': 1291, 'false_negatives': 2521, 'precision': 0.6636268890046899, 'recall': 0.5025651144435674, 'f1': 0.571973950145969}\n",
      "SAVED : saved/model_2022_12_19_0_21_49.pth\n",
      "Epochs n. 1\n",
      "F1 train: [0.         0.56753927 0.49214258 0.98953319 0.         0.78331034\n",
      " 0.         0.         0.02515723 0.         0.32445068 0.02214022\n",
      " 0.         0.         0.         0.         0.         0.47688921\n",
      " 0.00534759 0.05144695 0.         0.53918495 0.         0.\n",
      " 0.         0.         0.11567732]\n",
      "F1 avg train: 0.9648643331620281\n",
      "identification {'true_positives': 17169, 'false_positives': 1700, 'false_negatives': 7650, 'precision': 0.9099051354072818, 'recall': 0.6917684032394537, 'f1': 0.7859824208020508}\n",
      "classification_result {'true_positives': 12356, 'false_positives': 6513, 'false_negatives': 12463, 'precision': 0.6548306746515449, 'recall': 0.4978443934082759, 'f1': 0.5656473173411464}\n",
      "EPOCHS : 1\n",
      "F1 eval : [0.         0.59677419 0.53683148 0.99095824 0.         0.78439024\n",
      " 0.         0.         0.         0.         0.39830508 0.\n",
      " 0.         0.         0.         0.         0.         0.58555133\n",
      " 0.         0.         0.         0.49681529 0.         0.\n",
      " 0.         0.         0.        ]\n",
      "F1 avg eval : 0.9690515038419879\n",
      "identification {'true_positives': 3641, 'false_positives': 338, 'false_negatives': 1423, 'precision': 0.9150540336768033, 'recall': 0.718996840442338, 'f1': 0.8052637399093222}\n",
      "classification_result {'true_positives': 2685, 'false_positives': 1294, 'false_negatives': 2379, 'precision': 0.6747926614727319, 'recall': 0.5302132701421801, 'f1': 0.593829481366803}\n",
      "SAVED : saved/model_2022_12_19_0_21_49.pth\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [39], line 111\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[39m#-------------------------RESULT STORING----------------------------------\u001b[39;00m\n\u001b[1;32m    110\u001b[0m     predicted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(x, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 111\u001b[0m     p \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m predicted\u001b[39m.\u001b[39;49mtolist()\n\u001b[1;32m    112\u001b[0m     g \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m gt\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m    115\u001b[0m \u001b[39m#-------------------------RESULTS----------------------------------\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "currentDateAndTime = datetime.now()\n",
    "_id = str(currentDateAndTime.year)+\"_\"+str(currentDateAndTime.month)+\"_\"+str(currentDateAndTime.day)+\"_\"+str(currentDateAndTime.hour)+\"_\"+str(currentDateAndTime.minute)+\"_\"+str(currentDateAndTime.second)\n",
    "\n",
    "\n",
    "\n",
    "#optimizer = torch.optim.Adam(model.parameters(),lr = 0.000005)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "\n",
    "logSotfMax = torch.nn.LogSoftmax(dim=1)\n",
    "nll_loss = torch.nn.NLLLoss()\n",
    "\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "dataloader_dev = DataLoader(dev_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "mapping = dataloader_train.dataset.args_roles\n",
    "\n",
    "auto_model.eval()\n",
    "\n",
    "EPOCHS = 200\n",
    "patience_counter = 0\n",
    "patience = 5\n",
    "max_val_loss = 9999\n",
    "f1_score_max = 0\n",
    "output_path = \"saved\"\n",
    "model_name = \"model_\"+_id+\".pth\"\n",
    "PATH = os.path.join(output_path,model_name)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    #TRAINING\n",
    "    p = []\n",
    "    g = []\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_train):\n",
    "        #print(sample_batched)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "       \n",
    "        #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "        #------------------FILTERING SUB-WORDS----------------------\n",
    "        subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "        word_emebedding = []\n",
    "        for i in range(n):\n",
    "            subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "            flag = subtoken_mask[0,i,0]\n",
    "            if flag :\n",
    "                continue\n",
    "            else :\n",
    "                word_emebedding.append(subwords_embedding)\n",
    "        word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "        #-------------------------FORWARD/BACKWARD----------------------------------\n",
    "        x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "            perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "            predicate_index = sample_batched[\"predicate_index\"],\n",
    "            pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "            predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])        \n",
    "        b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "        loss = nll_loss(logSotfMax(x),gt)\n",
    "        total_loss = total_loss + loss\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "        counter += 1 \n",
    "            \n",
    "\n",
    "        #-------------------------RESULT STORING----------------------------------\n",
    "        predicted = torch.argmax(x, dim=1)\n",
    "        p += predicted.tolist()\n",
    "        g += gt.tolist()\n",
    "    \n",
    "\n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    scheduler.step()\n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    print(\"Epochs n.\", epoch)\n",
    "    print(\"F1 train:\",f1)\n",
    "    print(\"F1 avg train:\",f1_avg)\n",
    "    \n",
    "    avg_train_loss = total_loss/counter\n",
    "    writer.add_scalar(\"Loss/train\", avg_train_loss, epoch)\n",
    "\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"Train_EN/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"Train_EN/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "    #EVALUATION\n",
    "    p = []\n",
    "    g = []\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_dev):\n",
    "    \n",
    "      #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "            #------------------FILTERING SUB-WORDS----------------------\n",
    "            subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "            word_emebedding = []\n",
    "            for i in range(n): \n",
    "                subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "                flag = subtoken_mask[0,i,0]\n",
    "                if flag :\n",
    "                    continue\n",
    "                else :\n",
    "                    word_emebedding.append(subwords_embedding)\n",
    "            word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "            #-------------------------FORWARD----------------------------------\n",
    "            x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "                        perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "                        predicate_index = sample_batched[\"predicate_index\"],\n",
    "                        pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "                        predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])   \n",
    "\n",
    "\n",
    "            b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "            loss = nll_loss(logSotfMax(x),gt)\n",
    "            total_loss = total_loss + loss\n",
    "            #-------------------------RESULT STORING----------------------------------\n",
    "            predicted = torch.argmax(x, dim=1)\n",
    "            p += predicted.tolist()\n",
    "            g += gt.tolist()\n",
    "            counter += 1 \n",
    "    \n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    avg_eval_loss = total_loss/counter\n",
    "\n",
    "    if avg_eval_loss < max_val_loss:\n",
    "        max_val_loss = avg_eval_loss\n",
    "    else :\n",
    "        patience_counter += 1\n",
    "    \n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    if patience_counter >= patience :  \n",
    "\n",
    "\n",
    "        print(\"Early stopping at epoch : \",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 avg eval :\",f1_avg)\n",
    "        break\n",
    "    else :\n",
    "        print(\"EPOCHS :\",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 avg eval :\",f1_avg)\n",
    "    \n",
    "\n",
    "    writer.add_scalar(\"Loss/validation\", avg_eval_loss, epoch)\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"Eval_EN/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"Eval_EN/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if f1_avg > f1_score_max:\n",
    "        f1_score_max = f1_avg\n",
    "        print(\"SAVED :\",PATH)\n",
    "        torch.save(model.state_dict(),PATH)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Transfert Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning over Structural Information over English dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading English Pretrained Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Arg_Classifier(\n",
       "  (bi_lstm_portable): LSTM(132, 50, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  (embedding_predicate_flag): Embedding(2, 32, max_norm=True)\n",
       "  (embedding_predicate): Embedding(304, False, max_norm=True)\n",
       "  (embedding_pos): Embedding(18, 100, max_norm=True)\n",
       "  (bi_lstm): LSTM(900, 50, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  (dropout_in_classifier): Dropout(p=0.4, inplace=False)\n",
       "  (Relu): ReLU()\n",
       "  (Sigmoid): Sigmoid()\n",
       "  (linear0): Linear(in_features=300, out_features=675, bias=True)\n",
       "  (linear1): Linear(in_features=675, out_features=135, bias=True)\n",
       "  (linear2): Linear(in_features=135, out_features=27, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#note that parameter EN is only used for tracking on which dataset was/is trained\n",
    "model = Arg_Classifier(\"EN\",cfg)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language constained training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_language_constrains()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "currentDateAndTime = datetime.now()\n",
    "_id = str(currentDateAndTime.year)+\"_\"+str(currentDateAndTime.month)+\"_\"+str(currentDateAndTime.day)+\"_\"+str(currentDateAndTime.hour)+\"_\"+str(currentDateAndTime.minute)+\"_\"+str(currentDateAndTime.second)\n",
    "_id =  _id +\"Language constained training\"\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 0.000005)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "\n",
    "logSotfMax = torch.nn.LogSoftmax(dim=1)\n",
    "nll_loss = torch.nn.NLLLoss()\n",
    "\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "dataloader_dev = DataLoader(dev_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "mapping = dataloader_train.dataset.args_roles\n",
    "\n",
    "auto_model.eval()\n",
    "\n",
    "EPOCHS = 200\n",
    "patience_counter = 0\n",
    "patience = 5\n",
    "max_val_loss = 9999\n",
    "f1_score_max = 0\n",
    "output_path = \"saved\"\n",
    "model_name = \"model_\"+_id+\".pth\"\n",
    "PATH = os.path.join(output_path,model_name)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    #TRAINING\n",
    "    p = []\n",
    "    g = []\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_train):\n",
    "        #print(sample_batched)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "       \n",
    "        #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "        #------------------FILTERING SUB-WORDS----------------------\n",
    "        subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "        word_emebedding = []\n",
    "        for i in range(n):\n",
    "            subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "            flag = subtoken_mask[0,i,0]\n",
    "            if flag :\n",
    "                continue\n",
    "            else :\n",
    "                word_emebedding.append(subwords_embedding)\n",
    "        word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "        #-------------------------FORWARD/BACKWARD----------------------------------\n",
    "        x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "            perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "            predicate_index = sample_batched[\"predicate_index\"],\n",
    "            pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "            predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])        \n",
    "        b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "        loss = nll_loss(logSotfMax(x),gt)\n",
    "        total_loss = total_loss + loss\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "        counter += 1 \n",
    "            \n",
    "\n",
    "        #-------------------------RESULT STORING----------------------------------\n",
    "        predicted = torch.argmax(x, dim=1)\n",
    "        p += predicted.tolist()\n",
    "        g += gt.tolist()\n",
    "    \n",
    "\n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    print(\"Epochs n.\", epoch)\n",
    "    print(\"F1 train:\",f1_score(g, p, average=None))\n",
    "    scheduler.step()\n",
    "    avg_train_loss = total_loss/counter\n",
    "    writer.add_scalar(\"Loss/train\", avg_train_loss, epoch)\n",
    "\n",
    "\n",
    "\n",
    "    #EVALUATION\n",
    "    p = []\n",
    "    g = []\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_dev):\n",
    "    \n",
    "      #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "            #------------------FILTERING SUB-WORDS----------------------\n",
    "            subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "            word_emebedding = []\n",
    "            for i in range(n): \n",
    "                subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "                flag = subtoken_mask[0,i,0]\n",
    "                if flag :\n",
    "                    continue\n",
    "                else :\n",
    "                    word_emebedding.append(subwords_embedding)\n",
    "            word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "            #-------------------------FORWARD----------------------------------\n",
    "            x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "                        perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "                        predicate_index = sample_batched[\"predicate_index\"],\n",
    "                        pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "                        predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])   \n",
    "\n",
    "\n",
    "            b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "            loss = nll_loss(logSotfMax(x),gt)\n",
    "            total_loss = total_loss + loss\n",
    "            #-------------------------RESULT STORING----------------------------------\n",
    "            predicted = torch.argmax(x, dim=1)\n",
    "            p += predicted.tolist()\n",
    "            g += gt.tolist()\n",
    "            counter += 1 \n",
    "    \n",
    "    #-------------------------RESULTS----------------------------------\n",
    "\n",
    "    avg_eval_loss = total_loss/counter\n",
    "\n",
    "    if avg_eval_loss < max_val_loss:\n",
    "        max_val_loss = avg_eval_loss\n",
    "    else :\n",
    "        patience_counter += 1\n",
    "    \n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    if patience_counter >= patience :  \n",
    "\n",
    "\n",
    "        print(\"Early stopping at epoch : \",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 eval :\",f1_avg)\n",
    "        break\n",
    "    else :\n",
    "        print(\"EPOCHS :\",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 eval :\",f1_avg)\n",
    "    \n",
    "\n",
    "    writer.add_scalar(\"Loss/validation\", avg_eval_loss, epoch)\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"Eval_EN/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"Eval_EN/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if f1_avg > f1_score_max:\n",
    "        f1_score_max = f1_avg\n",
    "        torch.save(model.state_dict(),PATH)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spanish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Spanish Dataset\n",
    "bert-base-multilingual-cased\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model class is      : <class 'transformers.models.bert.modeling_bert.BertModel'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer.json: 100%|| 1.87M/1.87M [00:02<00:00, 733kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model class is      : <class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import random\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "\n",
    "auto_model = AutoModel.from_pretrained(\"bert-base-multilingual-cased\",output_hidden_states=True)\n",
    "print(f\"\\nmodel class is      : {type(auto_model)}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "print(f\"\\nmodel class is      : {type(tokenizer)}\")\n",
    "\n",
    "\n",
    "\n",
    "class SRL(Dataset):\n",
    " \n",
    "    def __init__(self,language,path,args_roles = None,pos_list = None) -> None:\n",
    "\n",
    "        self.path_root = 'data'\n",
    "        self.load_data(language,path)\n",
    "        if args_roles is None :\n",
    "            self.args_roles,self.list_broken_id = self.list_arg_roles()\n",
    "        else : \n",
    "            self.args_roles = args_roles\n",
    "            _,self.list_broken_id = self.list_arg_roles()\n",
    "        \n",
    "\n",
    "        if pos_list is None :\n",
    "            self.pos_list,_ = self.list_pos()\n",
    "            self.pos_list.append(\"Nothing\")\n",
    "        else : \n",
    "            self.pos_list = pos_list\n",
    "        \n",
    "\n",
    "\n",
    "        self.predicate_dis,_ = self.list_predicate_roles()\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(self.pos_list)\n",
    "\n",
    "        self.predicate_dis.append(\"Nothing\")\n",
    "\n",
    "    def load_data(self,language,mode):\n",
    "        \n",
    "        mode = mode+\".json\"\n",
    "        path = os.path.join(self.path_root,language,mode)\n",
    "        data_file = open(path)\n",
    "       \n",
    "        data_ = json.load(data_file)\n",
    "\n",
    "        list_data = []\n",
    "\n",
    "        for data in data_:\n",
    "            list_data.append(data_[data])\n",
    "        \n",
    "\n",
    "        self.data = list_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, id : int):\n",
    "\n",
    "        flag = False\n",
    "        if id in self.list_broken_id :\n",
    "            flag = True\n",
    "            while flag == True:\n",
    "\n",
    "                rand_id = random.randint(0, len(self.data)-1)\n",
    "                \n",
    "                if rand_id in self.list_broken_id :\n",
    "                    pass\n",
    "                else :\n",
    "                    flag = False\n",
    "                    id = rand_id        \n",
    "\n",
    "\n",
    "        data = self.pre_processing(self.data[id])\n",
    "        data = self.processig(data)\n",
    "        return data\n",
    "        \n",
    "    def pre_processing(self, data:dict):\n",
    "        data_list = []\n",
    "        for role in data[\"roles\"]:\n",
    "            dictionary = dict()\n",
    "            dictionary[\"words\"] = data[\"words\"]\n",
    "            dictionary[\"role\"] = data[\"roles\"][role]\n",
    "            dictionary[\"pre_idx\"] = role\n",
    "            dictionary[\"pos_tags\"] = data[\"pos_tags\"]\n",
    "            dictionary[\"predicate_meaning\"] = data[\"predicates\"]\n",
    "            data_list.append(dictionary)    \n",
    "        return data_list\n",
    "    \n",
    "    def processig(self,data_list:list):\n",
    "        \n",
    "        for dictionary in data_list:\n",
    "\n",
    "            #dictionary[\"words\"] = data[\"words\"]\n",
    "            dictionary[\"gt_arg_identification\"] = self.arg_id(dictionary[\"role\"])\n",
    "            dictionary[\"gt_arg_classification\"] = self.arg_class(dictionary[\"role\"])\n",
    "            dictionary[\"pos_idx\"] = self.pos_idx(dictionary[\"pos_tags\"])\n",
    "            dictionary[\"predicate_meaning_idx\"] = self.predicate_meaning_idx(dictionary[\"predicate_meaning\"])\n",
    "        \n",
    "        return data_list\n",
    "   \n",
    "    def list_arg_roles(self):\n",
    "        list_roles = []\n",
    "        list_broken_id = []\n",
    "        for i,element in enumerate(self.data):\n",
    "            flag = True\n",
    "            try : roles = element[\"roles\"]\n",
    "            except : flag = False\n",
    "            if flag :\n",
    "                for e in roles:\n",
    "                    sentence = element[\"roles\"][e]\n",
    "\n",
    "                    for word in sentence:\n",
    "                        \n",
    "                        list_roles.append(word)\n",
    "                list_roles = list(set(list_roles))\n",
    "            else : \n",
    "                list_broken_id.append(i)\n",
    "        return list_roles,list_broken_id\n",
    "\n",
    "    def list_predicate_roles(self):\n",
    "        list_predicate_roles = []\n",
    "        list_broken_id = []\n",
    "        for i,element in enumerate(self.data):\n",
    "            flag = True\n",
    "            try : predicates = element[\"predicates\"]\n",
    "            except : flag = False\n",
    "            if flag :\n",
    "                for pre in predicates:\n",
    "                    list_predicate_roles.append(pre)\n",
    "                list_predicate_roles = list(set(list_predicate_roles))\n",
    "            else : \n",
    "                list_broken_id.append(i)\n",
    "        return list_predicate_roles,list_broken_id\n",
    "\n",
    "    def list_pos(self):\n",
    "        list_pos = []\n",
    "        list_broken_id = []\n",
    "        for i,element in enumerate(self.data):\n",
    "            flag = True\n",
    "            try : pos = element[\"pos_tags\"]\n",
    "            except : flag = False\n",
    "            if flag :\n",
    "                for e in pos:\n",
    "                    list_pos.append(e)\n",
    "                list_pos = list(set(list_pos))\n",
    "            else : \n",
    "                list_broken_id.append(i)\n",
    "        return list_pos,list_broken_id\n",
    "  \n",
    "    def arg_class(self,role:list):\n",
    "        list_idxs = []\n",
    "        for element in role:\n",
    "            list_idxs.append(self.args_roles.index(element))\n",
    "        \n",
    "\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "\n",
    "    def arg_id(self,role:dict):\n",
    "        list_idxs = []\n",
    "        for element in role:\n",
    "            if element == \"_\":\n",
    "                list_idxs.append(0)\n",
    "            else :\n",
    "                list_idxs.append(1)\n",
    "\n",
    "        \n",
    "\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "\n",
    "    def pos_idx(self,pos_tags:dict):\n",
    "        list_idxs = []\n",
    "        list_idxs.append(self.pos_list.index(\"Nothing\"))\n",
    "\n",
    "        for element in pos_tags:\n",
    "            list_idxs.append(self.pos_list.index(element))\n",
    "        \n",
    "        list_idxs.append(self.pos_list.index(\"Nothing\"))\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "    \n",
    "    def predicate_meaning_idx(self,predicate_meaning_tags:dict):\n",
    "        list_idxs = []\n",
    "        list_idxs.append(self.predicate_dis.index(\"Nothing\"))\n",
    "\n",
    "        for element in predicate_meaning_tags:\n",
    "            list_idxs.append(self.predicate_dis.index(element))\n",
    "        \n",
    "        list_idxs.append(self.predicate_dis.index(\"Nothing\"))\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64) \n",
    "\n",
    "    \n",
    "# here we define our collate function\n",
    "def collate_fn(batch) -> Dict[str, torch.Tensor]:\n",
    "    #print(batch)\n",
    "    input = dict() \n",
    "    batch_sentence = [] \n",
    "    #print(len(batch))\n",
    "    for period in batch:\n",
    "        for sentence in period :\n",
    "        \n",
    "            #print(len(sentence[0][\"words\"]))\n",
    "            pre_idx = int(sentence[\"pre_idx\"])\n",
    "            \n",
    "\n",
    "            predicate = sentence[\"words\"][pre_idx]\n",
    "\n",
    "            text = \" \".join(sentence[\"words\"])\n",
    "            tokens: list[str] = text.split()\n",
    "            predicate: list[str] = predicate.split()\n",
    "\n",
    "            #text = sentence[0][\"words\"]\n",
    "            \n",
    "            t = (tokens,predicate)\n",
    "\n",
    "            batch_sentence.append(t)\n",
    "            #print(batch_sentence)\n",
    "\n",
    "    batch_output = tokenizer.batch_encode_plus(batch_sentence,padding=True,is_split_into_words=True, truncation=True,return_offsets_mapping=True, return_tensors=\"pt\")\n",
    "    #print(batch_output.keys())\n",
    "\n",
    "\n",
    "    gt = dict()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    for period in batch:\n",
    "\n",
    "        list_positional_predicate_encoding = []\n",
    "        list_arg_gt = []\n",
    "        list_predicate_index = [] \n",
    "        list_pos_index = [] \n",
    "        list_predicate_meaning_index = []\n",
    "\n",
    "        for sentence in period:\n",
    "            #positional_encoding\n",
    "            #+2 per il CLS iniziale ad SEP finale\n",
    "            sentence_words_lenght =  len(sentence[\"words\"])\n",
    "            positional_predicate_encoding = torch.zeros(1,sentence_words_lenght+2)\n",
    "            #+1 per il CLS iniziale\n",
    "            pre_idx = int(sentence[\"pre_idx\"])\n",
    "            positional_predicate_encoding[:,pre_idx+1] = 1\n",
    "            list_positional_predicate_encoding.append(positional_predicate_encoding)\n",
    "            #print(\"positional_prefix_encoding\",positional_predicate_encoding)\n",
    "            list_predicate_index.append(pre_idx)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            pos = torch.unsqueeze(sentence[\"pos_idx\"],dim = 0)\n",
    "            list_pos_index.append(pos)\n",
    "            predicate_meaning_idxs = torch.unsqueeze(sentence[\"predicate_meaning_idx\"],dim = 0)\n",
    "            list_predicate_meaning_index.append(predicate_meaning_idxs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #note CLS and SEP are discharder after Bi-LSTM, the Classifier takes in input only wokrds hidden state embedding\n",
    "            arg_gt = torch.unsqueeze(sentence[\"gt_arg_classification\"],dim = 0)\n",
    "            list_arg_gt.append(arg_gt)\n",
    "        \n",
    "\n",
    "    list_arg_gt = torch.cat(list_arg_gt,dim = 0)\n",
    "    list_pos_index = torch.cat(list_pos_index,dim = 0)\n",
    "    list_predicate_meaning_index = torch.cat(list_predicate_meaning_index,dim = 0)\n",
    "    list_positional_predicate_encoding = torch.cat(list_positional_predicate_encoding,dim = 0)\n",
    "    gt[\"arg_gt\"] = list_arg_gt\n",
    "    input[\"predicate_index\"] = list_predicate_index\n",
    "    input[\"pos_index\"] = list_pos_index.long()\n",
    "    input[\"predicate_meaning_idx\"] = list_predicate_meaning_index.long()\n",
    "    offset = batch_output.pop(\"offset_mapping\")\n",
    "    input[\"BERT_input\"] = batch_output\n",
    "    input[\"positional_encoding\"] = list_positional_predicate_encoding.long()\n",
    "    input[\"offset_mapping\"] = offset\n",
    "    input[\"gt\"] = gt\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    return input\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['experiencer', 'theme', 'patient', '_', 'material', 'agent', 'co-patient', 'value', 'destination', 'purpose', 'goal', 'co-theme', 'source', 'cause', 'time', 'asset', 'extent', 'recipient', 'stimulus', 'attribute', 'instrument', 'topic', 'co-agent', 'beneficiary', 'product', 'location', 'result']\n",
      "['experiencer', 'theme', 'patient', '_', 'material', 'agent', 'co-patient', 'value', 'destination', 'purpose', 'goal', 'co-theme', 'source', 'cause', 'time', 'asset', 'extent', 'recipient', 'stimulus', 'attribute', 'instrument', 'topic', 'co-agent', 'beneficiary', 'product', 'location', 'result']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#train_dataset = SRL(\"EN\",\"train\")\n",
    "\n",
    "#note here we are directly loading args_roles mapping as computed before the the dasetet where we have perfomerd \n",
    "#EN and ES dataset should have the same consistency in generating \n",
    "train_dataset = SRL(\"ES\",\"train\",train_dataset.args_roles,train_dataset.pos_list)\n",
    "#same mapping should be used in both the dataset\n",
    "dev_dataset = SRL(\"ES\",\"dev\",train_dataset.args_roles,train_dataset.pos_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English-Spanish attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Arg_Classifier(\n",
       "  (bi_lstm_portable): LSTM(132, 50, num_layers=2, batch_first=True, dropout=0.4, bidirectional=True)\n",
       "  (embedding_predicate_flag): Embedding(2, 32, max_norm=True)\n",
       "  (embedding_predicate): Embedding(304, False, max_norm=True)\n",
       "  (embedding_pos): Embedding(18, 100, max_norm=True)\n",
       "  (bi_lstm): LSTM(900, 50, num_layers=2, batch_first=True, dropout=0.4, bidirectional=True)\n",
       "  (dropout_language_constraint): Dropout(p=0.6, inplace=False)\n",
       "  (dropout_in_classifier): Dropout(p=0.4, inplace=False)\n",
       "  (Relu): ReLU()\n",
       "  (Sigmoid): Sigmoid()\n",
       "  (linear0): Linear(in_features=300, out_features=675, bias=True)\n",
       "  (linear1): Linear(in_features=675, out_features=135, bias=True)\n",
       "  (linear2): Linear(in_features=135, out_features=27, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#note that parameter EN is only used for tracking on which dataset was/is trained, and activate loading of the pretrained head\n",
    "#load the fine-tuned model over english\n",
    "PATH = \"saved/model_2022_12_18_21_33_45.pth\"\n",
    "model = Arg_Classifier(\"ES\",cfg)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.train().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs n. 0\n",
      "F1 train: [0.         0.41519926 0.00668896 0.98159286 0.         0.38427948\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9458893102533754\n",
      "identification {'true_positives': 657, 'false_positives': 127, 'false_negatives': 1495, 'precision': 0.8380102040816326, 'recall': 0.30529739776951675, 'f1': 0.4475476839237057}\n",
      "classification_result {'true_positives': 401, 'false_positives': 383, 'false_negatives': 1751, 'precision': 0.5114795918367347, 'recall': 0.18633828996282528, 'f1': 0.2731607629427793}\n",
      "EPOCHS : 0\n",
      "F1 eval : [0.         0.50540098 0.         0.98476308 0.25635359 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.951819105330271\n",
      "identification {'true_positives': 1784, 'false_positives': 270, 'false_negatives': 3098, 'precision': 0.8685491723466408, 'recall': 0.3654240065546907, 'f1': 0.5144175317185699}\n",
      "classification_result {'true_positives': 1004, 'false_positives': 1050, 'false_negatives': 3878, 'precision': 0.48880233690360275, 'recall': 0.20565342072920934, 'f1': 0.2895040369088812}\n",
      "SAVED : saved/model_2022_12_18_21_55_17.pth\n",
      "Epochs n. 1\n",
      "F1 train: [0.         0.4831553  0.07228916 0.98556403 0.         0.58646617\n",
      " 0.         0.         0.         0.         0.01639344 0.\n",
      " 0.         0.         0.         0.         0.14814815 0.\n",
      " 0.         0.         0.11940299 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9544334404786856\n",
      "identification {'true_positives': 1012, 'false_positives': 127, 'false_negatives': 1133, 'precision': 0.8884986830553117, 'recall': 0.4717948717948718, 'f1': 0.6163215590742995}\n",
      "classification_result {'true_positives': 635, 'false_positives': 504, 'false_negatives': 1510, 'precision': 0.5575065847234416, 'recall': 0.29603729603729606, 'f1': 0.38672350791717425}\n",
      "EPOCHS : 1\n",
      "F1 eval : [0.         0.51134548 0.         0.98473388 0.23535511 0.\n",
      " 0.         0.         0.         0.18390805 0.         0.\n",
      " 0.         0.         0.         0.         0.03448276 0.\n",
      " 0.         0.         0.01286174 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9524853334057606\n",
      "identification {'true_positives': 1901, 'false_positives': 415, 'false_negatives': 3022, 'precision': 0.8208117443868739, 'recall': 0.38614665854153973, 'f1': 0.5252106644564167}\n",
      "classification_result {'true_positives': 1065, 'false_positives': 1251, 'false_negatives': 3858, 'precision': 0.45984455958549225, 'recall': 0.21633150517976843, 'f1': 0.29423953584749274}\n",
      "SAVED : saved/model_2022_12_18_21_55_17.pth\n",
      "Epochs n. 2\n",
      "F1 train: [0.         0.49960349 0.07902736 0.98612618 0.         0.6159292\n",
      " 0.         0.         0.         0.         0.20689655 0.\n",
      " 0.         0.         0.         0.         0.19354839 0.\n",
      " 0.         0.         0.25333333 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.956746441350736\n",
      "identification {'true_positives': 1113, 'false_positives': 182, 'false_negatives': 1033, 'precision': 0.8594594594594595, 'recall': 0.5186393289841565, 'f1': 0.6469049694856146}\n",
      "classification_result {'true_positives': 722, 'false_positives': 573, 'false_negatives': 1424, 'precision': 0.5575289575289575, 'recall': 0.3364398881640261, 'f1': 0.41964545190351643}\n",
      "EPOCHS : 2\n",
      "F1 eval : [0.         0.51954603 0.09116809 0.98472378 0.14238411 0.\n",
      " 0.         0.         0.         0.23268698 0.         0.\n",
      " 0.         0.         0.         0.         0.0361991  0.\n",
      " 0.         0.         0.16374269 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9521032155241269\n",
      "identification {'true_positives': 1926, 'false_positives': 421, 'false_negatives': 2944, 'precision': 0.8206220707285897, 'recall': 0.395482546201232, 'f1': 0.5337397810724678}\n",
      "classification_result {'true_positives': 1059, 'false_positives': 1288, 'false_negatives': 3811, 'precision': 0.4512143161482744, 'recall': 0.21745379876796714, 'f1': 0.293473742552307}\n",
      "Epochs n. 3\n",
      "F1 train: [0.         0.4892772  0.16853933 0.98702312 0.         0.66314864\n",
      " 0.         0.         0.         0.         0.27329193 0.\n",
      " 0.         0.         0.         0.         0.19512195 0.\n",
      " 0.         0.         0.28187919 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9589274068393385\n",
      "identification {'true_positives': 1178, 'false_positives': 168, 'false_negatives': 963, 'precision': 0.87518573551263, 'recall': 0.5502101821578702, 'f1': 0.6756524232864928}\n",
      "classification_result {'true_positives': 770, 'false_positives': 576, 'false_negatives': 1371, 'precision': 0.5720653789004457, 'recall': 0.3596450256889304, 'f1': 0.4416403785488959}\n",
      "EPOCHS : 3\n",
      "F1 eval : [0.         0.52654867 0.12273361 0.98440412 0.1816387  0.\n",
      " 0.         0.         0.         0.25464191 0.         0.\n",
      " 0.         0.         0.         0.         0.18587361 0.\n",
      " 0.         0.         0.33248082 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9535452873154773\n",
      "identification {'true_positives': 2046, 'false_positives': 625, 'false_negatives': 2839, 'precision': 0.7660052414825907, 'recall': 0.41883316274309107, 'f1': 0.5415563790365272}\n",
      "classification_result {'true_positives': 1199, 'false_positives': 1472, 'false_negatives': 3686, 'precision': 0.4488955447397978, 'recall': 0.24544524053224157, 'f1': 0.31736368448914776}\n",
      "SAVED : saved/model_2022_12_18_21_55_17.pth\n",
      "Epochs n. 4\n",
      "F1 train: [0.         0.49594814 0.22051282 0.98784859 0.         0.70686767\n",
      " 0.         0.         0.         0.         0.34285714 0.\n",
      " 0.         0.         0.         0.         0.17241379 0.\n",
      " 0.         0.         0.32051282 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9609269395326534\n",
      "identification {'true_positives': 1255, 'false_positives': 162, 'false_negatives': 899, 'precision': 0.8856739590684545, 'recall': 0.5826369545032498, 'f1': 0.7028843461215346}\n",
      "classification_result {'true_positives': 836, 'false_positives': 581, 'false_negatives': 1318, 'precision': 0.5899788285109386, 'recall': 0.3881151346332405, 'f1': 0.46821618594231307}\n",
      "EPOCHS : 4\n",
      "F1 eval : [0.         0.51295681 0.14188267 0.98365281 0.14747859 0.\n",
      " 0.         0.         0.         0.23561644 0.         0.\n",
      " 0.         0.         0.         0.         0.06278027 0.\n",
      " 0.         0.         0.38834951 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9521465915526715\n",
      "identification {'true_positives': 1914, 'false_positives': 649, 'false_negatives': 2978, 'precision': 0.7467811158798283, 'recall': 0.3912510220768602, 'f1': 0.5134808853118713}\n",
      "classification_result {'true_positives': 1109, 'false_positives': 1454, 'false_negatives': 3783, 'precision': 0.43269605930550137, 'recall': 0.22669664758789862, 'f1': 0.29751844399731725}\n",
      "Epochs n. 5\n",
      "F1 train: [0.         0.52560873 0.3531746  0.98907995 0.         0.73452769\n",
      " 0.         0.         0.         0.         0.3583815  0.\n",
      " 0.         0.         0.         0.         0.32061069 0.\n",
      " 0.         0.         0.38323353 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.964453422178617\n",
      "identification {'true_positives': 1364, 'false_positives': 157, 'false_negatives': 814, 'precision': 0.8967784352399737, 'recall': 0.6262626262626263, 'f1': 0.7374966207082995}\n",
      "classification_result {'true_positives': 937, 'false_positives': 584, 'false_negatives': 1241, 'precision': 0.6160420775805391, 'recall': 0.43021120293847565, 'f1': 0.5066234117329007}\n",
      "Early stopping at epoch :  5\n",
      "F1 eval : [0.         0.51528998 0.23322332 0.98259752 0.04374703 0.\n",
      " 0.         0.         0.         0.25668449 0.         0.\n",
      " 0.         0.         0.         0.         0.21201413 0.\n",
      " 0.         0.         0.45248869 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9508463436472356\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "currentDateAndTime = datetime.now()\n",
    "_id = str(currentDateAndTime.year)+\"_\"+str(currentDateAndTime.month)+\"_\"+str(currentDateAndTime.day)+\"_\"+str(currentDateAndTime.hour)+\"_\"+str(currentDateAndTime.minute)+\"_\"+str(currentDateAndTime.second)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#optimizer = torch.optim.Adam(model.parameters(),lr = 0.000005)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "\n",
    "logSotfMax = torch.nn.LogSoftmax(dim=1)\n",
    "nll_loss = torch.nn.NLLLoss()\n",
    "\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "dataloader_dev = DataLoader(dev_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "mapping = dataloader_train.dataset.args_roles\n",
    "\n",
    "auto_model.eval()\n",
    "auto_model.cuda()\n",
    "\n",
    "EPOCHS = 200\n",
    "patience_counter = 0\n",
    "patience = 5\n",
    "max_val_loss = 9999\n",
    "f1_score_max = 0\n",
    "output_path = \"saved\"\n",
    "model_name = \"model_\"+_id+\".pth\"\n",
    "PATH = os.path.join(output_path,model_name)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    #TRAINING\n",
    "    p = []\n",
    "    g = []\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_train):\n",
    "        #print(sample_batched)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "       \n",
    "        #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "        #------------------FILTERING SUB-WORDS----------------------\n",
    "        subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "        word_emebedding = []\n",
    "        for i in range(n):\n",
    "            subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "            flag = subtoken_mask[0,i,0]\n",
    "            if flag :\n",
    "                continue\n",
    "            else :\n",
    "                word_emebedding.append(subwords_embedding)\n",
    "        word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "        #-------------------------FORWARD/BACKWARD----------------------------------\n",
    "        x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "            perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "            predicate_index = sample_batched[\"predicate_index\"],\n",
    "            pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "            predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])        \n",
    "        b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "        loss = nll_loss(logSotfMax(x),gt)\n",
    "        total_loss = total_loss + loss\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "        counter += 1 \n",
    "            \n",
    "\n",
    "        #-------------------------RESULT STORING----------------------------------\n",
    "        predicted = torch.argmax(x, dim=1)\n",
    "        p += predicted.tolist()\n",
    "        g += gt.tolist()\n",
    "    \n",
    "\n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    scheduler.step()\n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    print(\"Epochs n.\", epoch)\n",
    "    print(\"F1 train:\",f1)\n",
    "    print(\"F1 avg train:\",f1_avg)\n",
    "    \n",
    "    avg_train_loss = total_loss/counter\n",
    "    writer.add_scalar(\"EN_Loss_ES/train\", avg_train_loss, epoch)\n",
    "\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"EN_Train_ES/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"EN_Train_ES/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "    #EVALUATION\n",
    "    p = []\n",
    "    g = []\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_dev):\n",
    "    \n",
    "      #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "            #------------------FILTERING SUB-WORDS----------------------\n",
    "            subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "            word_emebedding = []\n",
    "            for i in range(n): \n",
    "                subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "                flag = subtoken_mask[0,i,0]\n",
    "                if flag :\n",
    "                    continue\n",
    "                else :\n",
    "                    word_emebedding.append(subwords_embedding)\n",
    "            word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "            #-------------------------FORWARD----------------------------------\n",
    "            x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "                        perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "                        predicate_index = sample_batched[\"predicate_index\"],\n",
    "                        pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "                        predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])   \n",
    "\n",
    "\n",
    "            b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "            loss = nll_loss(logSotfMax(x),gt)\n",
    "            total_loss = total_loss + loss\n",
    "            #-------------------------RESULT STORING----------------------------------\n",
    "            predicted = torch.argmax(x, dim=1)\n",
    "            p += predicted.tolist()\n",
    "            g += gt.tolist()\n",
    "            counter += 1 \n",
    "    \n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    avg_eval_loss = total_loss/counter\n",
    "\n",
    "    if avg_eval_loss < max_val_loss:\n",
    "        max_val_loss = avg_eval_loss\n",
    "    else :\n",
    "        patience_counter += 1\n",
    "    \n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    if patience_counter >= patience :  \n",
    "\n",
    "\n",
    "        print(\"Early stopping at epoch : \",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 avg eval :\",f1_avg)\n",
    "        break\n",
    "    else :\n",
    "        print(\"EPOCHS :\",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 avg eval :\",f1_avg)\n",
    "    \n",
    "\n",
    "    writer.add_scalar(\"EN_Loss_ES/validation\", avg_eval_loss, epoch)\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"EN_Eval_ES/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"EN_Eval_ES/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if f1_avg > f1_score_max:\n",
    "        f1_score_max = f1_avg\n",
    "        print(\"SAVED :\",PATH)\n",
    "        torch.save(model.state_dict(),PATH)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare without Transfert Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs n. 0\n",
      "F1 train: [0.         0.19946809 0.         0.9768764  0.         0.0295421\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9336015019908596\n",
      "identification {'true_positives': 172, 'false_positives': 71, 'false_negatives': 1987, 'precision': 0.7078189300411523, 'recall': 0.07966651227420102, 'f1': 0.14321398834304747}\n",
      "classification_result {'true_positives': 85, 'false_positives': 158, 'false_negatives': 2074, 'precision': 0.3497942386831276, 'recall': 0.03937007874015748, 'f1': 0.070774354704413}\n",
      "EPOCHS : 0\n",
      "F1 eval : [0.         0.23416618 0.         0.9777718  0.00265252 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9389493973815146\n",
      "identification {'true_positives': 1030, 'false_positives': 1096, 'false_negatives': 3858, 'precision': 0.48447789275634995, 'recall': 0.2107201309328969, 'f1': 0.2936983176504135}\n",
      "classification_result {'true_positives': 405, 'false_positives': 1721, 'false_negatives': 4483, 'precision': 0.1904985888993415, 'recall': 0.08285597381342062, 'f1': 0.11548331907613345}\n",
      "SAVED : saved/model_2022_12_18_22_2_58.pth\n",
      "Epochs n. 1\n",
      "F1 train: [0.         0.3872549  0.         0.98172752 0.         0.42610365\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9464882649310143\n",
      "identification {'true_positives': 798, 'false_positives': 268, 'false_negatives': 1337, 'precision': 0.7485928705440901, 'recall': 0.3737704918032787, 'f1': 0.4985941893158388}\n",
      "classification_result {'true_positives': 459, 'false_positives': 607, 'false_negatives': 1676, 'precision': 0.43058161350844276, 'recall': 0.21498829039812647, 'f1': 0.2867853795688848}\n",
      "EPOCHS : 1\n",
      "F1 eval : [0.         0.23666396 0.         0.97658545 0.00478469 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9376621220958927\n",
      "identification {'true_positives': 1127, 'false_positives': 1421, 'false_negatives': 3759, 'precision': 0.4423076923076923, 'recall': 0.23065902578796563, 'f1': 0.3032015065913371}\n",
      "classification_result {'true_positives': 441, 'false_positives': 2107, 'false_negatives': 4445, 'precision': 0.17307692307692307, 'recall': 0.09025787965616046, 'f1': 0.11864406779661017}\n",
      "Epochs n. 2\n",
      "F1 train: [0.         0.44530046 0.         0.98438053 0.         0.60050463\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.952145225902215\n",
      "identification {'true_positives': 1022, 'false_positives': 243, 'false_negatives': 1124, 'precision': 0.807905138339921, 'recall': 0.4762348555452004, 'f1': 0.5992377601876282}\n",
      "classification_result {'true_positives': 646, 'false_positives': 619, 'false_negatives': 1500, 'precision': 0.5106719367588933, 'recall': 0.30102516309412863, 'f1': 0.37877455291703316}\n",
      "EPOCHS : 2\n",
      "F1 eval : [0.         0.26013195 0.         0.9780147  0.0071599  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9395047264553521\n",
      "identification {'true_positives': 1022, 'false_positives': 1031, 'false_negatives': 3848, 'precision': 0.4978080857282026, 'recall': 0.20985626283367556, 'f1': 0.2952477249747219}\n",
      "classification_result {'true_positives': 420, 'false_positives': 1633, 'false_negatives': 4450, 'precision': 0.20457866536775451, 'recall': 0.08624229979466119, 'f1': 0.12133468149646107}\n",
      "SAVED : saved/model_2022_12_18_22_2_58.pth\n",
      "Epochs n. 3\n",
      "F1 train: [0.         0.47011952 0.02555911 0.98541674 0.         0.63305785\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9541615111910255\n",
      "identification {'true_positives': 1065, 'false_positives': 200, 'false_negatives': 1089, 'precision': 0.841897233201581, 'recall': 0.49442896935933145, 'f1': 0.6229891781222578}\n",
      "classification_result {'true_positives': 682, 'false_positives': 583, 'false_negatives': 1472, 'precision': 0.5391304347826087, 'recall': 0.3166202414113278, 'f1': 0.39894706054401874}\n",
      "EPOCHS : 3\n",
      "F1 eval : [0.         0.28145455 0.02816901 0.97774314 0.00762112 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9396751709187108\n",
      "identification {'true_positives': 871, 'false_positives': 950, 'false_negatives': 4019, 'precision': 0.4783086216364635, 'recall': 0.17811860940695295, 'f1': 0.2595738340038742}\n",
      "classification_result {'true_positives': 404, 'false_positives': 1417, 'false_negatives': 4486, 'precision': 0.22185612300933552, 'recall': 0.08261758691206544, 'f1': 0.12039934436000595}\n",
      "SAVED : saved/model_2022_12_18_22_2_58.pth\n",
      "Epochs n. 4\n",
      "F1 train: [0.         0.51370399 0.05590062 0.98651971 0.         0.69414674\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.07843137 0.\n",
      " 0.         0.         0.03333333 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9570333876174468\n",
      "identification {'true_positives': 1139, 'false_positives': 185, 'false_negatives': 994, 'precision': 0.8602719033232629, 'recall': 0.5339896858884201, 'f1': 0.658952849291293}\n",
      "classification_result {'true_positives': 764, 'false_positives': 560, 'false_negatives': 1369, 'precision': 0.5770392749244713, 'recall': 0.35818096577590247, 'f1': 0.4420017356089094}\n",
      "EPOCHS : 4\n",
      "F1 eval : [0.         0.27561328 0.01457726 0.9769951  0.00616333 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9388430347981687\n",
      "identification {'true_positives': 843, 'false_positives': 1092, 'false_negatives': 4038, 'precision': 0.4356589147286822, 'recall': 0.1727105101413645, 'f1': 0.24735915492957747}\n",
      "classification_result {'true_positives': 393, 'false_positives': 1542, 'false_negatives': 4488, 'precision': 0.20310077519379846, 'recall': 0.08051628764597418, 'f1': 0.11531690140845072}\n",
      "Epochs n. 5\n",
      "F1 train: [0.         0.51257862 0.12105263 0.98804271 0.         0.72871917\n",
      " 0.         0.         0.         0.         0.0620155  0.\n",
      " 0.         0.         0.         0.         0.12612613 0.\n",
      " 0.         0.         0.01652893 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9595282512313569\n",
      "identification {'true_positives': 1263, 'false_positives': 167, 'false_negatives': 879, 'precision': 0.8832167832167832, 'recall': 0.5896358543417367, 'f1': 0.7071668533034715}\n",
      "classification_result {'true_positives': 819, 'false_positives': 611, 'false_negatives': 1323, 'precision': 0.5727272727272728, 'recall': 0.38235294117647056, 'f1': 0.4585666293393057}\n",
      "Early stopping at epoch :  5\n",
      "F1 eval : [0.         0.30775194 0.03133903 0.97723285 0.00687623 0.\n",
      " 0.         0.         0.         0.04590164 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9394300496737596\n"
     ]
    }
   ],
   "source": [
    "model = Arg_Classifier(\"ES\",cfg).cuda()\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "currentDateAndTime = datetime.now()\n",
    "_id = str(currentDateAndTime.year)+\"_\"+str(currentDateAndTime.month)+\"_\"+str(currentDateAndTime.day)+\"_\"+str(currentDateAndTime.hour)+\"_\"+str(currentDateAndTime.minute)+\"_\"+str(currentDateAndTime.second)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#optimizer = torch.optim.Adam(model.parameters(),lr = 0.000005)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "\n",
    "logSotfMax = torch.nn.LogSoftmax(dim=1)\n",
    "nll_loss = torch.nn.NLLLoss()\n",
    "\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "dataloader_dev = DataLoader(dev_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "mapping = dataloader_train.dataset.args_roles\n",
    "\n",
    "auto_model.eval()\n",
    "auto_model.cuda()\n",
    "\n",
    "EPOCHS = 200\n",
    "patience_counter = 0\n",
    "patience = 5\n",
    "max_val_loss = 9999\n",
    "f1_score_max = 0\n",
    "output_path = \"saved\"\n",
    "model_name = \"model_\"+_id+\".pth\"\n",
    "PATH = os.path.join(output_path,model_name)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    #TRAINING\n",
    "    p = []\n",
    "    g = []\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_train):\n",
    "        #print(sample_batched)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "       \n",
    "        #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "        #------------------FILTERING SUB-WORDS----------------------\n",
    "        subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "        word_emebedding = []\n",
    "        for i in range(n):\n",
    "            subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "            flag = subtoken_mask[0,i,0]\n",
    "            if flag :\n",
    "                continue\n",
    "            else :\n",
    "                word_emebedding.append(subwords_embedding)\n",
    "        word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "        #-------------------------FORWARD/BACKWARD----------------------------------\n",
    "        x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "            perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "            predicate_index = sample_batched[\"predicate_index\"],\n",
    "            pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "            predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])        \n",
    "        b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "        loss = nll_loss(logSotfMax(x),gt)\n",
    "        total_loss = total_loss + loss\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "        counter += 1 \n",
    "            \n",
    "\n",
    "        #-------------------------RESULT STORING----------------------------------\n",
    "        predicted = torch.argmax(x, dim=1)\n",
    "        p += predicted.tolist()\n",
    "        g += gt.tolist()\n",
    "    \n",
    "\n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    scheduler.step()\n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    print(\"Epochs n.\", epoch)\n",
    "    print(\"F1 train:\",f1)\n",
    "    print(\"F1 avg train:\",f1_avg)\n",
    "    \n",
    "    avg_train_loss = total_loss/counter\n",
    "    writer.add_scalar(\"Loss_ES/train\", avg_train_loss, epoch)\n",
    "\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"Train_ES/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"Train_ES/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "    #EVALUATION\n",
    "    p = []\n",
    "    g = []\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_dev):\n",
    "    \n",
    "      #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "            #------------------FILTERING SUB-WORDS----------------------\n",
    "            subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "            word_emebedding = []\n",
    "            for i in range(n): \n",
    "                subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "                flag = subtoken_mask[0,i,0]\n",
    "                if flag :\n",
    "                    continue\n",
    "                else :\n",
    "                    word_emebedding.append(subwords_embedding)\n",
    "            word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "            #-------------------------FORWARD----------------------------------\n",
    "            x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "                        perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "                        predicate_index = sample_batched[\"predicate_index\"],\n",
    "                        pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "                        predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])   \n",
    "\n",
    "\n",
    "            b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "            loss = nll_loss(logSotfMax(x),gt)\n",
    "            total_loss = total_loss + loss\n",
    "            #-------------------------RESULT STORING----------------------------------\n",
    "            predicted = torch.argmax(x, dim=1)\n",
    "            p += predicted.tolist()\n",
    "            g += gt.tolist()\n",
    "            counter += 1 \n",
    "    \n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    avg_eval_loss = total_loss/counter\n",
    "\n",
    "    if avg_eval_loss < max_val_loss:\n",
    "        max_val_loss = avg_eval_loss\n",
    "    else :\n",
    "        patience_counter += 1\n",
    "    \n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    if patience_counter >= patience :  \n",
    "\n",
    "\n",
    "        print(\"Early stopping at epoch : \",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 avg eval :\",f1_avg)\n",
    "        break\n",
    "    else :\n",
    "        print(\"EPOCHS :\",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 avg eval :\",f1_avg)\n",
    "    \n",
    "\n",
    "    writer.add_scalar(\"Loss_ES/validation\", avg_eval_loss, epoch)\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"Eval_ES/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"Eval_ES/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if f1_avg > f1_score_max:\n",
    "        f1_score_max = f1_avg\n",
    "        print(\"SAVED :\",PATH)\n",
    "        torch.save(model.state_dict(),PATH)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### French"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New French Dataset\n",
    "bert-base-multilingual-cased\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 12\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Dict\n\u001b[0;32m---> 12\u001b[0m auto_model \u001b[39m=\u001b[39m AutoModel\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mbert-base-multilingual-cased\u001b[39m\u001b[39m\"\u001b[39m,output_hidden_states\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mmodel class is      : \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(auto_model)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mbert-base-multilingual-cased\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoModel' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import random\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "\n",
    "auto_model = AutoModel.from_pretrained(\"bert-base-multilingual-cased\",output_hidden_states=True)\n",
    "print(f\"\\nmodel class is      : {type(auto_model)}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "print(f\"\\nmodel class is      : {type(tokenizer)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SRL(Dataset):\n",
    " \n",
    "    def __init__(self,language,path,args_roles = None) -> None:\n",
    "\n",
    "        self.path_root = 'data'\n",
    "        self.load_data(language,path)\n",
    "        if args_roles is None :\n",
    "            self.args_roles,self.list_broken_id = self.list_arg_roles()\n",
    "        else : \n",
    "            self.args_roles = args_roles\n",
    "            _,self.list_broken_id = self.list_arg_roles()\n",
    "\n",
    "        self.pos_list,_ = self.list_pos()\n",
    "        print(self.args_roles)\n",
    "        self.predicate_dis,_ = self.list_predicate_roles()\n",
    "        self.pos_list.append(\"Nothing\")\n",
    "        self.predicate_dis.append(\"Nothing\")\n",
    "\n",
    "    def load_data(self,language,mode):\n",
    "        \n",
    "        mode = mode+\".json\"\n",
    "        path = os.path.join(self.path_root,language,mode)\n",
    "        data_file = open(path)\n",
    "       \n",
    "        data_ = json.load(data_file)\n",
    "\n",
    "        list_data = []\n",
    "\n",
    "        for data in data_:\n",
    "            list_data.append(data_[data])\n",
    "        \n",
    "\n",
    "        self.data = list_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, id : int):\n",
    "\n",
    "        flag = False\n",
    "        if id in self.list_broken_id :\n",
    "            flag = True\n",
    "            while flag == True:\n",
    "\n",
    "                rand_id = random.randint(0, len(self.data)-1)\n",
    "                \n",
    "                if rand_id in self.list_broken_id :\n",
    "                    pass\n",
    "                else :\n",
    "                    flag = False\n",
    "                    id = rand_id        \n",
    "\n",
    "\n",
    "        data = self.pre_processing(self.data[id])\n",
    "        data = self.processig(data)\n",
    "        return data\n",
    "        \n",
    "    def pre_processing(self, data:dict):\n",
    "        data_list = []\n",
    "        for role in data[\"roles\"]:\n",
    "            dictionary = dict()\n",
    "            dictionary[\"words\"] = data[\"words\"]\n",
    "            dictionary[\"role\"] = data[\"roles\"][role]\n",
    "            dictionary[\"pre_idx\"] = role\n",
    "            dictionary[\"pos_tags\"] = data[\"pos_tags\"]\n",
    "            dictionary[\"predicate_meaning\"] = data[\"predicates\"]\n",
    "            data_list.append(dictionary)    \n",
    "        return data_list\n",
    "    \n",
    "    def processig(self,data_list:list):\n",
    "        \n",
    "        for dictionary in data_list:\n",
    "\n",
    "            #dictionary[\"words\"] = data[\"words\"]\n",
    "            dictionary[\"gt_arg_identification\"] = self.arg_id(dictionary[\"role\"])\n",
    "            dictionary[\"gt_arg_classification\"] = self.arg_class(dictionary[\"role\"])\n",
    "            dictionary[\"pos_idx\"] = self.pos_idx(dictionary[\"pos_tags\"])\n",
    "            dictionary[\"predicate_meaning_idx\"] = self.predicate_meaning_idx(dictionary[\"predicate_meaning\"])\n",
    "        \n",
    "        return data_list\n",
    "   \n",
    "    def list_arg_roles(self):\n",
    "        list_roles = []\n",
    "        list_broken_id = []\n",
    "        for i,element in enumerate(self.data):\n",
    "            flag = True\n",
    "            try : roles = element[\"roles\"]\n",
    "            except : flag = False\n",
    "            if flag :\n",
    "                for e in roles:\n",
    "                    sentence = element[\"roles\"][e]\n",
    "\n",
    "                    for word in sentence:\n",
    "                        \n",
    "                        list_roles.append(word)\n",
    "                list_roles = list(set(list_roles))\n",
    "            else : \n",
    "                list_broken_id.append(i)\n",
    "        return list_roles,list_broken_id\n",
    "\n",
    "    def list_predicate_roles(self):\n",
    "        list_predicate_roles = []\n",
    "        list_broken_id = []\n",
    "        for i,element in enumerate(self.data):\n",
    "            flag = True\n",
    "            try : predicates = element[\"predicates\"]\n",
    "            except : flag = False\n",
    "            if flag :\n",
    "                for pre in predicates:\n",
    "                    list_predicate_roles.append(pre)\n",
    "                list_predicate_roles = list(set(list_predicate_roles))\n",
    "            else : \n",
    "                list_broken_id.append(i)\n",
    "        return list_predicate_roles,list_broken_id\n",
    "\n",
    "    def list_pos(self):\n",
    "        list_pos = []\n",
    "        list_broken_id = []\n",
    "        for i,element in enumerate(self.data):\n",
    "            flag = True\n",
    "            try : pos = element[\"pos_tags\"]\n",
    "            except : flag = False\n",
    "            if flag :\n",
    "                for e in pos:\n",
    "                    list_pos.append(e)\n",
    "                list_pos = list(set(list_pos))\n",
    "            else : \n",
    "                list_broken_id.append(i)\n",
    "        return list_pos,list_broken_id\n",
    "  \n",
    "    def arg_class(self,role:list):\n",
    "        list_idxs = []\n",
    "        for element in role:\n",
    "            list_idxs.append(self.args_roles.index(element))\n",
    "        \n",
    "\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "\n",
    "    def arg_id(self,role:dict):\n",
    "        list_idxs = []\n",
    "        for element in role:\n",
    "            if element == \"_\":\n",
    "                list_idxs.append(0)\n",
    "            else :\n",
    "                list_idxs.append(1)\n",
    "\n",
    "        \n",
    "\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "\n",
    "    def pos_idx(self,pos_tags:dict):\n",
    "        list_idxs = []\n",
    "        list_idxs.append(self.pos_list.index(\"Nothing\"))\n",
    "\n",
    "        for element in pos_tags:\n",
    "            list_idxs.append(self.pos_list.index(element))\n",
    "        \n",
    "        list_idxs.append(self.pos_list.index(\"Nothing\"))\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "    \n",
    "    def predicate_meaning_idx(self,predicate_meaning_tags:dict):\n",
    "        list_idxs = []\n",
    "        list_idxs.append(self.predicate_dis.index(\"Nothing\"))\n",
    "\n",
    "        for element in predicate_meaning_tags:\n",
    "            list_idxs.append(self.predicate_dis.index(element))\n",
    "        \n",
    "        list_idxs.append(self.predicate_dis.index(\"Nothing\"))\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64) \n",
    "\n",
    "    \n",
    "# here we define our collate function\n",
    "def collate_fn(batch) -> Dict[str, torch.Tensor]:\n",
    "    #print(batch)\n",
    "    input = dict() \n",
    "    batch_sentence = [] \n",
    "    #print(len(batch))\n",
    "    for period in batch:\n",
    "        for sentence in period :\n",
    "        \n",
    "            #print(len(sentence[0][\"words\"]))\n",
    "            pre_idx = int(sentence[\"pre_idx\"])\n",
    "            \n",
    "\n",
    "            predicate = sentence[\"words\"][pre_idx]\n",
    "\n",
    "            text = \" \".join(sentence[\"words\"])\n",
    "            tokens: list[str] = text.split()\n",
    "            predicate: list[str] = predicate.split()\n",
    "\n",
    "            #text = sentence[0][\"words\"]\n",
    "            \n",
    "            t = (tokens,predicate)\n",
    "\n",
    "            batch_sentence.append(t)\n",
    "            #print(batch_sentence)\n",
    "\n",
    "    batch_output = tokenizer.batch_encode_plus(batch_sentence,padding=True,is_split_into_words=True, truncation=True,return_offsets_mapping=True, return_tensors=\"pt\")\n",
    "    #print(batch_output.keys())\n",
    "\n",
    "\n",
    "    gt = dict()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    for period in batch:\n",
    "\n",
    "        list_positional_predicate_encoding = []\n",
    "        list_arg_gt = []\n",
    "        list_predicate_index = [] \n",
    "        list_pos_index = [] \n",
    "        list_predicate_meaning_index = []\n",
    "\n",
    "        for sentence in period:\n",
    "            #positional_encoding\n",
    "            #+2 per il CLS iniziale ad SEP finale\n",
    "            sentence_words_lenght =  len(sentence[\"words\"])\n",
    "            positional_predicate_encoding = torch.zeros(1,sentence_words_lenght+2)\n",
    "            #+1 per il CLS iniziale\n",
    "            pre_idx = int(sentence[\"pre_idx\"])\n",
    "            positional_predicate_encoding[:,pre_idx+1] = 1\n",
    "            list_positional_predicate_encoding.append(positional_predicate_encoding)\n",
    "            #print(\"positional_prefix_encoding\",positional_predicate_encoding)\n",
    "            list_predicate_index.append(pre_idx)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            pos = torch.unsqueeze(sentence[\"pos_idx\"],dim = 0)\n",
    "            list_pos_index.append(pos)\n",
    "            predicate_meaning_idxs = torch.unsqueeze(sentence[\"predicate_meaning_idx\"],dim = 0)\n",
    "            list_predicate_meaning_index.append(predicate_meaning_idxs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #note CLS and SEP are discharder after Bi-LSTM, the Classifier takes in input only wokrds hidden state embedding\n",
    "            arg_gt = torch.unsqueeze(sentence[\"gt_arg_classification\"],dim = 0)\n",
    "            list_arg_gt.append(arg_gt)\n",
    "        \n",
    "\n",
    "    list_arg_gt = torch.cat(list_arg_gt,dim = 0)\n",
    "    list_pos_index = torch.cat(list_pos_index,dim = 0)\n",
    "    list_predicate_meaning_index = torch.cat(list_predicate_meaning_index,dim = 0)\n",
    "    list_positional_predicate_encoding = torch.cat(list_positional_predicate_encoding,dim = 0)\n",
    "    gt[\"arg_gt\"] = list_arg_gt\n",
    "    input[\"predicate_index\"] = list_predicate_index\n",
    "    input[\"pos_index\"] = list_pos_index.long()\n",
    "    input[\"predicate_meaning_idx\"] = list_predicate_meaning_index.long()\n",
    "    offset = batch_output.pop(\"offset_mapping\")\n",
    "    input[\"BERT_input\"] = batch_output\n",
    "    input[\"positional_encoding\"] = list_positional_predicate_encoding.long()\n",
    "    input[\"offset_mapping\"] = offset\n",
    "    input[\"gt\"] = gt\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    return\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SRL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#train_dataset = SRL(\"EN\",\"train\")\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[39m#note here we are directly loading args_roles mapping as computed before the the dasetet where we have perfomerd \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m#EN and ES dataset should have the same consistency in generating \u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m train_dataset \u001b[39m=\u001b[39m SRL(\u001b[39m\"\u001b[39m\u001b[39mFR\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,train_dataset\u001b[39m.\u001b[39margs_roles,train_dataset\u001b[39m.\u001b[39mpos_list)\n\u001b[1;32m      6\u001b[0m \u001b[39m#same mapping should be used in both the dataset\u001b[39;00m\n\u001b[1;32m      7\u001b[0m dev_dataset \u001b[39m=\u001b[39m SRL(\u001b[39m\"\u001b[39m\u001b[39mFR\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mdev\u001b[39m\u001b[39m\"\u001b[39m,train_dataset\u001b[39m.\u001b[39margs_roles,train_dataset\u001b[39m.\u001b[39mpos_list)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SRL' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#train_dataset = SRL(\"EN\",\"train\")\n",
    "\n",
    "#note here we are directly loading args_roles mapping as computed before the the dasetet where we have perfomerd \n",
    "#EN and ES dataset should have the same consistency in generating \n",
    "train_dataset = SRL(\"FR\",\"train\",train_dataset.args_roles,train_dataset.pos_list)\n",
    "#same mapping should be used in both the dataset\n",
    "dev_dataset = SRL(\"FR\",\"dev\",train_dataset.args_roles,train_dataset.pos_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English-French attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Arg_Classifier(\n",
       "  (bi_lstm_portable): LSTM(132, 50, num_layers=2, batch_first=True, dropout=0.4, bidirectional=True)\n",
       "  (embedding_predicate_flag): Embedding(2, 32, max_norm=True)\n",
       "  (embedding_predicate): Embedding(304, False, max_norm=True)\n",
       "  (embedding_pos): Embedding(18, 100, max_norm=True)\n",
       "  (bi_lstm): LSTM(900, 50, num_layers=2, batch_first=True, dropout=0.4, bidirectional=True)\n",
       "  (dropout_language_constraint): Dropout(p=0.6, inplace=False)\n",
       "  (dropout_in_classifier): Dropout(p=0.4, inplace=False)\n",
       "  (Relu): ReLU()\n",
       "  (Sigmoid): Sigmoid()\n",
       "  (linear0): Linear(in_features=300, out_features=675, bias=True)\n",
       "  (linear1): Linear(in_features=675, out_features=135, bias=True)\n",
       "  (linear2): Linear(in_features=135, out_features=27, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#note that parameter EN is only used for tracking on which dataset was/is trained, and activate loading of the pretrained head\n",
    "#load the fine-tuned model over english\n",
    "PATH = \"saved/model_2022_12_18_21_33_45.pth\"\n",
    "model = Arg_Classifier(\"ES\",cfg)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.train().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs n. 0\n",
      "F1 train: [0.         0.41519926 0.00668896 0.98159286 0.         0.38427948\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9458893102533754\n",
      "identification {'true_positives': 657, 'false_positives': 127, 'false_negatives': 1495, 'precision': 0.8380102040816326, 'recall': 0.30529739776951675, 'f1': 0.4475476839237057}\n",
      "classification_result {'true_positives': 401, 'false_positives': 383, 'false_negatives': 1751, 'precision': 0.5114795918367347, 'recall': 0.18633828996282528, 'f1': 0.2731607629427793}\n",
      "EPOCHS : 0\n",
      "F1 eval : [0.         0.50540098 0.         0.98476308 0.25635359 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.951819105330271\n",
      "identification {'true_positives': 1784, 'false_positives': 270, 'false_negatives': 3098, 'precision': 0.8685491723466408, 'recall': 0.3654240065546907, 'f1': 0.5144175317185699}\n",
      "classification_result {'true_positives': 1004, 'false_positives': 1050, 'false_negatives': 3878, 'precision': 0.48880233690360275, 'recall': 0.20565342072920934, 'f1': 0.2895040369088812}\n",
      "SAVED : saved/model_2022_12_18_21_55_17.pth\n",
      "Epochs n. 1\n",
      "F1 train: [0.         0.4831553  0.07228916 0.98556403 0.         0.58646617\n",
      " 0.         0.         0.         0.         0.01639344 0.\n",
      " 0.         0.         0.         0.         0.14814815 0.\n",
      " 0.         0.         0.11940299 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9544334404786856\n",
      "identification {'true_positives': 1012, 'false_positives': 127, 'false_negatives': 1133, 'precision': 0.8884986830553117, 'recall': 0.4717948717948718, 'f1': 0.6163215590742995}\n",
      "classification_result {'true_positives': 635, 'false_positives': 504, 'false_negatives': 1510, 'precision': 0.5575065847234416, 'recall': 0.29603729603729606, 'f1': 0.38672350791717425}\n",
      "EPOCHS : 1\n",
      "F1 eval : [0.         0.51134548 0.         0.98473388 0.23535511 0.\n",
      " 0.         0.         0.         0.18390805 0.         0.\n",
      " 0.         0.         0.         0.         0.03448276 0.\n",
      " 0.         0.         0.01286174 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9524853334057606\n",
      "identification {'true_positives': 1901, 'false_positives': 415, 'false_negatives': 3022, 'precision': 0.8208117443868739, 'recall': 0.38614665854153973, 'f1': 0.5252106644564167}\n",
      "classification_result {'true_positives': 1065, 'false_positives': 1251, 'false_negatives': 3858, 'precision': 0.45984455958549225, 'recall': 0.21633150517976843, 'f1': 0.29423953584749274}\n",
      "SAVED : saved/model_2022_12_18_21_55_17.pth\n",
      "Epochs n. 2\n",
      "F1 train: [0.         0.49960349 0.07902736 0.98612618 0.         0.6159292\n",
      " 0.         0.         0.         0.         0.20689655 0.\n",
      " 0.         0.         0.         0.         0.19354839 0.\n",
      " 0.         0.         0.25333333 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.956746441350736\n",
      "identification {'true_positives': 1113, 'false_positives': 182, 'false_negatives': 1033, 'precision': 0.8594594594594595, 'recall': 0.5186393289841565, 'f1': 0.6469049694856146}\n",
      "classification_result {'true_positives': 722, 'false_positives': 573, 'false_negatives': 1424, 'precision': 0.5575289575289575, 'recall': 0.3364398881640261, 'f1': 0.41964545190351643}\n",
      "EPOCHS : 2\n",
      "F1 eval : [0.         0.51954603 0.09116809 0.98472378 0.14238411 0.\n",
      " 0.         0.         0.         0.23268698 0.         0.\n",
      " 0.         0.         0.         0.         0.0361991  0.\n",
      " 0.         0.         0.16374269 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9521032155241269\n",
      "identification {'true_positives': 1926, 'false_positives': 421, 'false_negatives': 2944, 'precision': 0.8206220707285897, 'recall': 0.395482546201232, 'f1': 0.5337397810724678}\n",
      "classification_result {'true_positives': 1059, 'false_positives': 1288, 'false_negatives': 3811, 'precision': 0.4512143161482744, 'recall': 0.21745379876796714, 'f1': 0.293473742552307}\n",
      "Epochs n. 3\n",
      "F1 train: [0.         0.4892772  0.16853933 0.98702312 0.         0.66314864\n",
      " 0.         0.         0.         0.         0.27329193 0.\n",
      " 0.         0.         0.         0.         0.19512195 0.\n",
      " 0.         0.         0.28187919 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9589274068393385\n",
      "identification {'true_positives': 1178, 'false_positives': 168, 'false_negatives': 963, 'precision': 0.87518573551263, 'recall': 0.5502101821578702, 'f1': 0.6756524232864928}\n",
      "classification_result {'true_positives': 770, 'false_positives': 576, 'false_negatives': 1371, 'precision': 0.5720653789004457, 'recall': 0.3596450256889304, 'f1': 0.4416403785488959}\n",
      "EPOCHS : 3\n",
      "F1 eval : [0.         0.52654867 0.12273361 0.98440412 0.1816387  0.\n",
      " 0.         0.         0.         0.25464191 0.         0.\n",
      " 0.         0.         0.         0.         0.18587361 0.\n",
      " 0.         0.         0.33248082 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9535452873154773\n",
      "identification {'true_positives': 2046, 'false_positives': 625, 'false_negatives': 2839, 'precision': 0.7660052414825907, 'recall': 0.41883316274309107, 'f1': 0.5415563790365272}\n",
      "classification_result {'true_positives': 1199, 'false_positives': 1472, 'false_negatives': 3686, 'precision': 0.4488955447397978, 'recall': 0.24544524053224157, 'f1': 0.31736368448914776}\n",
      "SAVED : saved/model_2022_12_18_21_55_17.pth\n",
      "Epochs n. 4\n",
      "F1 train: [0.         0.49594814 0.22051282 0.98784859 0.         0.70686767\n",
      " 0.         0.         0.         0.         0.34285714 0.\n",
      " 0.         0.         0.         0.         0.17241379 0.\n",
      " 0.         0.         0.32051282 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9609269395326534\n",
      "identification {'true_positives': 1255, 'false_positives': 162, 'false_negatives': 899, 'precision': 0.8856739590684545, 'recall': 0.5826369545032498, 'f1': 0.7028843461215346}\n",
      "classification_result {'true_positives': 836, 'false_positives': 581, 'false_negatives': 1318, 'precision': 0.5899788285109386, 'recall': 0.3881151346332405, 'f1': 0.46821618594231307}\n",
      "EPOCHS : 4\n",
      "F1 eval : [0.         0.51295681 0.14188267 0.98365281 0.14747859 0.\n",
      " 0.         0.         0.         0.23561644 0.         0.\n",
      " 0.         0.         0.         0.         0.06278027 0.\n",
      " 0.         0.         0.38834951 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9521465915526715\n",
      "identification {'true_positives': 1914, 'false_positives': 649, 'false_negatives': 2978, 'precision': 0.7467811158798283, 'recall': 0.3912510220768602, 'f1': 0.5134808853118713}\n",
      "classification_result {'true_positives': 1109, 'false_positives': 1454, 'false_negatives': 3783, 'precision': 0.43269605930550137, 'recall': 0.22669664758789862, 'f1': 0.29751844399731725}\n",
      "Epochs n. 5\n",
      "F1 train: [0.         0.52560873 0.3531746  0.98907995 0.         0.73452769\n",
      " 0.         0.         0.         0.         0.3583815  0.\n",
      " 0.         0.         0.         0.         0.32061069 0.\n",
      " 0.         0.         0.38323353 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.964453422178617\n",
      "identification {'true_positives': 1364, 'false_positives': 157, 'false_negatives': 814, 'precision': 0.8967784352399737, 'recall': 0.6262626262626263, 'f1': 0.7374966207082995}\n",
      "classification_result {'true_positives': 937, 'false_positives': 584, 'false_negatives': 1241, 'precision': 0.6160420775805391, 'recall': 0.43021120293847565, 'f1': 0.5066234117329007}\n",
      "Early stopping at epoch :  5\n",
      "F1 eval : [0.         0.51528998 0.23322332 0.98259752 0.04374703 0.\n",
      " 0.         0.         0.         0.25668449 0.         0.\n",
      " 0.         0.         0.         0.         0.21201413 0.\n",
      " 0.         0.         0.45248869 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9508463436472356\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "currentDateAndTime = datetime.now()\n",
    "_id = str(currentDateAndTime.year)+\"_\"+str(currentDateAndTime.month)+\"_\"+str(currentDateAndTime.day)+\"_\"+str(currentDateAndTime.hour)+\"_\"+str(currentDateAndTime.minute)+\"_\"+str(currentDateAndTime.second)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#optimizer = torch.optim.Adam(model.parameters(),lr = 0.000005)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "\n",
    "logSotfMax = torch.nn.LogSoftmax(dim=1)\n",
    "nll_loss = torch.nn.NLLLoss()\n",
    "\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "dataloader_dev = DataLoader(dev_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "mapping = dataloader_train.dataset.args_roles\n",
    "\n",
    "auto_model.eval()\n",
    "auto_model.cuda()\n",
    "\n",
    "EPOCHS = 200\n",
    "patience_counter = 0\n",
    "patience = 5\n",
    "max_val_loss = 9999\n",
    "f1_score_max = 0\n",
    "output_path = \"saved\"\n",
    "model_name = \"model_\"+_id+\".pth\"\n",
    "PATH = os.path.join(output_path,model_name)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    #TRAINING\n",
    "    p = []\n",
    "    g = []\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_train):\n",
    "        #print(sample_batched)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "       \n",
    "        #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "        #------------------FILTERING SUB-WORDS----------------------\n",
    "        subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "        word_emebedding = []\n",
    "        for i in range(n):\n",
    "            subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "            flag = subtoken_mask[0,i,0]\n",
    "            if flag :\n",
    "                continue\n",
    "            else :\n",
    "                word_emebedding.append(subwords_embedding)\n",
    "        word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "        #-------------------------FORWARD/BACKWARD----------------------------------\n",
    "        x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "            perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "            predicate_index = sample_batched[\"predicate_index\"],\n",
    "            pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "            predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])        \n",
    "        b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "        loss = nll_loss(logSotfMax(x),gt)\n",
    "        total_loss = total_loss + loss\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "        counter += 1 \n",
    "            \n",
    "\n",
    "        #-------------------------RESULT STORING----------------------------------\n",
    "        predicted = torch.argmax(x, dim=1)\n",
    "        p += predicted.tolist()\n",
    "        g += gt.tolist()\n",
    "    \n",
    "\n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    scheduler.step()\n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    print(\"Epochs n.\", epoch)\n",
    "    print(\"F1 train:\",f1)\n",
    "    print(\"F1 avg train:\",f1_avg)\n",
    "    \n",
    "    avg_train_loss = total_loss/counter\n",
    "    writer.add_scalar(\"EN_Loss_ES/train\", avg_train_loss, epoch)\n",
    "\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"EN_Train_ES/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"EN_Train_ES/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "    #EVALUATION\n",
    "    p = []\n",
    "    g = []\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_dev):\n",
    "    \n",
    "      #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "            #------------------FILTERING SUB-WORDS----------------------\n",
    "            subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "            word_emebedding = []\n",
    "            for i in range(n): \n",
    "                subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "                flag = subtoken_mask[0,i,0]\n",
    "                if flag :\n",
    "                    continue\n",
    "                else :\n",
    "                    word_emebedding.append(subwords_embedding)\n",
    "            word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "            #-------------------------FORWARD----------------------------------\n",
    "            x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "                        perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "                        predicate_index = sample_batched[\"predicate_index\"],\n",
    "                        pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "                        predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])   \n",
    "\n",
    "\n",
    "            b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "            loss = nll_loss(logSotfMax(x),gt)\n",
    "            total_loss = total_loss + loss\n",
    "            #-------------------------RESULT STORING----------------------------------\n",
    "            predicted = torch.argmax(x, dim=1)\n",
    "            p += predicted.tolist()\n",
    "            g += gt.tolist()\n",
    "            counter += 1 \n",
    "    \n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    avg_eval_loss = total_loss/counter\n",
    "\n",
    "    if avg_eval_loss < max_val_loss:\n",
    "        max_val_loss = avg_eval_loss\n",
    "    else :\n",
    "        patience_counter += 1\n",
    "    \n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    if patience_counter >= patience :  \n",
    "\n",
    "\n",
    "        print(\"Early stopping at epoch : \",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 avg eval :\",f1_avg)\n",
    "        break\n",
    "    else :\n",
    "        print(\"EPOCHS :\",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 avg eval :\",f1_avg)\n",
    "    \n",
    "\n",
    "    writer.add_scalar(\"EN_Loss_ES/validation\", avg_eval_loss, epoch)\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"EN_Eval_ES/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"EN_Eval_ES/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if f1_avg > f1_score_max:\n",
    "        f1_score_max = f1_avg\n",
    "        print(\"SAVED :\",PATH)\n",
    "        torch.save(model.state_dict(),PATH)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare without Transfert Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs n. 0\n",
      "F1 train: [0.         0.19946809 0.         0.9768764  0.         0.0295421\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9336015019908596\n",
      "identification {'true_positives': 172, 'false_positives': 71, 'false_negatives': 1987, 'precision': 0.7078189300411523, 'recall': 0.07966651227420102, 'f1': 0.14321398834304747}\n",
      "classification_result {'true_positives': 85, 'false_positives': 158, 'false_negatives': 2074, 'precision': 0.3497942386831276, 'recall': 0.03937007874015748, 'f1': 0.070774354704413}\n",
      "EPOCHS : 0\n",
      "F1 eval : [0.         0.23416618 0.         0.9777718  0.00265252 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9389493973815146\n",
      "identification {'true_positives': 1030, 'false_positives': 1096, 'false_negatives': 3858, 'precision': 0.48447789275634995, 'recall': 0.2107201309328969, 'f1': 0.2936983176504135}\n",
      "classification_result {'true_positives': 405, 'false_positives': 1721, 'false_negatives': 4483, 'precision': 0.1904985888993415, 'recall': 0.08285597381342062, 'f1': 0.11548331907613345}\n",
      "SAVED : saved/model_2022_12_18_22_2_58.pth\n",
      "Epochs n. 1\n",
      "F1 train: [0.         0.3872549  0.         0.98172752 0.         0.42610365\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9464882649310143\n",
      "identification {'true_positives': 798, 'false_positives': 268, 'false_negatives': 1337, 'precision': 0.7485928705440901, 'recall': 0.3737704918032787, 'f1': 0.4985941893158388}\n",
      "classification_result {'true_positives': 459, 'false_positives': 607, 'false_negatives': 1676, 'precision': 0.43058161350844276, 'recall': 0.21498829039812647, 'f1': 0.2867853795688848}\n",
      "EPOCHS : 1\n",
      "F1 eval : [0.         0.23666396 0.         0.97658545 0.00478469 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9376621220958927\n",
      "identification {'true_positives': 1127, 'false_positives': 1421, 'false_negatives': 3759, 'precision': 0.4423076923076923, 'recall': 0.23065902578796563, 'f1': 0.3032015065913371}\n",
      "classification_result {'true_positives': 441, 'false_positives': 2107, 'false_negatives': 4445, 'precision': 0.17307692307692307, 'recall': 0.09025787965616046, 'f1': 0.11864406779661017}\n",
      "Epochs n. 2\n",
      "F1 train: [0.         0.44530046 0.         0.98438053 0.         0.60050463\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.952145225902215\n",
      "identification {'true_positives': 1022, 'false_positives': 243, 'false_negatives': 1124, 'precision': 0.807905138339921, 'recall': 0.4762348555452004, 'f1': 0.5992377601876282}\n",
      "classification_result {'true_positives': 646, 'false_positives': 619, 'false_negatives': 1500, 'precision': 0.5106719367588933, 'recall': 0.30102516309412863, 'f1': 0.37877455291703316}\n",
      "EPOCHS : 2\n",
      "F1 eval : [0.         0.26013195 0.         0.9780147  0.0071599  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9395047264553521\n",
      "identification {'true_positives': 1022, 'false_positives': 1031, 'false_negatives': 3848, 'precision': 0.4978080857282026, 'recall': 0.20985626283367556, 'f1': 0.2952477249747219}\n",
      "classification_result {'true_positives': 420, 'false_positives': 1633, 'false_negatives': 4450, 'precision': 0.20457866536775451, 'recall': 0.08624229979466119, 'f1': 0.12133468149646107}\n",
      "SAVED : saved/model_2022_12_18_22_2_58.pth\n",
      "Epochs n. 3\n",
      "F1 train: [0.         0.47011952 0.02555911 0.98541674 0.         0.63305785\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9541615111910255\n",
      "identification {'true_positives': 1065, 'false_positives': 200, 'false_negatives': 1089, 'precision': 0.841897233201581, 'recall': 0.49442896935933145, 'f1': 0.6229891781222578}\n",
      "classification_result {'true_positives': 682, 'false_positives': 583, 'false_negatives': 1472, 'precision': 0.5391304347826087, 'recall': 0.3166202414113278, 'f1': 0.39894706054401874}\n",
      "EPOCHS : 3\n",
      "F1 eval : [0.         0.28145455 0.02816901 0.97774314 0.00762112 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9396751709187108\n",
      "identification {'true_positives': 871, 'false_positives': 950, 'false_negatives': 4019, 'precision': 0.4783086216364635, 'recall': 0.17811860940695295, 'f1': 0.2595738340038742}\n",
      "classification_result {'true_positives': 404, 'false_positives': 1417, 'false_negatives': 4486, 'precision': 0.22185612300933552, 'recall': 0.08261758691206544, 'f1': 0.12039934436000595}\n",
      "SAVED : saved/model_2022_12_18_22_2_58.pth\n",
      "Epochs n. 4\n",
      "F1 train: [0.         0.51370399 0.05590062 0.98651971 0.         0.69414674\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.07843137 0.\n",
      " 0.         0.         0.03333333 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9570333876174468\n",
      "identification {'true_positives': 1139, 'false_positives': 185, 'false_negatives': 994, 'precision': 0.8602719033232629, 'recall': 0.5339896858884201, 'f1': 0.658952849291293}\n",
      "classification_result {'true_positives': 764, 'false_positives': 560, 'false_negatives': 1369, 'precision': 0.5770392749244713, 'recall': 0.35818096577590247, 'f1': 0.4420017356089094}\n",
      "EPOCHS : 4\n",
      "F1 eval : [0.         0.27561328 0.01457726 0.9769951  0.00616333 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9388430347981687\n",
      "identification {'true_positives': 843, 'false_positives': 1092, 'false_negatives': 4038, 'precision': 0.4356589147286822, 'recall': 0.1727105101413645, 'f1': 0.24735915492957747}\n",
      "classification_result {'true_positives': 393, 'false_positives': 1542, 'false_negatives': 4488, 'precision': 0.20310077519379846, 'recall': 0.08051628764597418, 'f1': 0.11531690140845072}\n",
      "Epochs n. 5\n",
      "F1 train: [0.         0.51257862 0.12105263 0.98804271 0.         0.72871917\n",
      " 0.         0.         0.         0.         0.0620155  0.\n",
      " 0.         0.         0.         0.         0.12612613 0.\n",
      " 0.         0.         0.01652893 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9595282512313569\n",
      "identification {'true_positives': 1263, 'false_positives': 167, 'false_negatives': 879, 'precision': 0.8832167832167832, 'recall': 0.5896358543417367, 'f1': 0.7071668533034715}\n",
      "classification_result {'true_positives': 819, 'false_positives': 611, 'false_negatives': 1323, 'precision': 0.5727272727272728, 'recall': 0.38235294117647056, 'f1': 0.4585666293393057}\n",
      "Early stopping at epoch :  5\n",
      "F1 eval : [0.         0.30775194 0.03133903 0.97723285 0.00687623 0.\n",
      " 0.         0.         0.         0.04590164 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9394300496737596\n"
     ]
    }
   ],
   "source": [
    "model = Arg_Classifier(\"FR\",cfg).cuda()\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "currentDateAndTime = datetime.now()\n",
    "_id = str(currentDateAndTime.year)+\"_\"+str(currentDateAndTime.month)+\"_\"+str(currentDateAndTime.day)+\"_\"+str(currentDateAndTime.hour)+\"_\"+str(currentDateAndTime.minute)+\"_\"+str(currentDateAndTime.second)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#optimizer = torch.optim.Adam(model.parameters(),lr = 0.000005)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "\n",
    "logSotfMax = torch.nn.LogSoftmax(dim=1)\n",
    "nll_loss = torch.nn.NLLLoss()\n",
    "\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "dataloader_dev = DataLoader(dev_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "mapping = dataloader_train.dataset.args_roles\n",
    "\n",
    "auto_model.eval()\n",
    "auto_model.cuda()\n",
    "\n",
    "EPOCHS = 200\n",
    "patience_counter = 0\n",
    "patience = 5\n",
    "max_val_loss = 9999\n",
    "f1_score_max = 0\n",
    "output_path = \"saved\"\n",
    "model_name = \"model_\"+_id+\".pth\"\n",
    "PATH = os.path.join(output_path,model_name)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    #TRAINING\n",
    "    p = []\n",
    "    g = []\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_train):\n",
    "        #print(sample_batched)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "       \n",
    "        #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "        #------------------FILTERING SUB-WORDS----------------------\n",
    "        subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "        word_emebedding = []\n",
    "        for i in range(n):\n",
    "            subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "            flag = subtoken_mask[0,i,0]\n",
    "            if flag :\n",
    "                continue\n",
    "            else :\n",
    "                word_emebedding.append(subwords_embedding)\n",
    "        word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "        #-------------------------FORWARD/BACKWARD----------------------------------\n",
    "        x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "            perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "            predicate_index = sample_batched[\"predicate_index\"],\n",
    "            pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "            predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])        \n",
    "        b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "        loss = nll_loss(logSotfMax(x),gt)\n",
    "        total_loss = total_loss + loss\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "        counter += 1 \n",
    "            \n",
    "\n",
    "        #-------------------------RESULT STORING----------------------------------\n",
    "        predicted = torch.argmax(x, dim=1)\n",
    "        p += predicted.tolist()\n",
    "        g += gt.tolist()\n",
    "    \n",
    "\n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    scheduler.step()\n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    print(\"Epochs n.\", epoch)\n",
    "    print(\"F1 train:\",f1)\n",
    "    print(\"F1 avg train:\",f1_avg)\n",
    "    \n",
    "    avg_train_loss = total_loss/counter\n",
    "    writer.add_scalar(\"Loss_ES/train\", avg_train_loss, epoch)\n",
    "\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"Train_ES/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"Train_ES/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "    #EVALUATION\n",
    "    p = []\n",
    "    g = []\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_dev):\n",
    "    \n",
    "      #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "            #------------------FILTERING SUB-WORDS----------------------\n",
    "            subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "            word_emebedding = []\n",
    "            for i in range(n): \n",
    "                subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "                flag = subtoken_mask[0,i,0]\n",
    "                if flag :\n",
    "                    continue\n",
    "                else :\n",
    "                    word_emebedding.append(subwords_embedding)\n",
    "            word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "            #-------------------------FORWARD----------------------------------\n",
    "            x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "                        perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "                        predicate_index = sample_batched[\"predicate_index\"],\n",
    "                        pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "                        predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])   \n",
    "\n",
    "\n",
    "            b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "            loss = nll_loss(logSotfMax(x),gt)\n",
    "            total_loss = total_loss + loss\n",
    "            #-------------------------RESULT STORING----------------------------------\n",
    "            predicted = torch.argmax(x, dim=1)\n",
    "            p += predicted.tolist()\n",
    "            g += gt.tolist()\n",
    "            counter += 1 \n",
    "    \n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    avg_eval_loss = total_loss/counter\n",
    "\n",
    "    if avg_eval_loss < max_val_loss:\n",
    "        max_val_loss = avg_eval_loss\n",
    "    else :\n",
    "        patience_counter += 1\n",
    "    \n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    if patience_counter >= patience :  \n",
    "\n",
    "\n",
    "        print(\"Early stopping at epoch : \",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 avg eval :\",f1_avg)\n",
    "        break\n",
    "    else :\n",
    "        print(\"EPOCHS :\",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 avg eval :\",f1_avg)\n",
    "    \n",
    "\n",
    "    writer.add_scalar(\"Loss_ES/validation\", avg_eval_loss, epoch)\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"Eval_ES/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"Eval_ES/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if f1_avg > f1_score_max:\n",
    "        f1_score_max = f1_avg\n",
    "        print(\"SAVED :\",PATH)\n",
    "        torch.save(model.state_dict(),PATH)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('nlp2022-hw3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ea0fd027ae2f380e05c4e3293ede0f85a80d4d466dd9309da3a748502c958571"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
