{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT and TOKENIZER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model class is      : <class 'transformers.models.bert.modeling_bert.BertModel'>\n",
      "\n",
      "model class is      : <class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel,AutoTokenizer\n",
    "\n",
    "auto_model = AutoModel.from_pretrained(\"bert-base-cased\",output_hidden_states=True)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nmodel class is      : {type(auto_model)}\")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "print(f\"\\nmodel class is      : {type(tokenizer)}\")\n",
    "\n",
    "\n",
    "torch.save(auto_model,\"hw2/stud/saved/bert.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Role Labelling Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import random\n",
    "from typing import Dict\n",
    "\n",
    "class SRL(Dataset):\n",
    " \n",
    "    def __init__(self,language,tokenizer,path,args_roles = None,pos_list = None,predicate_dis = None) -> None:\n",
    "        #train\n",
    "        #self.path_root = 'data'\n",
    "        #inference \n",
    "        self.path_root = 'hw2/stud/data'\n",
    "        #self.path_root = 'stud/data'\n",
    "        self.load_data(language,path)\n",
    "        if args_roles is None :\n",
    "            self.args_roles,self.list_broken_id = self.list_arg_roles()\n",
    "            self.args_roles.append(\"UNK\")\n",
    "        else : \n",
    "            self.args_roles = args_roles\n",
    "            _,self.list_broken_id = self.list_arg_roles()\n",
    "        \n",
    "\n",
    "        if pos_list is None :\n",
    "            self.pos_list,_ = self.list_pos()\n",
    "            self.pos_list.append(\"Nothing\")\n",
    "            self.pos_list.append(\"UNK\")\n",
    "        else : \n",
    "            self.pos_list = pos_list\n",
    "        \n",
    "\n",
    "\n",
    "        if predicate_dis is None :\n",
    "            self.predicate_dis,_ = self.list_predicate_roles()\n",
    "            self.predicate_dis.append(\"Nothing\")\n",
    "            self.predicate_dis.append(\"UNK\")\n",
    "        else : \n",
    "            self.predicate_dis = predicate_dis\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def load_data(self,language,mode):\n",
    "        \n",
    "        mode = mode+\".json\"\n",
    "        path = os.path.join(self.path_root,language,mode)\n",
    "        data_file = open(path)\n",
    "       \n",
    "        data_ = json.load(data_file)\n",
    "\n",
    "        list_data = []\n",
    "\n",
    "        for data in data_:\n",
    "            list_data.append(data_[data])\n",
    "        \n",
    "\n",
    "        self.data = list_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, id : int):\n",
    "\n",
    "        flag = False\n",
    "        if id in self.list_broken_id :\n",
    "            flag = True\n",
    "            while flag == True:\n",
    "\n",
    "                rand_id = random.randint(0, len(self.data)-1)\n",
    "                \n",
    "                if rand_id in self.list_broken_id :\n",
    "                    pass\n",
    "                else :\n",
    "                    flag = False\n",
    "                    id = rand_id        \n",
    "\n",
    "\n",
    "        data = self.pre_processing(self.data[id])\n",
    "        data = self.processig(data)\n",
    "        return data\n",
    "        \n",
    "    def pre_processing(self, data:dict):\n",
    "        data_list = []\n",
    "        for role in data[\"roles\"]:\n",
    "            dictionary = dict()\n",
    "            dictionary[\"words\"] = data[\"words\"]\n",
    "            dictionary[\"role\"] = data[\"roles\"][role]\n",
    "            dictionary[\"pre_idx\"] = role\n",
    "            dictionary[\"pos_tags\"] = data[\"pos_tags\"]\n",
    "            dictionary[\"predicate_meaning\"] = data[\"predicates\"]\n",
    "            data_list.append(dictionary)    \n",
    "        return data_list\n",
    "    \n",
    "    def processig(self,data_list:list):\n",
    "        \n",
    "        for dictionary in data_list:\n",
    "\n",
    "            #dictionary[\"words\"] = data[\"words\"]\n",
    "            dictionary[\"gt_arg_identification\"] = self.arg_id(dictionary[\"role\"])\n",
    "            dictionary[\"gt_arg_classification\"] = self.arg_class(dictionary[\"role\"])\n",
    "            dictionary[\"pos_idx\"] = self.pos_idx(dictionary[\"pos_tags\"])\n",
    "            dictionary[\"predicate_meaning_idx\"] = self.predicate_meaning_idx(dictionary[\"predicate_meaning\"])\n",
    "        \n",
    "        return data_list\n",
    "   \n",
    "    def list_arg_roles(self):\n",
    "        list_roles = []\n",
    "        list_broken_id = []\n",
    "        for i,element in enumerate(self.data):\n",
    "            flag = True\n",
    "            try : roles = element[\"roles\"]\n",
    "            except : flag = False\n",
    "            if flag :\n",
    "                for e in roles:\n",
    "                    sentence = element[\"roles\"][e]\n",
    "\n",
    "                    for word in sentence:\n",
    "                        \n",
    "                        list_roles.append(word)\n",
    "                list_roles = list(set(list_roles))\n",
    "            else : \n",
    "                list_broken_id.append(i)\n",
    "        return list_roles,list_broken_id\n",
    "\n",
    "    def list_predicate_roles(self):\n",
    "        list_predicate_roles = []\n",
    "        list_broken_id = []\n",
    "        for i,element in enumerate(self.data):\n",
    "            flag = True\n",
    "            try : predicates = element[\"predicates\"]\n",
    "            except : flag = False\n",
    "            if flag :\n",
    "                for pre in predicates:\n",
    "                    list_predicate_roles.append(pre)\n",
    "                list_predicate_roles = list(set(list_predicate_roles))\n",
    "            else : \n",
    "                list_broken_id.append(i)\n",
    "        return list_predicate_roles,list_broken_id\n",
    "\n",
    "    def list_pos(self):\n",
    "        list_pos = []\n",
    "        list_broken_id = []\n",
    "        for i,element in enumerate(self.data):\n",
    "            flag = True\n",
    "            try : pos = element[\"pos_tags\"]\n",
    "            except : flag = False\n",
    "            if flag :\n",
    "                for e in pos:\n",
    "                    list_pos.append(e)\n",
    "                list_pos = list(set(list_pos))\n",
    "            else : \n",
    "                list_broken_id.append(i)\n",
    "        return list_pos,list_broken_id\n",
    "  \n",
    "    def arg_class(self,role:list):\n",
    "        list_idxs = []\n",
    "        for element in role:\n",
    "            try : list_idxs.append(self.args_roles.index(element))\n",
    "            except : list_idxs.append(self.args_roles.index(\"UNK\"))\n",
    "        \n",
    "\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "\n",
    "    def arg_id(self,role:dict):\n",
    "        list_idxs = []\n",
    "        for element in role:\n",
    "            if element == \"_\":\n",
    "                list_idxs.append(0)\n",
    "            else :\n",
    "                list_idxs.append(1)\n",
    "\n",
    "        \n",
    "\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "\n",
    "    def pos_idx(self,pos_tags:dict):\n",
    "        list_idxs = []\n",
    "        list_idxs.append(self.pos_list.index(\"Nothing\"))\n",
    "\n",
    "        for element in pos_tags:\n",
    "            try :list_idxs.append(self.pos_list.index(element))\n",
    "            except :list_idxs.append(self.pos_list.index(\"UNK\"))\n",
    "        \n",
    "        list_idxs.append(self.pos_list.index(\"Nothing\"))\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "    \n",
    "    def predicate_meaning_idx(self,predicate_meaning_tags:dict):\n",
    "        list_idxs = []\n",
    "        list_idxs.append(self.predicate_dis.index(\"Nothing\"))\n",
    "\n",
    "        for element in predicate_meaning_tags:\n",
    "            try : list_idxs.append(self.predicate_dis.index(element))\n",
    "            except : list_idxs.append(self.predicate_dis.index(\"UNK\"))\n",
    "            \n",
    "        \n",
    "        list_idxs.append(self.predicate_dis.index(\"Nothing\"))\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "   \n",
    "    def role_gen(self,sentence):\n",
    "\n",
    "        base = [\"_\"]*len(sentence[\"predicates\"])\n",
    "        roles_dict = dict()\n",
    "        counter = 0\n",
    "        for i,item in enumerate(sentence[\"predicates\"]):\n",
    "\n",
    "            if item != \"_\":\n",
    "                base = [\"_\"]*len(sentence[\"predicates\"])\n",
    "                sentence[\"roles\"] = 10\n",
    "                roles_dict[str(i)] = base\n",
    "                counter += 1\n",
    "        \n",
    "        if counter == 0:\n",
    "            sentence[\"roles\"] = { }\n",
    "            flag = False\n",
    "            \n",
    "                \n",
    "        else :\n",
    "            sentence[\"roles\"] = roles_dict\n",
    "            flag = True\n",
    "\n",
    "        return sentence,flag\n",
    "        \n",
    "    def prepare_batch(self,sentence):\n",
    "\n",
    "        sentence,flag = self.role_gen(sentence)\n",
    "        \n",
    "        if flag :\n",
    "\n",
    "            data = self.pre_processing(sentence)\n",
    "            data = self.processig(data)\n",
    "            data = [data]\n",
    "            \n",
    "            \n",
    "            input = dict() \n",
    "            gt = dict()\n",
    "            batch_sentence = [] \n",
    "            \n",
    "            for period in data:\n",
    "                for sentence in period :\n",
    "\n",
    "                    \n",
    "                \n",
    "                    #print(len(sentence[0][\"words\"]))\n",
    "                    pre_idx = int(sentence[\"pre_idx\"])\n",
    "                    \n",
    "\n",
    "                    predicate = sentence[\"words\"][pre_idx]\n",
    "\n",
    "                    text = \" \".join(sentence[\"words\"])\n",
    "                    tokens: list[str] = text.split()\n",
    "                    predicate: list[str] = predicate.split()\n",
    "\n",
    "                    #text = sentence[0][\"words\"]\n",
    "                    \n",
    "                    t = (tokens,predicate)\n",
    "\n",
    "                    batch_sentence.append(t)\n",
    "                \n",
    "                \n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "            batch_output = self.tokenizer.batch_encode_plus(batch_sentence,padding=True,is_split_into_words=True, truncation=True,return_offsets_mapping=True, return_tensors=\"pt\")\n",
    "            \n",
    "\n",
    "\n",
    "            for period in data:\n",
    "\n",
    "                list_positional_predicate_encoding = []\n",
    "                list_predicate_index = [] \n",
    "                list_pos_index = [] \n",
    "                list_arg_gt = []\n",
    "                list_predicate_meaning_index = []\n",
    "                list_meaning_predicate_encoding = []\n",
    "\n",
    "                for sentence in period:\n",
    "                    #positional_encoding\n",
    "                    #+2 per il CLS iniziale ad SEP finale\n",
    "                    sentence_words_lenght =  len(sentence[\"words\"])\n",
    "                    positional_predicate_encoding = torch.zeros(1,sentence_words_lenght+2)\n",
    "                    #+1 per il CLS iniziale\n",
    "                    pre_idx = int(sentence[\"pre_idx\"])\n",
    "                    positional_predicate_encoding[:,pre_idx+1] = 1\n",
    "                    list_positional_predicate_encoding.append(positional_predicate_encoding)\n",
    "                    #print(\"positional_prefix_encoding\",positional_predicate_encoding)\n",
    "                    list_predicate_index.append(pre_idx)\n",
    "\n",
    "                    meaning_predicate_encoding = torch.zeros(1,sentence_words_lenght+2)\n",
    "                    pre_idx = int(sentence[\"pre_idx\"])\n",
    "                    #rather then set the flag 0,1 set with class verb\n",
    "                    meaning_predicate_encoding[:,pre_idx+1] = sentence[\"predicate_meaning_idx\"][pre_idx+1]\n",
    "                    list_meaning_predicate_encoding.append(meaning_predicate_encoding)\n",
    "                    \n",
    "\n",
    "                    pos = torch.unsqueeze(sentence[\"pos_idx\"],dim = 0)\n",
    "                    list_pos_index.append(pos)\n",
    "                    predicate_meaning_idxs = torch.unsqueeze(sentence[\"predicate_meaning_idx\"],dim = 0)\n",
    "                    list_predicate_meaning_index.append(predicate_meaning_idxs)\n",
    "\n",
    "\n",
    "                    arg_gt = torch.unsqueeze(sentence[\"gt_arg_classification\"],dim = 0)\n",
    "                    list_arg_gt.append(arg_gt)\n",
    "\n",
    "\n",
    "            list_arg_gt = torch.cat(list_arg_gt,dim = 0)\n",
    "            list_pos_index = torch.cat(list_pos_index,dim = 0)\n",
    "            list_predicate_meaning_index = torch.cat(list_predicate_meaning_index,dim = 0)\n",
    "            list_positional_predicate_encoding = torch.cat(list_positional_predicate_encoding,dim = 0)\n",
    "            list_predicate_meaning_index_bis = torch.cat(list_meaning_predicate_encoding,dim = 0)\n",
    "            gt[\"arg_gt\"] = list_arg_gt\n",
    "            input[\"predicate_index\"] = list_predicate_index\n",
    "            input[\"pos_index\"] = list_pos_index.long()\n",
    "            input[\"predicate_meaning_idx\"] = list_predicate_meaning_index.long()\n",
    "            input[\"predicate_meaning_idx_bis\"] = list_predicate_meaning_index_bis.long()\n",
    "            offset = batch_output.pop(\"offset_mapping\")\n",
    "            input[\"BERT_input\"] = batch_output\n",
    "            input[\"positional_encoding\"] = list_positional_predicate_encoding.long()\n",
    "            input[\"offset_mapping\"] = offset\n",
    "            input[\"gt\"] = gt\n",
    "        \n",
    "        else :\n",
    "            input = sentence\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return input,flag\n",
    "    \n",
    "# here we define our collate function\n",
    "def collate_fn(batch) -> Dict[str, torch.Tensor]:\n",
    "    #print(batch)\n",
    "    input = dict() \n",
    "    batch_sentence = [] \n",
    "    #print(len(batch))\n",
    "    for period in batch:\n",
    "        for sentence in period :\n",
    "        \n",
    "            #print(len(sentence[0][\"words\"]))\n",
    "            pre_idx = int(sentence[\"pre_idx\"])\n",
    "            \n",
    "\n",
    "            predicate = sentence[\"words\"][pre_idx]\n",
    "\n",
    "            text = \" \".join(sentence[\"words\"])\n",
    "            tokens: list[str] = text.split()\n",
    "            predicate: list[str] = predicate.split()\n",
    "\n",
    "            #text = sentence[0][\"words\"]\n",
    "            \n",
    "            t = (tokens,predicate)\n",
    "\n",
    "            batch_sentence.append(t)\n",
    "            #print(batch_sentence)\n",
    "\n",
    "    batch_output = tokenizer.batch_encode_plus(batch_sentence,padding=True,is_split_into_words=True, truncation=True,return_offsets_mapping=True, return_tensors=\"pt\")\n",
    "    #print(batch_output.keys())\n",
    "\n",
    "\n",
    "    gt = dict()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    for period in batch:\n",
    "\n",
    "        list_positional_predicate_encoding = []\n",
    "        list_arg_gt = []\n",
    "        list_predicate_index = [] \n",
    "        list_pos_index = [] \n",
    "        list_predicate_meaning_index = []\n",
    "        list_meaning_predicate_encoding = []\n",
    "\n",
    "        for sentence in period:\n",
    "            #positional_encoding\n",
    "            #+2 per il CLS iniziale ad SEP finale\n",
    "            sentence_words_lenght =  len(sentence[\"words\"])\n",
    "            positional_predicate_encoding = torch.zeros(1,sentence_words_lenght+2)\n",
    "            #+1 per il CLS iniziale\n",
    "            pre_idx = int(sentence[\"pre_idx\"])\n",
    "            positional_predicate_encoding[:,pre_idx+1] = 1\n",
    "            list_positional_predicate_encoding.append(positional_predicate_encoding)\n",
    "            #print(\"positional_prefix_encoding\",positional_predicate_encoding)\n",
    "            list_predicate_index.append(pre_idx)\n",
    "\n",
    "\n",
    "            meaning_predicate_encoding = torch.zeros(1,sentence_words_lenght+2)\n",
    "            pre_idx = int(sentence[\"pre_idx\"])\n",
    "            #rather then set the flag 0,1 set with class verb\n",
    "            meaning_predicate_encoding[:,pre_idx+1] = sentence[\"predicate_meaning_idx\"][pre_idx+1]\n",
    "            list_meaning_predicate_encoding.append(meaning_predicate_encoding)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            pos = torch.unsqueeze(sentence[\"pos_idx\"],dim = 0)\n",
    "            list_pos_index.append(pos)\n",
    "            predicate_meaning_idxs = torch.unsqueeze(sentence[\"predicate_meaning_idx\"],dim = 0)\n",
    "            list_predicate_meaning_index.append(predicate_meaning_idxs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #note CLS and SEP are discharder after Bi-LSTM, the Classifier takes in input only wokrds hidden state embedding\n",
    "            arg_gt = torch.unsqueeze(sentence[\"gt_arg_classification\"],dim = 0)\n",
    "            list_arg_gt.append(arg_gt)\n",
    "        \n",
    "\n",
    "    list_arg_gt = torch.cat(list_arg_gt,dim = 0)\n",
    "    list_pos_index = torch.cat(list_pos_index,dim = 0)\n",
    "    list_predicate_meaning_index = torch.cat(list_predicate_meaning_index,dim = 0)\n",
    "    list_predicate_meaning_index_bis = torch.cat(list_meaning_predicate_encoding,dim = 0)\n",
    "    list_positional_predicate_encoding = torch.cat(list_positional_predicate_encoding,dim = 0)\n",
    "    gt[\"arg_gt\"] = list_arg_gt\n",
    "    input[\"predicate_index\"] = list_predicate_index\n",
    "    input[\"pos_index\"] = list_pos_index.long()\n",
    "    input[\"predicate_meaning_idx\"] = list_predicate_meaning_index.long()\n",
    "    input[\"predicate_meaning_idx_bis\"] = list_predicate_meaning_index_bis.long()\n",
    "    offset = batch_output.pop(\"offset_mapping\")\n",
    "    input[\"BERT_input\"] = batch_output\n",
    "    input[\"positional_encoding\"] = list_positional_predicate_encoding.long()\n",
    "    input[\"offset_mapping\"] = offset\n",
    "    input[\"gt\"] = gt\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    return input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SRL(\"EN\",tokenizer,\"train\")\n",
    "#same mapping should be used in both the dataset\n",
    "dev_dataset = SRL(\"EN\",tokenizer,\"dev\",train_dataset.args_roles,train_dataset.pos_list,train_dataset.predicate_dis)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'args_roles': ['result', 'goal', 'co-agent', 'purpose', 'stimulus', 'co-patient', 'attribute', 'time', 'destination', 'agent', 'asset', '_', 'location', 'cause', 'product', 'experiencer', 'topic', 'source', 'extent', 'theme', 'beneficiary', 'patient', 'value', 'co-theme', 'recipient', 'instrument', 'material', 'UNK'], 'pos_list': ['ADV', 'VERB', 'NOUN', 'PRON', 'INTJ', 'ADJ', 'ADP', 'PUNCT', 'DET', 'PART', 'NUM', 'SCONJ', 'CCONJ', 'AUX', 'X', 'PROPN', 'SYM', 'Nothing', 'UNK'], 'predicate_dis': ['VISIT', 'DISCARD', 'WAIT', 'JUSTIFY_EXCUSE', 'HURT_HARM_ACHE', 'RECORD', 'RELY', 'OPPOSE_REBEL_DISSENT', 'LIBERATE_ALLOW_AFFORD', 'INCREASE_ENLARGE_MULTIPLY', 'OBTAIN', 'HIRE', 'FOLLOW-IN-SPACE', 'APPEAR', 'SPEED-UP', 'BUY', 'TAKE-A-SERVICE_RENT', 'HIT', 'VERIFY', 'ALLY_ASSOCIATE_MARRY', 'SORT_CLASSIFY_ARRANGE', 'REPEAT', 'IMAGINE', 'NAME', 'RESIST', 'ATTEND', '_', 'RESULT_CONSEQUENCE', 'REPAIR_REMEDY', 'DESTROY', 'LEARN', 'EXTEND', 'DISCUSS', 'LEAVE_DEPART_RUN-AWAY', 'TRY', 'CONSIDER', 'ADJUST_CORRECT', 'STAY_DWELL', 'DROP', 'LEAD_GOVERN', 'STEAL_DEPRIVE', 'SETTLE_CONCILIATE', 'SHAPE', 'TAKE-INTO-ACCOUNT_CONSIDER', 'INFLUENCE', 'OPEN', 'POSSESS', 'TEACH', 'CONSUME_SPEND', 'LOWER', 'GROUP', 'ANSWER', 'TREAT', 'CHARGE', 'LEND', 'PRECLUDE_FORBID_EXPEL', 'MOVE-SOMETHING', 'CHANGE-APPEARANCE/STATE', 'SEND', 'OVERLAP', 'COME-FROM', 'DISBAND_BREAK-UP', 'ASCRIBE', 'BURDEN_BEAR', 'SUMMARIZE', 'SEEM', 'BELIEVE', 'REGRET_SORRY', 'HOST_MEAL_INVITE', 'PUBLICIZE', 'COST', 'PRINT', 'UNDERGO-EXPERIENCE', 'FLOW', 'REMAIN', 'WORSEN', 'LEAVE-BEHIND', 'ORDER', 'CRITICIZE', 'COPY', 'COMPETE', 'PLAN_SCHEDULE', 'SUBJECTIVE-JUDGING', 'ATTACK_BOMB', 'RESTRAIN', 'EXEMPT', 'SHARPEN', 'INVERT_REVERSE-', 'COUNT', 'CORRELATE', 'CAGE_IMPRISON', 'RECEIVE', 'PROPOSE', 'HAVE-A-FUNCTION_SERVE', 'TRAVEL', 'FIND', 'MOVE-ONESELF', 'FACE_CHALLENGE', 'REVEAL', 'PROTECT', 'EXIST_LIVE', 'RESIGN_RETIRE', 'SEARCH', 'VIOLATE', 'REFER', 'PRESERVE', 'MATCH', 'SLOW-DOWN', 'SOLVE', 'LOAD_PROVIDE_CHARGE_FURNISH', 'GIVE-BIRTH', 'ASK_REQUEST', 'CAUSE-SMT', 'REQUIRE_NEED_WANT_HOPE', 'SEPARATE_FILTER_DETACH', 'CELEBRATE_PARTY', 'PRESS_PUSH_FOLD', 'COMMUNICATE_CONTACT', 'SHOW', 'ACCUSE', 'CANCEL_ELIMINATE', 'RETAIN_KEEP_SAVE-MONEY', 'RAISE', 'KILL', 'INSERT', 'DECREE_DECLARE', 'ATTACH', 'MEASURE_EVALUATE', 'STOP', 'DEFEAT', 'HELP_HEAL_CARE_CURE', 'TURN_CHANGE-DIRECTION', 'ABSTAIN_AVOID_REFRAIN', 'INCLUDE-AS', 'CALCULATE_ESTIMATE', 'RISK', 'RESTORE-TO-PREVIOUS/INITIAL-STATE_UNDO_UNWIND', 'COMPARE', 'ACHIEVE', 'ACCOMPANY', 'CONTINUE', 'CATCH', 'ANALYZE', 'AGREE_ACCEPT', 'EMIT', 'GIVE_GIFT', 'SPEAK', 'SATISFY_FULFILL', 'OPERATE', 'PAINT', 'MISS_OMIT_LACK', 'UNDERSTAND', 'REACT', 'SHARE', 'APPROVE_PRAISE', 'REACH', 'WIN', 'COOL', 'REFUSE', 'GIVE-UP_ABOLISH_ABANDON', 'SPEND-TIME_PASS-TIME', 'EMBELLISH', 'PREPARE', 'REDUCE_DIMINISH', 'EARN', 'MOVE-BACK', 'STRENGTHEN_MAKE-RESISTANT', 'OBEY', 'PARTICIPATE', 'INCITE_INDUCE', 'CO-OPT', 'ENDANGER', 'LIE', 'CHASE', 'BEHAVE', 'EXHAUST', 'ARRIVE', 'DERIVE', 'WELCOME', 'ABSORB', 'CONTRACT-AN-ILLNESS_INFECT', 'COVER_SPREAD_SURMOUNT', 'SHOOT_LAUNCH_PROPEL', 'INFORM', 'DECIDE_DETERMINE', 'PERCEIVE', 'READ', 'PUT_APPLY_PLACE_PAVE', 'MANAGE', 'TAKE', 'GUARANTEE_ENSURE_PROMISE', 'FAIL_LOSE', 'CITE', 'AROUSE_WAKE_ENLIVEN', 'CARRY_TRANSPORT', 'DELAY', 'SECURE_FASTEN_TIE', 'JOIN_CONNECT', 'APPLY', 'COME-AFTER_FOLLOW-IN-TIME', 'WATCH_LOOK-OUT', 'PERSUADE', 'ADD', 'INTERPRET', 'COMPENSATE', 'PROVE', 'REFLECT', 'GROUND_BASE_FOUND', 'REMEMBER', 'BORDER', 'PAY', 'KNOW', 'TAKE-SHELTER', 'REPLACE', 'WASH_CLEAN', 'AUTHORIZE_ADMIT', 'PERFORM', 'PULL', 'AFFIRM', 'PROMOTE', 'DIVIDE', 'OVERCOME_SURPASS', 'HAPPEN_OCCUR', 'USE', 'GROW_PLOW', 'ENCLOSE_WRAP', 'IMPLY', 'FOLLOW_SUPPORT_SPONSOR_FUND', 'HEAR_LISTEN', 'ORIENT', 'REPRESENT', 'EXPLAIN', 'COMBINE_MIX_UNITE', 'ORGANIZE', 'WARN', 'AMELIORATE', 'STABILIZE_SUPPORT-PHYSICALLY', 'CIRCULATE_SPREAD_DISTRIBUTE', 'PUBLISH', 'CONTAIN', 'LOSE', 'CREATE_MATERIALIZE', 'BREAK_DETERIORATE', 'WRITE', 'OBLIGE_FORCE', 'FINISH_CONCLUDE_END', 'FILL', 'CHANGE_SWITCH', 'DRIVE-BACK', 'LOCATE-IN-TIME_DATE', 'LIKE', 'THINK', 'MEET', 'FOCUS', 'EMPHASIZE', 'MOUNT_ASSEMBLE_PRODUCE', 'REMOVE_TAKE-AWAY_KIDNAP', 'ARGUE-IN-DEFENSE', 'TRANSLATE', 'EMPTY_UNLOAD', 'DEBASE_ADULTERATE', 'MEAN', 'CAUSE-MENTAL-STATE', 'RECOGNIZE_ADMIT_IDENTIFY', 'CHOOSE', 'SIMPLIFY', 'AFFECT', 'NEGOTIATE', 'INVERT_REVERSE', 'NOURISH_FEED', 'ENTER', 'BENEFIT_EXPLOIT', 'FIT', 'FIGHT', 'DOWNPLAY_HUMILIATE', 'CARRY-OUT-ACTION', 'HARMONIZE', 'GUESS', 'BEGIN', 'TOLERATE', 'DISMISS_FIRE-SMN', 'DISTINGUISH_DIFFER', 'SUMMON', 'METEOROLOGICAL', 'ASSIGN-SMT-TO-SMN', 'SEE', 'SIGNAL_INDICATE', 'DIRECT_AIM_MANEUVER', 'ESTABLISH', 'WORK', 'BEFRIEND', 'BE-LOCATED_BASE', 'SELL', 'TRANSMIT', 'BRING', 'GO-FORWARD', 'EXIST-WITH-FEATURE', 'AMASS', 'DIVERSIFY', 'QUARREL_POLEMICIZE', 'OFFER', 'PUNISH', 'ATTRACT_SUCK', 'Nothing', 'UNK']}\n",
      "['ADV', 'VERB', 'NOUN', 'PRON', 'INTJ', 'ADJ', 'ADP', 'PUNCT', 'DET', 'PART', 'NUM', 'SCONJ', 'CCONJ', 'AUX', 'X', 'PROPN', 'SYM', 'Nothing', 'UNK']\n"
     ]
    }
   ],
   "source": [
    "#print(train_dataset.args_roles,train_dataset.pos_list,train_dataset.predicate_dis)\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "data = {\n",
    "    'args_roles' : train_dataset.args_roles,\n",
    "    \"pos_list\" : train_dataset.pos_list,\n",
    "    \"predicate_dis\" : train_dataset.predicate_dis,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open('hw2/stud/saved/vocabulary.json', 'w') as outfile:\n",
    "    json.dump(data, outfile)\n",
    "\n",
    "\n",
    "with open('hw2/stud/saved/vocabulary.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "print(data)\n",
    "pos_list = data['pos_list']\n",
    "args_roles = data['args_roles']\n",
    "predicate_dis = data['predicate_dis']\n",
    "print(pos_list)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "embeddings = dict()\n",
    "\n",
    "embeddings[\"predicate_flag_embedding_output_dim\"] = 32\n",
    "#defined in initial exploration of the dataset\n",
    "embeddings[\"pos_embedding_input_dim\"] = 0\n",
    "embeddings[\"pos_embedding_output_dim\"] = 100\n",
    "#-------------------------------------------------\n",
    "embeddings[\"predicate_embedding_input_dim\"] = 0\n",
    "embeddings[\"predicate_embedding_output_dim\"] = False\n",
    "#defined in initial exploration of the dataset\n",
    "n_classes = 0\n",
    "\n",
    "\n",
    "\n",
    "bilstm = dict()\n",
    "bilstm[\"n_layers\"] = 2\n",
    "bilstm[\"output_dim\"] = 50\n",
    "dropouts = [0.4,0.3,0.3]\n",
    "\n",
    "language_portable = True\n",
    "predicate_meaning = True\n",
    "pos = True\n",
    "\n",
    "cfg = dict()\n",
    "cfg[\"embeddings\"] = embeddings\n",
    "cfg[\"n_classes\"] = n_classes\n",
    "cfg[\"bilstm\"] = bilstm\n",
    "cfg[\"language_portable\"] = language_portable\n",
    "cfg[\"dropouts\"] = dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arg_Classifier(\n",
      "  (bi_lstm_portable): LSTM(132, 50, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "  (embedding_predicate_flag): Embedding(2, 32, max_norm=True)\n",
      "  (embedding_predicate): Embedding(305, False, max_norm=True)\n",
      "  (embedding_pos): Embedding(19, 100, max_norm=True)\n",
      "  (bi_lstm): LSTM(900, 50, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "  (dropout_language_constraint): Dropout(p=0.6, inplace=False)\n",
      "  (dropout_in_classifier): Dropout(p=0.4, inplace=False)\n",
      "  (Relu): ReLU()\n",
      "  (Sigmoid): Sigmoid()\n",
      "  (linear0): Linear(in_features=300, out_features=700, bias=True)\n",
      "  (linear1): Linear(in_features=700, out_features=140, bias=True)\n",
      "  (linear2): Linear(in_features=140, out_features=28, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#from mmcv import Config\n",
    "from hw2.stud.arg import Arg_Classifier, Arg_Classifier_from_paper\n",
    "\n",
    "\n",
    "\n",
    "#cfg = Config.fromfile('/home/francesco/Desktop/nlp2022-hw2-main/hw2/stud/configs/model.py')\n",
    "\n",
    "cfg[\"embeddings\"][\"pos_embedding_input_dim\"] = len(train_dataset.pos_list)\n",
    "cfg[\"embeddings\"][\"predicate_embedding_input_dim\"] = len(train_dataset.predicate_dis)\n",
    "cfg[\"n_classes\"] = len(train_dataset.args_roles)\n",
    "\n",
    "\n",
    "model = Arg_Classifier(\"EN\",cfg).cuda()\n",
    "#model = Arg_Classifier_from_paper(\"EN\",cfg).cuda()\n",
    "print(model)\n",
    "\n",
    "automodel = auto_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def metrics(gold,pred):\n",
    "    true_positives, false_positives, false_negatives = 0, 0, 0\n",
    "    null_tag = \"_\"\n",
    "\n",
    "    for i,r_g in  enumerate(gold):\n",
    "        r_p = pred[i]\n",
    "\n",
    "        if r_g != null_tag and r_p != null_tag:\n",
    "            true_positives += 1\n",
    "        elif r_g != null_tag and r_p == null_tag:\n",
    "            false_negatives += 1\n",
    "        elif r_g == null_tag and r_p != null_tag:\n",
    "            false_positives += 1\n",
    "\n",
    "    a = true_positives + false_positives\n",
    "    b = true_positives + false_negatives\n",
    "    if a == 0 and b == 0 :        \n",
    "        argument_identification = {\n",
    "            \"true_positives\": 0,\n",
    "            \"false_positives\": 0,\n",
    "            \"false_negatives\": 0,\n",
    "            \"precision\": 0,\n",
    "            \"recall\": 0,\n",
    "            \"f1\": 0,\n",
    "        } \n",
    "\n",
    "    else : \n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        argument_identification = {\n",
    "            \"true_positives\": true_positives,\n",
    "            \"false_positives\": false_positives,\n",
    "            \"false_negatives\": false_negatives,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    true_positives, false_positives, false_negatives = 0, 0, 0\n",
    "    for i,r_g in  enumerate(gold):\n",
    "        r_p = pred[i]\n",
    "\n",
    "        if r_g != null_tag and r_p != null_tag:\n",
    "            if r_g == r_p:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                false_positives += 1\n",
    "                false_negatives += 1\n",
    "        elif r_g != null_tag and r_p == null_tag:\n",
    "                false_negatives += 1\n",
    "        elif r_g == null_tag and r_p != null_tag:\n",
    "                false_positives += 1\n",
    "\n",
    "\n",
    "    a = true_positives + false_positives\n",
    "    b = true_positives + false_negatives\n",
    "    if a == 0 and b == 0 :\n",
    "        argument_classification = {\n",
    "            \"true_positives\": 0,\n",
    "            \"false_positives\": 0,\n",
    "            \"false_negatives\": 0,\n",
    "            \"precision\": 0,\n",
    "            \"recall\": 0,\n",
    "            \"f1\": 0,\n",
    "        } \n",
    "\n",
    "    else : \n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        argument_classification = {\n",
    "            \"true_positives\": true_positives,\n",
    "            \"false_positives\": false_positives,\n",
    "            \"false_negatives\": false_negatives,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "        }\n",
    "\n",
    "\n",
    "    return argument_identification,argument_classification\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "from [1,1,8,8,8,8,8,8,8,8,8,8,8,8,2,2,2,8,8,8......]\n",
    "to from [agent,agent,_,_,_........,]\n",
    "\"\"\"\n",
    "def mapping_args(g,p,mapping):\n",
    "    \n",
    "    \n",
    "    gt = [mapping[elem] for elem in g]\n",
    "    predictions = [mapping[elem] for elem in p]\n",
    "\n",
    "\n",
    "    return gt,predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Argument Identification and Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs n. 0\n",
      "F1 train: [0.08643815 0.21511018 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.74044153 0.         0.98816612\n",
      " 0.         0.         0.         0.         0.46160962 0.\n",
      " 0.         0.52845967 0.         0.32482172 0.         0.\n",
      " 0.28207307 0.         0.        ]\n",
      "F1 avg train: 0.9601526783280693\n",
      "identification {'true_positives': 16033, 'false_positives': 1728, 'false_negatives': 8855, 'precision': 0.9027081808456731, 'recall': 0.6442060430729669, 'f1': 0.7518581912823279}\n",
      "classification_result {'true_positives': 10839, 'false_positives': 6922, 'false_negatives': 14049, 'precision': 0.6102696920218457, 'recall': 0.4355110896817743, 'f1': 0.5082885882435696}\n",
      "EPOCHS : 0\n",
      "F1 eval : [0.         0.2697201  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.75091697 0.         0.98994539\n",
      " 0.         0.         0.         0.         0.50916497 0.\n",
      " 0.         0.54124189 0.         0.27832512 0.         0.\n",
      " 0.35744681 0.         0.        ]\n",
      "F1 avg eval : 0.9644224733321759\n",
      "identification {'true_positives': 3442, 'false_positives': 337, 'false_negatives': 1620, 'precision': 0.9108229690394284, 'recall': 0.6799683919399447, 'f1': 0.7786449496663274}\n",
      "classification_result {'true_positives': 2335, 'false_positives': 1444, 'false_negatives': 2727, 'precision': 0.6178883302460969, 'recall': 0.4612801264322402, 'f1': 0.5282207895034499}\n",
      "SAVED : hw2/stud/saved/model_2022_12_24_17_43_42.pth\n",
      "Epochs n. 1\n",
      "F1 train: [0.12103746 0.25648855 0.         0.         0.         0.\n",
      " 0.03236246 0.         0.02027027 0.77278686 0.         0.98924065\n",
      " 0.         0.         0.         0.         0.50930549 0.\n",
      " 0.         0.55908531 0.         0.44156894 0.         0.\n",
      " 0.43524096 0.         0.        ]\n",
      "F1 avg train: 0.963522184841015\n",
      "identification {'true_positives': 16917, 'false_positives': 1658, 'false_negatives': 7968, 'precision': 0.9107402422611036, 'recall': 0.6798071127185051, 'f1': 0.778508973768983}\n",
      "classification_result {'true_positives': 11927, 'false_positives': 6648, 'false_negatives': 12958, 'precision': 0.6420995962314939, 'recall': 0.4792847096644565, 'f1': 0.5488725264611136}\n",
      "EPOCHS : 1\n",
      "F1 eval : [0.         0.43137255 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.78307953 0.         0.99060473\n",
      " 0.         0.         0.         0.         0.63423423 0.\n",
      " 0.         0.58283133 0.         0.5647452  0.         0.\n",
      " 0.54480287 0.         0.        ]\n",
      "F1 avg eval : 0.9691117609692753\n",
      "identification {'true_positives': 3476, 'false_positives': 246, 'false_negatives': 1575, 'precision': 0.9339065018807093, 'recall': 0.6881805583052861, 'f1': 0.7924313233785479}\n",
      "classification_result {'true_positives': 2620, 'false_positives': 1102, 'false_negatives': 2431, 'precision': 0.7039226222461042, 'recall': 0.5187091665016829, 'f1': 0.5972871309700216}\n",
      "SAVED : hw2/stud/saved/model_2022_12_24_17_43_42.pth\n",
      "Epochs n. 2\n",
      "F1 train: [0.1734104  0.35854342 0.         0.         0.         0.\n",
      " 0.13580247 0.         0.06329114 0.79658353 0.         0.99010076\n",
      " 0.         0.         0.         0.         0.55413168 0.\n",
      " 0.         0.59291115 0.         0.54435178 0.         0.00743494\n",
      " 0.54004252 0.         0.        ]\n",
      "F1 avg train: 0.966702122539958\n",
      "identification {'true_positives': 17624, 'false_positives': 1605, 'false_negatives': 7250, 'precision': 0.9165323209735295, 'recall': 0.7085309962209536, 'f1': 0.7992200077092262}\n",
      "classification_result {'true_positives': 13024, 'false_positives': 6205, 'false_negatives': 11850, 'precision': 0.6773103125487545, 'recall': 0.5235989386508, 'f1': 0.5906174183162143}\n",
      "EPOCHS : 2\n",
      "F1 eval : [0.         0.49212598 0.         0.         0.         0.\n",
      " 0.06896552 0.         0.04347826 0.77575356 0.         0.9911847\n",
      " 0.         0.         0.         0.         0.60218978 0.\n",
      " 0.         0.60677855 0.         0.53748783 0.         0.\n",
      " 0.59016393 0.         0.        ]\n",
      "F1 avg eval : 0.969869036620631\n",
      "identification {'true_positives': 3707, 'false_positives': 350, 'false_negatives': 1358, 'precision': 0.9137293566674883, 'recall': 0.7318854886475814, 'f1': 0.8127603595702697}\n",
      "classification_result {'true_positives': 2752, 'false_positives': 1305, 'false_negatives': 2313, 'precision': 0.6783337441459206, 'recall': 0.5433366238894373, 'f1': 0.6033764525323395}\n",
      "SAVED : hw2/stud/saved/model_2022_12_24_17_43_42.pth\n",
      "Epochs n. 3\n",
      "F1 train: [0.23129252 0.40291705 0.         0.         0.15242494 0.\n",
      " 0.16997167 0.         0.12849162 0.80813163 0.         0.99065653\n",
      " 0.         0.         0.         0.06413994 0.59454927 0.01680672\n",
      " 0.         0.61349607 0.         0.59715121 0.         0.05498282\n",
      " 0.59347971 0.         0.        ]\n",
      "F1 avg train: 0.9687184445530945\n",
      "identification {'true_positives': 18171, 'false_positives': 1621, 'false_negatives': 6735, 'precision': 0.9180982215036378, 'recall': 0.7295832329559142, 'f1': 0.8130565125956418}\n",
      "classification_result {'true_positives': 13740, 'false_positives': 6052, 'false_negatives': 11166, 'precision': 0.6942198868229588, 'recall': 0.5516742953505179, 'f1': 0.6147926081703879}\n",
      "EPOCHS : 3\n",
      "F1 eval : [0.11428571 0.51111111 0.         0.         0.14705882 0.\n",
      " 0.13793103 0.         0.04255319 0.79384418 0.         0.99183648\n",
      " 0.         0.         0.         0.         0.64144144 0.\n",
      " 0.         0.63422933 0.         0.64367816 0.         0.04347826\n",
      " 0.66447368 0.         0.        ]\n",
      "F1 avg eval : 0.9724229807270155\n",
      "identification {'true_positives': 3893, 'false_positives': 412, 'false_negatives': 1172, 'precision': 0.9042973286875726, 'recall': 0.7686080947680158, 'f1': 0.8309498399146212}\n",
      "classification_result {'true_positives': 2996, 'false_positives': 1309, 'false_negatives': 2069, 'precision': 0.6959349593495935, 'recall': 0.5915103652517275, 'f1': 0.6394877267876201}\n",
      "SAVED : hw2/stud/saved/model_2022_12_24_17_43_42.pth\n",
      "Epochs n. 4\n",
      "F1 train: [0.28761651 0.43802015 0.         0.         0.27272727 0.\n",
      " 0.24930748 0.         0.17811705 0.82192152 0.         0.99124831\n",
      " 0.         0.         0.         0.17662338 0.63305785 0.04897959\n",
      " 0.03571429 0.64540548 0.00660066 0.63806777 0.         0.14379085\n",
      " 0.64490862 0.         0.        ]\n",
      "F1 avg train: 0.9709762246910226\n",
      "identification {'true_positives': 18683, 'false_positives': 1641, 'false_negatives': 6181, 'precision': 0.9192580200747884, 'recall': 0.7514076576576577, 'f1': 0.8269009471541117}\n",
      "classification_result {'true_positives': 14527, 'false_positives': 5797, 'false_negatives': 10337, 'precision': 0.714770714426294, 'recall': 0.5842583655083655, 'f1': 0.6429583075152695}\n",
      "EPOCHS : 4\n",
      "F1 eval : [0.13461538 0.54545455 0.         0.         0.14925373 0.\n",
      " 0.13559322 0.         0.16666667 0.79923518 0.         0.99198534\n",
      " 0.         0.         0.         0.14925373 0.68512111 0.\n",
      " 0.         0.63885992 0.         0.66999169 0.         0.07843137\n",
      " 0.64347826 0.         0.        ]\n",
      "F1 avg eval : 0.9732424421879764\n",
      "identification {'true_positives': 3902, 'false_positives': 397, 'false_negatives': 1151, 'precision': 0.9076529425447779, 'recall': 0.7722145260241441, 'f1': 0.8344739093242086}\n",
      "classification_result {'true_positives': 3050, 'false_positives': 1249, 'false_negatives': 2003, 'precision': 0.7094673179809258, 'recall': 0.6036018207005739, 'f1': 0.6522668947818648}\n",
      "SAVED : hw2/stud/saved/model_2022_12_24_17_43_42.pth\n",
      "Epochs n. 5\n",
      "F1 train: [0.33889602 0.4670913  0.         0.         0.32475248 0.\n",
      " 0.27894737 0.         0.22900763 0.83300722 0.         0.9917313\n",
      " 0.         0.         0.         0.24197531 0.64977885 0.09448819\n",
      " 0.06779661 0.65810056 0.02848101 0.66726879 0.         0.2028169\n",
      " 0.65889968 0.         0.        ]\n",
      "F1 avg train: 0.9723917903825492\n",
      "identification {'true_positives': 19114, 'false_positives': 1623, 'false_negatives': 5757, 'precision': 0.9217340984713315, 'recall': 0.7685255920550038, 'f1': 0.8381862831082266}\n",
      "classification_result {'true_positives': 15019, 'false_positives': 5718, 'false_negatives': 9852, 'precision': 0.724260982784395, 'recall': 0.6038760001608299, 'f1': 0.6586125241185756}\n",
      "EPOCHS : 5\n",
      "F1 eval : [0.35       0.52991453 0.         0.         0.23684211 0.\n",
      " 0.22222222 0.         0.11764706 0.80381558 0.         0.99182898\n",
      " 0.         0.         0.         0.2        0.7026087  0.\n",
      " 0.         0.6557377  0.         0.67881944 0.         0.14545455\n",
      " 0.68027211 0.         0.        ]\n",
      "F1 avg eval : 0.9738829297845334\n",
      "identification {'true_positives': 3829, 'false_positives': 350, 'false_negatives': 1237, 'precision': 0.916247906197655, 'recall': 0.7558231346229767, 'f1': 0.8283396430502975}\n",
      "classification_result {'true_positives': 3072, 'false_positives': 1107, 'false_negatives': 1994, 'precision': 0.7351040918880115, 'recall': 0.6063955783655745, 'f1': 0.6645754461871282}\n",
      "SAVED : hw2/stud/saved/model_2022_12_24_17_43_42.pth\n",
      "Epochs n. 6\n",
      "F1 train: [0.37313433 0.52768456 0.         0.         0.37254902 0.\n",
      " 0.32994924 0.         0.21182266 0.83898362 0.         0.9919998\n",
      " 0.00970874 0.         0.         0.36986301 0.68189103 0.13846154\n",
      " 0.26470588 0.68581105 0.0896861  0.69209139 0.         0.22960725\n",
      " 0.69338422 0.         0.        ]\n",
      "F1 avg train: 0.9740159528495089\n",
      "identification {'true_positives': 19327, 'false_positives': 1568, 'false_negatives': 5591, 'precision': 0.9249581239530988, 'recall': 0.7756240468737459, 'f1': 0.8437343112217056}\n",
      "classification_result {'true_positives': 15608, 'false_positives': 5287, 'false_negatives': 9310, 'precision': 0.7469729600382867, 'recall': 0.6263745083875111, 'f1': 0.6813786479820139}\n",
      "EPOCHS : 6\n",
      "F1 eval : [0.39344262 0.58089669 0.06060606 0.         0.25316456 0.\n",
      " 0.27272727 0.         0.12244898 0.81206349 0.         0.992386\n",
      " 0.         0.         0.         0.27027027 0.71232877 0.\n",
      " 0.         0.66059379 0.04225352 0.66901408 0.         0.24137931\n",
      " 0.69830508 0.         0.        ]\n",
      "F1 avg eval : 0.974964277094567\n",
      "identification {'true_positives': 3969, 'false_positives': 378, 'false_negatives': 1094, 'precision': 0.9130434782608695, 'recall': 0.783922575548094, 'f1': 0.8435706695005313}\n",
      "classification_result {'true_positives': 3165, 'false_positives': 1182, 'false_negatives': 1898, 'precision': 0.728088336783989, 'recall': 0.6251234445980643, 'f1': 0.6726886291179596}\n",
      "SAVED : hw2/stud/saved/model_2022_12_24_17_43_42.pth\n",
      "Epochs n. 7\n",
      "F1 train: [0.4        0.56516443 0.05194805 0.         0.40148699 0.\n",
      " 0.32352941 0.05555556 0.28187919 0.84894219 0.         0.99225549\n",
      " 0.03846154 0.         0.         0.46376812 0.6978389  0.19787986\n",
      " 0.24657534 0.69746519 0.19072848 0.71360843 0.         0.29787234\n",
      " 0.718398   0.         0.        ]\n",
      "F1 avg train: 0.9752705406480231\n",
      "identification {'true_positives': 19594, 'false_positives': 1605, 'false_negatives': 5300, 'precision': 0.9242888815510165, 'recall': 0.787097292520286, 'f1': 0.8501941726509449}\n",
      "classification_result {'true_positives': 16056, 'false_positives': 5143, 'false_negatives': 8838, 'precision': 0.7573942167083353, 'recall': 0.6449746926970354, 'f1': 0.696678454429089}\n",
      "EPOCHS : 7\n",
      "F1 eval : [0.46616541 0.59642147 0.06060606 0.         0.34146341 0.\n",
      " 0.33802817 0.         0.25396825 0.81849857 0.         0.99255547\n",
      " 0.         0.         0.         0.34567901 0.73248408 0.04444444\n",
      " 0.25       0.68581952 0.34736842 0.71312804 0.         0.32911392\n",
      " 0.73417722 0.         0.        ]\n",
      "F1 avg eval : 0.9767464713508704\n",
      "identification {'true_positives': 4000, 'false_positives': 382, 'false_negatives': 1056, 'precision': 0.9128251939753537, 'recall': 0.7911392405063291, 'f1': 0.8476372112735749}\n",
      "classification_result {'true_positives': 3287, 'false_positives': 1095, 'false_negatives': 1769, 'precision': 0.7501141031492469, 'recall': 0.650118670886076, 'f1': 0.6965458783640602}\n",
      "SAVED : hw2/stud/saved/model_2022_12_24_17_43_42.pth\n",
      "Epochs n. 8\n",
      "F1 train: [0.43488943 0.56570513 0.08917197 0.         0.50440917 0.\n",
      " 0.38979118 0.         0.28361858 0.85314497 0.         0.99260837\n",
      " 0.01877934 0.         0.         0.512      0.72812135 0.26351351\n",
      " 0.25352113 0.70929485 0.2106599  0.72167139 0.         0.32116788\n",
      " 0.73379484 0.         0.        ]\n",
      "F1 avg train: 0.9763071815363296\n",
      "identification {'true_positives': 19834, 'false_positives': 1573, 'false_negatives': 5012, 'precision': 0.926519362825244, 'recall': 0.7982773887144812, 'f1': 0.8576308563768836}\n",
      "classification_result {'true_positives': 16364, 'false_positives': 5043, 'false_negatives': 8482, 'precision': 0.7644228523380203, 'recall': 0.6586170812203171, 'f1': 0.7075865349274642}\n",
      "EPOCHS : 8\n",
      "F1 eval : [0.46511628 0.58823529 0.         0.         0.34090909 0.\n",
      " 0.36842105 0.         0.22222222 0.81615509 0.         0.99255368\n",
      " 0.         0.         0.         0.325      0.7245409  0.12244898\n",
      " 0.4        0.69462366 0.37113402 0.71742809 0.         0.38636364\n",
      " 0.73220339 0.         0.        ]\n",
      "F1 avg eval : 0.9768454060053281\n",
      "identification {'true_positives': 3977, 'false_positives': 358, 'false_negatives': 1082, 'precision': 0.9174163783160323, 'recall': 0.7861237398695394, 'f1': 0.8467106663827975}\n",
      "classification_result {'true_positives': 3272, 'false_positives': 1063, 'false_negatives': 1787, 'precision': 0.7547866205305652, 'recall': 0.646768135995256, 'f1': 0.6966148605492869}\n",
      "SAVED : hw2/stud/saved/model_2022_12_24_17_43_42.pth\n",
      "Epochs n. 9\n",
      "F1 train: [0.45241038 0.58952497 0.07407407 0.         0.51273345 0.\n",
      " 0.43847875 0.10810811 0.35       0.86080881 0.         0.99285914\n",
      " 0.01923077 0.         0.         0.56916996 0.73756695 0.30967742\n",
      " 0.35       0.72706373 0.24907522 0.738041   0.         0.34070796\n",
      " 0.76118463 0.         0.        ]\n",
      "F1 avg train: 0.9774247194159079\n",
      "identification {'true_positives': 20029, 'false_positives': 1534, 'false_negatives': 4825, 'precision': 0.9288596206464778, 'recall': 0.8058662589522814, 'f1': 0.8630027791541892}\n",
      "classification_result {'true_positives': 16770, 'false_positives': 4793, 'false_negatives': 8084, 'precision': 0.7777210963224042, 'recall': 0.6747404844290658, 'f1': 0.72258008919146}\n",
      "EPOCHS : 9\n",
      "F1 eval : [0.44094488 0.59203036 0.27027027 0.         0.32911392 0.\n",
      " 0.37333333 0.         0.29411765 0.81971304 0.         0.99276465\n",
      " 0.         0.         0.         0.29333333 0.73421927 0.16\n",
      " 0.28571429 0.68475992 0.28571429 0.71760797 0.         0.38709677\n",
      " 0.73065015 0.         0.        ]\n",
      "F1 avg eval : 0.976994753691466\n",
      "identification {'true_positives': 4109, 'false_positives': 453, 'false_negatives': 945, 'precision': 0.9007014467338886, 'recall': 0.8130193905817175, 'f1': 0.8546173044925124}\n",
      "classification_result {'true_positives': 3353, 'false_positives': 1209, 'false_negatives': 1701, 'precision': 0.7349846558526962, 'recall': 0.6634349030470914, 'f1': 0.6973793677204658}\n",
      "SAVED : hw2/stud/saved/model_2022_12_24_17_43_42.pth\n",
      "Epochs n. 10\n",
      "F1 train: [0.45862884 0.5957617  0.2122905  0.         0.53094463 0.03225806\n",
      " 0.41685144 0.05263158 0.38852097 0.86689053 0.         0.99316385\n",
      " 0.05381166 0.         0.04040404 0.61036468 0.75603217 0.41642229\n",
      " 0.36363636 0.73479911 0.32349469 0.75131165 0.         0.37209302\n",
      " 0.74968553 0.         0.        ]\n",
      "F1 avg train: 0.9783302642708992\n",
      "identification {'true_positives': 20306, 'false_positives': 1527, 'false_negatives': 4560, 'precision': 0.9300600009160446, 'recall': 0.8166170674817019, 'f1': 0.8696545964581682}\n",
      "classification_result {'true_positives': 17091, 'false_positives': 4742, 'false_negatives': 7775, 'precision': 0.7828058443640361, 'recall': 0.6873240569452264, 'f1': 0.7319642818904044}\n",
      "EPOCHS : 10\n",
      "F1 eval : [0.45801527 0.60348162 0.33333333 0.         0.35294118 0.\n",
      " 0.37974684 0.         0.26666667 0.81933039 0.         0.99273093\n",
      " 0.         0.         0.         0.38636364 0.76083467 0.25454545\n",
      " 0.28571429 0.69119769 0.43925234 0.72868217 0.         0.35051546\n",
      " 0.74074074 0.         0.        ]\n",
      "F1 avg eval : 0.9775654311154274\n",
      "identification {'true_positives': 4055, 'false_positives': 412, 'false_negatives': 990, 'precision': 0.9077680770091784, 'recall': 0.8037661050545094, 'f1': 0.8526072329688813}\n",
      "classification_result {'true_positives': 3347, 'false_positives': 1120, 'false_negatives': 1698, 'precision': 0.7492724423550481, 'recall': 0.6634291377601585, 'f1': 0.7037426408746845}\n",
      "SAVED : hw2/stud/saved/model_2022_12_24_17_43_42.pth\n",
      "Epochs n. 11\n",
      "F1 train: [0.4774044  0.61573896 0.21978022 0.         0.53109244 0.09375\n",
      " 0.48927039 0.13636364 0.35730858 0.87546967 0.         0.99340443\n",
      " 0.09322034 0.         0.03960396 0.60902256 0.75846501 0.35795455\n",
      " 0.45977011 0.75141571 0.39116022 0.77119228 0.         0.3776824\n",
      " 0.76674938 0.         0.        ]\n",
      "F1 avg train: 0.9793292388092468\n",
      "identification {'true_positives': 20517, 'false_positives': 1500, 'false_negatives': 4385, 'precision': 0.9318708270881592, 'recall': 0.8239097261264156, 'f1': 0.8745710692896268}\n",
      "classification_result {'true_positives': 17469, 'false_positives': 4548, 'false_negatives': 7433, 'precision': 0.7934323477312986, 'recall': 0.7015099188820175, 'f1': 0.7446450265350923}\n",
      "EPOCHS : 11\n",
      "F1 eval : [0.47692308 0.62697023 0.35555556 0.         0.39583333 0.\n",
      " 0.4        0.         0.36111111 0.81887755 0.         0.99276495\n",
      " 0.         0.         0.         0.42222222 0.74757282 0.21818182\n",
      " 0.33333333 0.70512334 0.42424242 0.73301738 0.         0.42696629\n",
      " 0.74657534 0.         0.        ]\n",
      "F1 avg eval : 0.9779636805988673\n",
      "identification {'true_positives': 4039, 'false_positives': 381, 'false_negatives': 1017, 'precision': 0.9138009049773755, 'recall': 0.7988528481012658, 'f1': 0.8524693963697764}\n",
      "classification_result {'true_positives': 3369, 'false_positives': 1051, 'false_negatives': 1687, 'precision': 0.7622171945701357, 'recall': 0.6663370253164557, 'f1': 0.7110595187842972}\n",
      "SAVED : hw2/stud/saved/model_2022_12_24_17_43_42.pth\n",
      "Epochs n. 12\n",
      "F1 train: [0.49195402 0.61087212 0.20430108 0.         0.58227848 0.\n",
      " 0.50420168 0.13636364 0.4017094  0.87819529 0.         0.99359319\n",
      " 0.09958506 0.03773585 0.04       0.62794918 0.77445855 0.43478261\n",
      " 0.48888889 0.7568508  0.40707965 0.77945514 0.         0.4248927\n",
      " 0.77791719 0.         0.        ]\n",
      "F1 avg train: 0.979933355575844\n",
      "identification {'true_positives': 20676, 'false_positives': 1491, 'false_negatives': 4221, 'precision': 0.9327378535661118, 'recall': 0.8304615013857091, 'f1': 0.8786333503314634}\n",
      "classification_result {'true_positives': 17690, 'false_positives': 4477, 'false_negatives': 7207, 'precision': 0.7980331122840258, 'recall': 0.7105273727758364, 'f1': 0.751742308346082}\n",
      "EPOCHS : 12\n",
      "F1 eval : [0.48529412 0.63448276 0.32       0.         0.41304348 0.\n",
      " 0.38095238 0.         0.35       0.82146279 0.         0.99287396\n",
      " 0.23529412 0.         0.125      0.50505051 0.75039746 0.31578947\n",
      " 0.28571429 0.70822381 0.45689655 0.73726115 0.         0.37113402\n",
      " 0.74675325 0.         0.        ]\n",
      "F1 avg eval : 0.9784508235917998\n",
      "identification {'true_positives': 4113, 'false_positives': 435, 'false_negatives': 948, 'precision': 0.9043535620052771, 'recall': 0.8126852400711322, 'f1': 0.856072432094911}\n",
      "classification_result {'true_positives': 3428, 'false_positives': 1120, 'false_negatives': 1633, 'precision': 0.7537379067722075, 'recall': 0.6773364947638807, 'f1': 0.7134977625143095}\n",
      "SAVED : hw2/stud/saved/model_2022_12_24_17_43_42.pth\n",
      "Epochs n. 13\n",
      "F1 train: [0.51020408 0.64667436 0.29850746 0.         0.58990536 0.09090909\n",
      " 0.46625767 0.13636364 0.4556962  0.88062492 0.         0.99377133\n",
      " 0.18677043 0.03508772 0.05769231 0.63913824 0.79158843 0.47150259\n",
      " 0.48       0.76924192 0.43278689 0.77781641 0.         0.43333333\n",
      " 0.79827267 0.         0.        ]\n",
      "F1 avg train: 0.9806714639711746\n",
      "identification {'true_positives': 20850, 'false_positives': 1511, 'false_negatives': 4033, 'precision': 0.9324269934260543, 'recall': 0.8379214724912591, 'f1': 0.8826517653035306}\n",
      "classification_result {'true_positives': 17974, 'false_positives': 4387, 'false_negatives': 6909, 'precision': 0.8038102052681008, 'recall': 0.7223405537917453, 'f1': 0.7609008551350437}\n",
      "EPOCHS : 13\n",
      "F1 eval : [0.46616541 0.64184397 0.36734694 0.         0.38709677 0.\n",
      " 0.41860465 0.         0.32911392 0.82747705 0.         0.99297237\n",
      " 0.         0.         0.11764706 0.4        0.75829384 0.3125\n",
      " 0.33333333 0.70967742 0.47169811 0.73658134 0.         0.37777778\n",
      " 0.72897196 0.         0.        ]\n",
      "F1 avg eval : 0.978499439636591\n",
      "identification {'true_positives': 4132, 'false_positives': 432, 'false_negatives': 925, 'precision': 0.9053461875547765, 'recall': 0.8170852283962824, 'f1': 0.8589543706475419}\n",
      "classification_result {'true_positives': 3444, 'false_positives': 1120, 'false_negatives': 1613, 'precision': 0.754601226993865, 'recall': 0.6810361874629227, 'f1': 0.7159338946055502}\n",
      "SAVED : hw2/stud/saved/model_2022_12_24_17_43_42.pth\n",
      "Epochs n. 14\n",
      "F1 train: [0.5273743  0.64452081 0.359447   0.         0.61806656 0.20253165\n",
      " 0.53543307 0.13333333 0.43913043 0.88645764 0.         0.994069\n",
      " 0.2247191  0.16666667 0.0733945  0.6726297  0.78803936 0.47214854\n",
      " 0.5        0.7742826  0.44979079 0.79325921 0.         0.45398773\n",
      " 0.79269061 0.         0.        ]\n",
      "F1 avg train: 0.9813987360795716\n",
      "identification {'true_positives': 21013, 'false_positives': 1451, 'false_negatives': 3820, 'precision': 0.9354077635327636, 'recall': 0.8461724318447228, 'f1': 0.888555299490454}\n",
      "classification_result {'true_positives': 18156, 'false_positives': 4308, 'false_negatives': 6677, 'precision': 0.8082264957264957, 'recall': 0.7311239077034591, 'f1': 0.7677442543924561}\n",
      "EPOCHS : 14\n",
      "F1 eval : [0.46715328 0.62435678 0.38461538 0.         0.37777778 0.125\n",
      " 0.41758242 0.         0.4        0.82998405 0.         0.99293184\n",
      " 0.16216216 0.         0.21052632 0.43956044 0.77742947 0.35135135\n",
      " 0.25       0.71563089 0.48927039 0.73909596 0.         0.41304348\n",
      " 0.77377049 0.         0.        ]\n",
      "F1 avg eval : 0.9788876115912107\n",
      "identification {'true_positives': 4147, 'false_positives': 447, 'false_negatives': 925, 'precision': 0.9026991728341315, 'recall': 0.8176261829652997, 'f1': 0.8580591764949307}\n",
      "classification_result {'true_positives': 3479, 'false_positives': 1115, 'false_negatives': 1593, 'precision': 0.7572921201567262, 'recall': 0.685922712933754, 'f1': 0.7198427477757087}\n",
      "SAVED : hw2/stud/saved/model_2022_12_24_17_43_42.pth\n",
      "Epochs n. 15\n",
      "F1 train: [0.55904659 0.64577039 0.33488372 0.         0.64364207 0.17142857\n",
      " 0.50901804 0.25       0.49166667 0.88687662 0.         0.99412478\n",
      " 0.26428571 0.0952381  0.16260163 0.69708029 0.80695266 0.50761421\n",
      " 0.53191489 0.78516138 0.44444444 0.80256544 0.         0.43162393\n",
      " 0.79974651 0.         0.        ]\n",
      "F1 avg train: 0.9818560071017882\n",
      "identification {'true_positives': 21094, 'false_positives': 1467, 'false_negatives': 3755, 'precision': 0.9349762865121227, 'recall': 0.8488872791661636, 'f1': 0.8898544610841594}\n",
      "classification_result {'true_positives': 18367, 'false_positives': 4194, 'false_negatives': 6482, 'precision': 0.8141039847524489, 'recall': 0.739144432371524, 'f1': 0.7748154397806369}\n",
      "Early stopping at epoch :  15\n",
      "F1 eval : [0.47482014 0.61060329 0.42857143 0.         0.3956044  0.14285714\n",
      " 0.46153846 0.28571429 0.34782609 0.82802146 0.         0.99300714\n",
      " 0.15       0.         0.3        0.45454545 0.77480315 0.36363636\n",
      " 0.22222222 0.71695295 0.50678733 0.74468085 0.         0.35\n",
      " 0.75510204 0.         0.        ]\n",
      "F1 avg eval : 0.978944571700779\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "currentDateAndTime = datetime.now()\n",
    "_id = str(currentDateAndTime.year)+\"_\"+str(currentDateAndTime.month)+\"_\"+str(currentDateAndTime.day)+\"_\"+str(currentDateAndTime.hour)+\"_\"+str(currentDateAndTime.minute)+\"_\"+str(currentDateAndTime.second)\n",
    "\n",
    "\n",
    "\n",
    "#optimizer = torch.optim.Adam(model.parameters(),lr = 0.000005)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "\n",
    "logSotfMax = torch.nn.LogSoftmax(dim=1)\n",
    "nll_loss = torch.nn.NLLLoss()\n",
    "\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "dataloader_dev = DataLoader(dev_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "mapping = dataloader_train.dataset.args_roles\n",
    "\n",
    "auto_model.eval()\n",
    "\n",
    "EPOCHS = 200\n",
    "patience_counter = 0\n",
    "patience = 5\n",
    "max_val_loss = 9999\n",
    "f1_score_max = 0\n",
    "output_path = \"hw2/stud/saved\"\n",
    "model_name = \"model_\"+_id+\".pth\"\n",
    "PATH = os.path.join(output_path,model_name)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    #TRAINING\n",
    "    p = []\n",
    "    g = []\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_train):\n",
    "        #print(sample_batched)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "       \n",
    "        #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "        #------------------FILTERING SUB-WORDS----------------------\n",
    "        subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "        word_emebedding = []\n",
    "        for i in range(n):\n",
    "            subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "            flag = subtoken_mask[0,i,0]\n",
    "            if flag :\n",
    "                continue\n",
    "            else :\n",
    "                word_emebedding.append(subwords_embedding)\n",
    "        word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "        #-------------------------FORWARD/BACKWARD----------------------------------\n",
    "        x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "            perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "            predicate_index = sample_batched[\"predicate_index\"],\n",
    "            pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "            predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])        \n",
    "        b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "        loss = nll_loss(logSotfMax(x),gt)\n",
    "        total_loss = total_loss + loss\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "        counter += 1 \n",
    "            \n",
    "\n",
    "        #-------------------------RESULT STORING----------------------------------\n",
    "        predicted = torch.argmax(x, dim=1)\n",
    "        p += predicted.tolist()\n",
    "        g += gt.tolist()\n",
    "    \n",
    "\n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    scheduler.step()\n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    print(\"Epochs n.\", epoch)\n",
    "    print(\"F1 train:\",f1)\n",
    "    print(\"F1 avg train:\",f1_avg)\n",
    "    \n",
    "    avg_train_loss = total_loss/counter\n",
    "    writer.add_scalar(\"Loss/train\", avg_train_loss, epoch)\n",
    "\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"Train_EN/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"Train_EN/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "    #EVALUATION\n",
    "    p = []\n",
    "    g = []\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_dev):\n",
    "    \n",
    "      #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "            #------------------FILTERING SUB-WORDS----------------------\n",
    "            subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "            word_emebedding = []\n",
    "            for i in range(n): \n",
    "                subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "                flag = subtoken_mask[0,i,0]\n",
    "                if flag :\n",
    "                    continue\n",
    "                else :\n",
    "                    word_emebedding.append(subwords_embedding)\n",
    "            word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "            #-------------------------FORWARD----------------------------------\n",
    "            x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "                        perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "                        predicate_index = sample_batched[\"predicate_index\"],\n",
    "                        pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "                        predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])   \n",
    "\n",
    "\n",
    "            b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "            loss = nll_loss(logSotfMax(x),gt)\n",
    "            total_loss = total_loss + loss\n",
    "            #-------------------------RESULT STORING----------------------------------\n",
    "            predicted = torch.argmax(x, dim=1)\n",
    "            p += predicted.tolist()\n",
    "            g += gt.tolist()\n",
    "            counter += 1 \n",
    "    \n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    avg_eval_loss = total_loss/counter\n",
    "\n",
    "    if avg_eval_loss < max_val_loss:\n",
    "        max_val_loss = avg_eval_loss\n",
    "    else :\n",
    "        patience_counter += 1\n",
    "    \n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    if patience_counter >= patience :  \n",
    "\n",
    "\n",
    "        print(\"Early stopping at epoch : \",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 avg eval :\",f1_avg)\n",
    "        break\n",
    "    else :\n",
    "        print(\"EPOCHS :\",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 avg eval :\",f1_avg)\n",
    "    \n",
    "\n",
    "    writer.add_scalar(\"Loss/validation\", avg_eval_loss, epoch)\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"Eval_EN/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"Eval_EN/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if f1_avg > f1_score_max:\n",
    "        f1_score_max = f1_avg\n",
    "        print(\"SAVED :\",PATH)\n",
    "        torch.save(model.state_dict(),PATH)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Transfert Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning over Structural Information over English dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading English Pretrained Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Arg_Classifier(\n",
       "  (bi_lstm_portable): LSTM(132, 50, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (embedding_predicate_flag): Embedding(2, 32, max_norm=True)\n",
       "  (embedding_predicate): Embedding(304, False, max_norm=True)\n",
       "  (embedding_pos): Embedding(18, 100, max_norm=True)\n",
       "  (bi_lstm): LSTM(900, 50, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (dropout_language_constraint): Dropout(p=0.6, inplace=False)\n",
       "  (dropout_in_classifier): Dropout(p=0.4, inplace=False)\n",
       "  (Relu): ReLU()\n",
       "  (Sigmoid): Sigmoid()\n",
       "  (linear0): Linear(in_features=300, out_features=675, bias=True)\n",
       "  (linear1): Linear(in_features=675, out_features=135, bias=True)\n",
       "  (linear2): Linear(in_features=135, out_features=27, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#note that parameter EN is only used for tracking on which dataset was/is trained\n",
    "model = Arg_Classifier(\"EN\",cfg).cuda()\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language constained training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_language_constrains()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28])\n",
      "2 14\n",
      "torch.Size([320])\n",
      "5 64\n",
      "torch.Size([150])\n",
      "5 30\n",
      "torch.Size([329])\n",
      "7 47\n",
      "torch.Size([45])\n",
      "1 45\n",
      "torch.Size([34])\n",
      "2 17\n",
      "torch.Size([192])\n",
      "3 64\n",
      "torch.Size([69])\n",
      "3 23\n",
      "torch.Size([30])\n",
      "2 15\n",
      "torch.Size([68])\n",
      "4 17\n",
      "torch.Size([58])\n",
      "1 58\n",
      "torch.Size([17])\n",
      "1 17\n",
      "torch.Size([48])\n",
      "2 24\n",
      "torch.Size([116])\n",
      "4 29\n",
      "torch.Size([44])\n",
      "2 22\n",
      "torch.Size([105])\n",
      "3 35\n",
      "torch.Size([20])\n",
      "1 20\n",
      "torch.Size([99])\n",
      "3 33\n",
      "torch.Size([32])\n",
      "2 16\n",
      "torch.Size([45])\n",
      "3 15\n",
      "torch.Size([36])\n",
      "2 18\n",
      "torch.Size([84])\n",
      "2 42\n",
      "torch.Size([50])\n",
      "2 25\n",
      "torch.Size([46])\n",
      "2 23\n",
      "torch.Size([432])\n",
      "6 72\n",
      "torch.Size([250])\n",
      "5 50\n",
      "torch.Size([176])\n",
      "4 44\n",
      "torch.Size([78])\n",
      "3 26\n",
      "torch.Size([192])\n",
      "4 48\n",
      "torch.Size([72])\n",
      "2 36\n",
      "torch.Size([57])\n",
      "3 19\n",
      "torch.Size([190])\n",
      "5 38\n",
      "torch.Size([102])\n",
      "3 34\n",
      "torch.Size([26])\n",
      "2 13\n",
      "torch.Size([120])\n",
      "3 40\n",
      "torch.Size([108])\n",
      "2 54\n",
      "torch.Size([76])\n",
      "2 38\n",
      "torch.Size([180])\n",
      "4 45\n",
      "torch.Size([54])\n",
      "2 27\n",
      "torch.Size([66])\n",
      "2 33\n",
      "torch.Size([40])\n",
      "2 20\n",
      "torch.Size([102])\n",
      "3 34\n",
      "torch.Size([106])\n",
      "2 53\n",
      "torch.Size([26])\n",
      "1 26\n",
      "torch.Size([18])\n",
      "1 18\n",
      "torch.Size([66])\n",
      "3 22\n",
      "torch.Size([33])\n",
      "1 33\n",
      "torch.Size([141])\n",
      "3 47\n",
      "torch.Size([16])\n",
      "1 16\n",
      "torch.Size([164])\n",
      "4 41\n",
      "torch.Size([34])\n",
      "2 17\n",
      "torch.Size([80])\n",
      "2 40\n",
      "torch.Size([168])\n",
      "4 42\n",
      "torch.Size([40])\n",
      "1 40\n",
      "torch.Size([207])\n",
      "3 69\n",
      "torch.Size([84])\n",
      "3 28\n",
      "torch.Size([48])\n",
      "2 24\n",
      "torch.Size([18])\n",
      "1 18\n",
      "torch.Size([14])\n",
      "1 14\n",
      "torch.Size([87])\n",
      "3 29\n",
      "torch.Size([72])\n",
      "3 24\n",
      "torch.Size([135])\n",
      "3 45\n",
      "torch.Size([110])\n",
      "2 55\n",
      "torch.Size([96])\n",
      "3 32\n",
      "torch.Size([108])\n",
      "3 36\n",
      "torch.Size([48])\n",
      "2 24\n",
      "torch.Size([23])\n",
      "1 23\n",
      "torch.Size([117])\n",
      "3 39\n",
      "torch.Size([34])\n",
      "2 17\n",
      "torch.Size([64])\n",
      "2 32\n",
      "torch.Size([18])\n",
      "1 18\n",
      "torch.Size([78])\n",
      "2 39\n",
      "torch.Size([106])\n",
      "2 53\n",
      "torch.Size([70])\n",
      "2 35\n",
      "torch.Size([48])\n",
      "2 24\n",
      "torch.Size([60])\n",
      "2 30\n",
      "torch.Size([240])\n",
      "4 60\n",
      "torch.Size([468])\n",
      "9 52\n",
      "torch.Size([138])\n",
      "3 46\n",
      "torch.Size([352])\n",
      "8 44\n",
      "torch.Size([228])\n",
      "4 57\n",
      "torch.Size([132])\n",
      "4 33\n",
      "torch.Size([108])\n",
      "3 36\n",
      "torch.Size([58])\n",
      "2 29\n",
      "torch.Size([160])\n",
      "4 40\n",
      "torch.Size([114])\n",
      "2 57\n",
      "torch.Size([58])\n",
      "2 29\n",
      "torch.Size([87])\n",
      "3 29\n",
      "torch.Size([50])\n",
      "2 25\n",
      "torch.Size([52])\n",
      "1 52\n",
      "torch.Size([40])\n",
      "2 20\n",
      "torch.Size([108])\n",
      "4 27\n",
      "torch.Size([68])\n",
      "2 34\n",
      "torch.Size([53])\n",
      "1 53\n",
      "torch.Size([56])\n",
      "2 28\n",
      "torch.Size([27])\n",
      "1 27\n",
      "torch.Size([72])\n",
      "2 36\n",
      "torch.Size([16])\n",
      "1 16\n",
      "torch.Size([30])\n",
      "2 15\n",
      "torch.Size([104])\n",
      "2 52\n",
      "torch.Size([25])\n",
      "1 25\n",
      "torch.Size([46])\n",
      "2 23\n",
      "torch.Size([26])\n",
      "2 13\n",
      "torch.Size([81])\n",
      "3 27\n",
      "torch.Size([42])\n",
      "2 21\n",
      "torch.Size([16])\n",
      "1 16\n",
      "torch.Size([264])\n",
      "6 44\n",
      "torch.Size([84])\n",
      "2 42\n",
      "torch.Size([26])\n",
      "1 26\n",
      "torch.Size([64])\n",
      "2 32\n",
      "torch.Size([324])\n",
      "6 54\n",
      "torch.Size([64])\n",
      "2 32\n",
      "torch.Size([30])\n",
      "2 15\n",
      "torch.Size([180])\n",
      "5 36\n",
      "torch.Size([135])\n",
      "3 45\n",
      "torch.Size([141])\n",
      "3 47\n",
      "torch.Size([60])\n",
      "3 20\n",
      "torch.Size([245])\n",
      "5 49\n",
      "torch.Size([190])\n",
      "5 38\n",
      "torch.Size([111])\n",
      "3 37\n",
      "torch.Size([22])\n",
      "1 22\n",
      "torch.Size([60])\n",
      "2 30\n",
      "torch.Size([130])\n",
      "2 65\n",
      "torch.Size([78])\n",
      "3 26\n",
      "torch.Size([50])\n",
      "2 25\n",
      "torch.Size([56])\n",
      "1 56\n",
      "torch.Size([99])\n",
      "3 33\n",
      "torch.Size([130])\n",
      "5 26\n",
      "torch.Size([23])\n",
      "1 23\n",
      "torch.Size([164])\n",
      "4 41\n",
      "torch.Size([46])\n",
      "2 23\n",
      "torch.Size([57])\n",
      "3 19\n",
      "torch.Size([88])\n",
      "4 22\n",
      "torch.Size([56])\n",
      "2 28\n",
      "torch.Size([240])\n",
      "3 80\n",
      "torch.Size([260])\n",
      "4 65\n",
      "torch.Size([44])\n",
      "2 22\n",
      "torch.Size([17])\n",
      "1 17\n",
      "torch.Size([62])\n",
      "2 31\n",
      "torch.Size([22])\n",
      "1 22\n",
      "torch.Size([90])\n",
      "2 45\n",
      "torch.Size([17])\n",
      "1 17\n",
      "torch.Size([34])\n",
      "2 17\n",
      "torch.Size([106])\n",
      "2 53\n",
      "torch.Size([18])\n",
      "1 18\n",
      "torch.Size([42])\n",
      "2 21\n",
      "torch.Size([90])\n",
      "3 30\n",
      "torch.Size([88])\n",
      "2 44\n",
      "torch.Size([26])\n",
      "1 26\n",
      "torch.Size([84])\n",
      "2 42\n",
      "torch.Size([54])\n",
      "2 27\n",
      "torch.Size([19])\n",
      "1 19\n",
      "torch.Size([162])\n",
      "6 27\n",
      "torch.Size([441])\n",
      "7 63\n",
      "torch.Size([26])\n",
      "2 13\n",
      "torch.Size([84])\n",
      "2 42\n",
      "torch.Size([15])\n",
      "1 15\n",
      "torch.Size([54])\n",
      "2 27\n",
      "torch.Size([222])\n",
      "3 74\n",
      "torch.Size([68])\n",
      "2 34\n",
      "torch.Size([56])\n",
      "2 28\n",
      "torch.Size([44])\n",
      "2 22\n",
      "torch.Size([16])\n",
      "1 16\n",
      "torch.Size([21])\n",
      "1 21\n",
      "torch.Size([13])\n",
      "1 13\n",
      "torch.Size([74])\n",
      "2 37\n",
      "torch.Size([42])\n",
      "2 21\n",
      "torch.Size([46])\n",
      "2 23\n",
      "torch.Size([112])\n",
      "2 56\n",
      "torch.Size([26])\n",
      "1 26\n",
      "torch.Size([82])\n",
      "2 41\n",
      "torch.Size([141])\n",
      "3 47\n",
      "torch.Size([88])\n",
      "2 44\n",
      "torch.Size([18])\n",
      "1 18\n",
      "torch.Size([99])\n",
      "3 33\n",
      "torch.Size([25])\n",
      "1 25\n",
      "torch.Size([87])\n",
      "3 29\n",
      "torch.Size([43])\n",
      "1 43\n",
      "torch.Size([38])\n",
      "1 38\n",
      "torch.Size([190])\n",
      "5 38\n",
      "torch.Size([62])\n",
      "2 31\n",
      "torch.Size([32])\n",
      "2 16\n",
      "torch.Size([28])\n",
      "1 28\n",
      "torch.Size([14])\n",
      "1 14\n",
      "torch.Size([128])\n",
      "2 64\n",
      "torch.Size([156])\n",
      "3 52\n",
      "torch.Size([210])\n",
      "3 70\n",
      "torch.Size([344])\n",
      "4 86\n",
      "torch.Size([15])\n",
      "1 15\n",
      "torch.Size([81])\n",
      "3 27\n",
      "torch.Size([190])\n",
      "5 38\n",
      "torch.Size([123])\n",
      "3 41\n",
      "torch.Size([99])\n",
      "3 33\n",
      "torch.Size([16])\n",
      "1 16\n",
      "torch.Size([20])\n",
      "1 20\n",
      "torch.Size([44])\n",
      "2 22\n",
      "torch.Size([42])\n",
      "2 21\n",
      "torch.Size([21])\n",
      "1 21\n",
      "torch.Size([160])\n",
      "4 40\n",
      "torch.Size([123])\n",
      "3 41\n",
      "torch.Size([120])\n",
      "3 40\n",
      "torch.Size([66])\n",
      "3 22\n",
      "torch.Size([14])\n",
      "1 14\n",
      "torch.Size([435])\n",
      "5 87\n",
      "torch.Size([64])\n",
      "2 32\n",
      "torch.Size([68])\n",
      "2 34\n",
      "torch.Size([260])\n",
      "5 52\n",
      "torch.Size([42])\n",
      "2 21\n",
      "torch.Size([212])\n",
      "4 53\n",
      "torch.Size([62])\n",
      "2 31\n",
      "torch.Size([46])\n",
      "2 23\n",
      "torch.Size([52])\n",
      "2 26\n",
      "torch.Size([504])\n",
      "7 72\n",
      "torch.Size([126])\n",
      "3 42\n",
      "torch.Size([20])\n",
      "1 20\n",
      "torch.Size([216])\n",
      "4 54\n",
      "torch.Size([93])\n",
      "3 31\n",
      "torch.Size([165])\n",
      "5 33\n",
      "torch.Size([52])\n",
      "2 26\n",
      "torch.Size([168])\n",
      "4 42\n",
      "torch.Size([42])\n",
      "2 21\n",
      "torch.Size([108])\n",
      "4 27\n",
      "torch.Size([125])\n",
      "5 25\n",
      "torch.Size([70])\n",
      "2 35\n",
      "torch.Size([26])\n",
      "2 13\n",
      "torch.Size([114])\n",
      "3 38\n",
      "torch.Size([366])\n",
      "6 61\n",
      "torch.Size([102])\n",
      "2 51\n",
      "torch.Size([54])\n",
      "3 18\n",
      "torch.Size([50])\n",
      "1 50\n",
      "torch.Size([105])\n",
      "3 35\n",
      "torch.Size([300])\n",
      "6 50\n",
      "torch.Size([41])\n",
      "1 41\n",
      "torch.Size([42])\n",
      "2 21\n",
      "torch.Size([129])\n",
      "3 43\n",
      "torch.Size([156])\n",
      "4 39\n",
      "torch.Size([82])\n",
      "2 41\n",
      "torch.Size([38])\n",
      "2 19\n",
      "torch.Size([41])\n",
      "1 41\n",
      "torch.Size([30])\n",
      "2 15\n",
      "torch.Size([81])\n",
      "3 27\n",
      "torch.Size([105])\n",
      "3 35\n",
      "torch.Size([460])\n",
      "5 92\n",
      "torch.Size([60])\n",
      "3 20\n",
      "torch.Size([70])\n",
      "2 35\n",
      "torch.Size([108])\n",
      "3 36\n",
      "torch.Size([240])\n",
      "6 40\n",
      "torch.Size([26])\n",
      "2 13\n",
      "torch.Size([34])\n",
      "1 34\n",
      "torch.Size([48])\n",
      "3 16\n",
      "torch.Size([44])\n",
      "2 22\n",
      "torch.Size([136])\n",
      "2 68\n",
      "torch.Size([32])\n",
      "2 16\n",
      "torch.Size([20])\n",
      "1 20\n",
      "torch.Size([22])\n",
      "1 22\n",
      "torch.Size([216])\n",
      "3 72\n",
      "torch.Size([28])\n",
      "1 28\n",
      "torch.Size([69])\n",
      "3 23\n",
      "torch.Size([336])\n",
      "7 48\n",
      "torch.Size([255])\n",
      "5 51\n",
      "torch.Size([40])\n",
      "2 20\n",
      "torch.Size([136])\n",
      "4 34\n",
      "torch.Size([180])\n",
      "5 36\n",
      "torch.Size([48])\n",
      "2 24\n",
      "torch.Size([70])\n",
      "2 35\n",
      "torch.Size([68])\n",
      "2 34\n",
      "torch.Size([54])\n",
      "2 27\n",
      "torch.Size([22])\n",
      "1 22\n",
      "torch.Size([90])\n",
      "3 30\n",
      "torch.Size([34])\n",
      "2 17\n",
      "torch.Size([16])\n",
      "1 16\n",
      "torch.Size([25])\n",
      "1 25\n",
      "torch.Size([144])\n",
      "3 48\n",
      "torch.Size([29])\n",
      "1 29\n",
      "torch.Size([132])\n",
      "4 33\n",
      "torch.Size([78])\n",
      "3 26\n",
      "torch.Size([57])\n",
      "3 19\n",
      "torch.Size([60])\n",
      "2 30\n",
      "torch.Size([70])\n",
      "2 35\n",
      "torch.Size([66])\n",
      "2 33\n",
      "torch.Size([160])\n",
      "5 32\n",
      "torch.Size([228])\n",
      "6 38\n",
      "torch.Size([160])\n",
      "4 40\n",
      "torch.Size([305])\n",
      "5 61\n",
      "torch.Size([200])\n",
      "5 40\n",
      "torch.Size([144])\n",
      "3 48\n",
      "torch.Size([96])\n",
      "3 32\n",
      "torch.Size([72])\n",
      "3 24\n",
      "torch.Size([213])\n",
      "3 71\n",
      "torch.Size([116])\n",
      "4 29\n",
      "torch.Size([140])\n",
      "4 35\n",
      "torch.Size([33])\n",
      "1 33\n",
      "torch.Size([66])\n",
      "3 22\n",
      "torch.Size([172])\n",
      "4 43\n",
      "torch.Size([36])\n",
      "2 18\n",
      "torch.Size([92])\n",
      "2 46\n",
      "torch.Size([108])\n",
      "3 36\n",
      "torch.Size([72])\n",
      "4 18\n",
      "torch.Size([30])\n",
      "2 15\n",
      "torch.Size([175])\n",
      "5 35\n",
      "torch.Size([120])\n",
      "5 24\n",
      "torch.Size([156])\n",
      "4 39\n",
      "torch.Size([58])\n",
      "2 29\n",
      "torch.Size([159])\n",
      "3 53\n",
      "torch.Size([141])\n",
      "3 47\n",
      "torch.Size([87])\n",
      "3 29\n",
      "torch.Size([34])\n",
      "1 34\n",
      "torch.Size([42])\n",
      "1 42\n",
      "torch.Size([183])\n",
      "3 61\n",
      "torch.Size([651])\n",
      "7 93\n",
      "torch.Size([31])\n",
      "1 31\n",
      "torch.Size([74])\n",
      "2 37\n",
      "torch.Size([15])\n",
      "1 15\n",
      "torch.Size([104])\n",
      "4 26\n",
      "torch.Size([39])\n",
      "1 39\n",
      "torch.Size([15])\n",
      "1 15\n",
      "torch.Size([30])\n",
      "1 30\n",
      "torch.Size([13])\n",
      "1 13\n",
      "torch.Size([75])\n",
      "3 25\n",
      "torch.Size([54])\n",
      "2 27\n",
      "torch.Size([13])\n",
      "1 13\n",
      "torch.Size([100])\n",
      "4 25\n",
      "torch.Size([14])\n",
      "1 14\n",
      "torch.Size([18])\n",
      "1 18\n",
      "torch.Size([16])\n",
      "1 16\n",
      "torch.Size([16])\n",
      "1 16\n",
      "torch.Size([36])\n",
      "2 18\n",
      "torch.Size([68])\n",
      "2 34\n",
      "torch.Size([15])\n",
      "1 15\n",
      "torch.Size([102])\n",
      "3 34\n",
      "torch.Size([60])\n",
      "2 30\n",
      "torch.Size([99])\n",
      "3 33\n",
      "torch.Size([20])\n",
      "1 20\n",
      "torch.Size([17])\n",
      "1 17\n",
      "torch.Size([14])\n",
      "1 14\n",
      "torch.Size([44])\n",
      "2 22\n",
      "torch.Size([192])\n",
      "4 48\n",
      "torch.Size([16])\n",
      "1 16\n",
      "torch.Size([18])\n",
      "1 18\n",
      "torch.Size([15])\n",
      "1 15\n",
      "torch.Size([23])\n",
      "1 23\n",
      "torch.Size([78])\n",
      "2 39\n",
      "torch.Size([50])\n",
      "2 25\n",
      "torch.Size([34])\n",
      "1 34\n",
      "torch.Size([19])\n",
      "1 19\n",
      "torch.Size([60])\n",
      "2 30\n",
      "torch.Size([58])\n",
      "2 29\n",
      "torch.Size([102])\n",
      "3 34\n",
      "torch.Size([15])\n",
      "1 15\n",
      "torch.Size([84])\n",
      "3 28\n",
      "torch.Size([14])\n",
      "1 14\n",
      "torch.Size([46])\n",
      "2 23\n",
      "torch.Size([34])\n",
      "2 17\n",
      "torch.Size([165])\n",
      "3 55\n",
      "torch.Size([46])\n",
      "2 23\n",
      "torch.Size([14])\n",
      "1 14\n",
      "torch.Size([18])\n",
      "1 18\n",
      "torch.Size([56])\n",
      "2 28\n",
      "torch.Size([40])\n",
      "2 20\n",
      "torch.Size([38])\n",
      "2 19\n",
      "torch.Size([13])\n",
      "1 13\n",
      "torch.Size([44])\n",
      "2 22\n",
      "torch.Size([45])\n",
      "1 45\n",
      "torch.Size([27])\n",
      "1 27\n",
      "torch.Size([46])\n",
      "2 23\n",
      "torch.Size([28])\n",
      "1 28\n",
      "torch.Size([20])\n",
      "1 20\n",
      "torch.Size([41])\n",
      "1 41\n",
      "torch.Size([16])\n",
      "1 16\n",
      "torch.Size([40])\n",
      "1 40\n",
      "torch.Size([24])\n",
      "1 24\n",
      "torch.Size([210])\n",
      "3 70\n",
      "torch.Size([64])\n",
      "2 32\n",
      "torch.Size([66])\n",
      "1 66\n",
      "torch.Size([42])\n",
      "1 42\n",
      "torch.Size([76])\n",
      "1 76\n",
      "torch.Size([39])\n",
      "1 39\n",
      "torch.Size([40])\n",
      "2 20\n",
      "torch.Size([52])\n",
      "1 52\n",
      "torch.Size([130])\n",
      "2 65\n",
      "torch.Size([24])\n",
      "1 24\n",
      "torch.Size([81])\n",
      "3 27\n",
      "torch.Size([21])\n",
      "1 21\n",
      "torch.Size([24])\n",
      "1 24\n",
      "torch.Size([69])\n",
      "1 69\n",
      "torch.Size([21])\n",
      "1 21\n",
      "torch.Size([23])\n",
      "1 23\n",
      "torch.Size([43])\n",
      "1 43\n",
      "torch.Size([98])\n",
      "2 49\n",
      "torch.Size([117])\n",
      "3 39\n",
      "torch.Size([18])\n",
      "1 18\n",
      "torch.Size([176])\n",
      "4 44\n",
      "torch.Size([15])\n",
      "1 15\n",
      "torch.Size([43])\n",
      "1 43\n",
      "torch.Size([28])\n",
      "1 28\n",
      "torch.Size([70])\n",
      "2 35\n",
      "torch.Size([155])\n",
      "5 31\n",
      "torch.Size([66])\n",
      "3 22\n",
      "torch.Size([15])\n",
      "1 15\n",
      "torch.Size([93])\n",
      "3 31\n",
      "torch.Size([45])\n",
      "3 15\n",
      "torch.Size([38])\n",
      "2 19\n",
      "torch.Size([40])\n",
      "1 40\n",
      "torch.Size([37])\n",
      "1 37\n",
      "torch.Size([160])\n",
      "4 40\n",
      "torch.Size([144])\n",
      "4 36\n",
      "torch.Size([80])\n",
      "4 20\n",
      "torch.Size([42])\n",
      "2 21\n",
      "torch.Size([336])\n",
      "4 84\n",
      "torch.Size([15])\n",
      "1 15\n",
      "torch.Size([270])\n",
      "3 90\n",
      "torch.Size([123])\n",
      "3 41\n",
      "torch.Size([30])\n",
      "1 30\n",
      "torch.Size([230])\n",
      "5 46\n",
      "torch.Size([16])\n",
      "1 16\n",
      "torch.Size([30])\n",
      "2 15\n",
      "torch.Size([231])\n",
      "3 77\n",
      "torch.Size([177])\n",
      "3 59\n",
      "torch.Size([43])\n",
      "1 43\n",
      "torch.Size([34])\n",
      "2 17\n",
      "torch.Size([40])\n",
      "2 20\n",
      "torch.Size([16])\n",
      "1 16\n",
      "torch.Size([13])\n",
      "1 13\n",
      "torch.Size([73])\n",
      "1 73\n",
      "torch.Size([136])\n",
      "4 34\n",
      "torch.Size([170])\n",
      "5 34\n",
      "torch.Size([45])\n",
      "3 15\n",
      "torch.Size([94])\n",
      "2 47\n",
      "torch.Size([19])\n",
      "1 19\n",
      "torch.Size([26])\n",
      "1 26\n",
      "torch.Size([360])\n",
      "5 72\n",
      "torch.Size([38])\n",
      "2 19\n",
      "torch.Size([108])\n",
      "3 36\n",
      "torch.Size([87])\n",
      "3 29\n",
      "torch.Size([150])\n",
      "5 30\n",
      "torch.Size([54])\n",
      "2 27\n",
      "torch.Size([17])\n",
      "1 17\n",
      "torch.Size([66])\n",
      "2 33\n",
      "torch.Size([26])\n",
      "2 13\n",
      "torch.Size([46])\n",
      "2 23\n",
      "torch.Size([80])\n",
      "2 40\n",
      "torch.Size([20])\n",
      "1 20\n",
      "torch.Size([46])\n",
      "1 46\n",
      "torch.Size([40])\n",
      "2 20\n",
      "torch.Size([58])\n",
      "2 29\n",
      "torch.Size([108])\n",
      "2 54\n",
      "torch.Size([44])\n",
      "2 22\n",
      "torch.Size([98])\n",
      "2 49\n",
      "torch.Size([50])\n",
      "2 25\n",
      "torch.Size([84])\n",
      "4 21\n",
      "torch.Size([21])\n",
      "1 21\n",
      "torch.Size([171])\n",
      "3 57\n",
      "torch.Size([65])\n",
      "1 65\n",
      "torch.Size([18])\n",
      "1 18\n",
      "torch.Size([62])\n",
      "2 31\n",
      "torch.Size([30])\n",
      "2 15\n",
      "torch.Size([392])\n",
      "7 56\n",
      "torch.Size([44])\n",
      "1 44\n",
      "torch.Size([46])\n",
      "2 23\n",
      "torch.Size([132])\n",
      "2 66\n",
      "torch.Size([104])\n",
      "2 52\n",
      "torch.Size([14])\n",
      "1 14\n",
      "torch.Size([15])\n",
      "1 15\n",
      "torch.Size([45])\n",
      "3 15\n",
      "torch.Size([32])\n",
      "2 16\n",
      "torch.Size([28])\n",
      "2 14\n",
      "torch.Size([26])\n",
      "1 26\n",
      "torch.Size([32])\n",
      "2 16\n",
      "torch.Size([26])\n",
      "1 26\n",
      "torch.Size([18])\n",
      "1 18\n",
      "torch.Size([66])\n",
      "2 33\n",
      "torch.Size([18])\n",
      "1 18\n",
      "torch.Size([42])\n",
      "2 21\n",
      "torch.Size([19])\n",
      "1 19\n",
      "torch.Size([56])\n",
      "2 28\n",
      "torch.Size([90])\n",
      "2 45\n",
      "torch.Size([72])\n",
      "2 36\n",
      "torch.Size([34])\n",
      "2 17\n",
      "torch.Size([49])\n",
      "1 49\n",
      "torch.Size([16])\n",
      "1 16\n",
      "torch.Size([74])\n",
      "2 37\n",
      "torch.Size([17])\n",
      "1 17\n",
      "torch.Size([59])\n",
      "1 59\n",
      "torch.Size([104])\n",
      "2 52\n",
      "torch.Size([25])\n",
      "1 25\n",
      "torch.Size([117])\n",
      "3 39\n",
      "torch.Size([20])\n",
      "1 20\n",
      "torch.Size([105])\n",
      "3 35\n",
      "torch.Size([58])\n",
      "2 29\n",
      "torch.Size([86])\n",
      "2 43\n",
      "torch.Size([82])\n",
      "2 41\n",
      "torch.Size([80])\n",
      "2 40\n",
      "torch.Size([48])\n",
      "2 24\n",
      "torch.Size([17])\n",
      "1 17\n",
      "torch.Size([84])\n",
      "2 42\n",
      "torch.Size([23])\n",
      "1 23\n",
      "torch.Size([102])\n",
      "3 34\n",
      "torch.Size([21])\n",
      "1 21\n",
      "torch.Size([160])\n",
      "4 40\n",
      "torch.Size([108])\n",
      "2 54\n",
      "torch.Size([56])\n",
      "2 28\n",
      "torch.Size([96])\n",
      "3 32\n",
      "torch.Size([30])\n",
      "1 30\n",
      "torch.Size([38])\n",
      "2 19\n",
      "torch.Size([28])\n",
      "2 14\n",
      "torch.Size([26])\n",
      "2 13\n",
      "torch.Size([38])\n",
      "1 38\n",
      "torch.Size([13])\n",
      "1 13\n",
      "torch.Size([18])\n",
      "1 18\n",
      "torch.Size([27])\n",
      "1 27\n",
      "torch.Size([18])\n",
      "1 18\n",
      "torch.Size([48])\n",
      "1 48\n",
      "torch.Size([46])\n",
      "2 23\n",
      "torch.Size([90])\n",
      "2 45\n",
      "torch.Size([84])\n",
      "2 42\n",
      "torch.Size([40])\n",
      "2 20\n",
      "torch.Size([28])\n",
      "1 28\n",
      "torch.Size([112])\n",
      "4 28\n",
      "torch.Size([212])\n",
      "4 53\n",
      "torch.Size([50])\n",
      "2 25\n",
      "torch.Size([120])\n",
      "4 30\n",
      "torch.Size([60])\n",
      "2 30\n",
      "torch.Size([96])\n",
      "3 32\n",
      "torch.Size([156])\n",
      "3 52\n",
      "torch.Size([66])\n",
      "3 22\n",
      "torch.Size([56])\n",
      "2 28\n",
      "torch.Size([69])\n",
      "3 23\n",
      "torch.Size([150])\n",
      "3 50\n",
      "torch.Size([104])\n",
      "4 26\n",
      "torch.Size([21])\n",
      "1 21\n",
      "torch.Size([82])\n",
      "2 41\n",
      "torch.Size([42])\n",
      "1 42\n",
      "torch.Size([324])\n",
      "4 81\n",
      "torch.Size([68])\n",
      "2 34\n",
      "torch.Size([16])\n",
      "1 16\n",
      "torch.Size([19])\n",
      "1 19\n",
      "torch.Size([33])\n",
      "1 33\n",
      "torch.Size([258])\n",
      "3 86\n",
      "torch.Size([32])\n",
      "2 16\n",
      "torch.Size([24])\n",
      "1 24\n",
      "torch.Size([66])\n",
      "2 33\n",
      "torch.Size([29])\n",
      "1 29\n",
      "torch.Size([172])\n",
      "4 43\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 75\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m#-----------------BERT EMBEDDING---------------------------\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 75\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_bert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     output_hidden_states_sum \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(output\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m:], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     77\u001b[0m     b,n,h \u001b[38;5;241m=\u001b[39m output_hidden_states_sum\u001b[38;5;241m.\u001b[39msize()\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1131'>1132</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1014\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1004'>1005</a>\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1006'>1007</a>\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1007'>1008</a>\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1008'>1009</a>\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1011'>1012</a>\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1012'>1013</a>\u001b[0m )\n\u001b[0;32m-> <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1013'>1014</a>\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1014'>1015</a>\u001b[0m     embedding_output,\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1015'>1016</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1016'>1017</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1017'>1018</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1018'>1019</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1019'>1020</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1020'>1021</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1021'>1022</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1022'>1023</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1023'>1024</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1024'>1025</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1025'>1026</a>\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1026'>1027</a>\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1131'>1132</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:603\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=593'>594</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=594'>595</a>\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=595'>596</a>\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=599'>600</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=600'>601</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=601'>602</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=602'>603</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=603'>604</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=604'>605</a>\u001b[0m         attention_mask,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=605'>606</a>\u001b[0m         layer_head_mask,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=606'>607</a>\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=607'>608</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=608'>609</a>\u001b[0m         past_key_value,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=609'>610</a>\u001b[0m         output_attentions,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=610'>611</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=612'>613</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=613'>614</a>\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1131'>1132</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:531\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=527'>528</a>\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=528'>529</a>\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=530'>531</a>\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=531'>532</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, attention_output\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=532'>533</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=533'>534</a>\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=535'>536</a>\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/pytorch_utils.py:246\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/pytorch_utils.py?line=242'>243</a>\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/pytorch_utils.py?line=243'>244</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[0;32m--> <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/pytorch_utils.py?line=245'>246</a>\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:543\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=541'>542</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[0;32m--> <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=542'>543</a>\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintermediate(attention_output)\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=543'>544</a>\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=544'>545</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1131'>1132</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:443\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=441'>442</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=442'>443</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(hidden_states)\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=443'>444</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=444'>445</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1131'>1132</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=112'>113</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=113'>114</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "currentDateAndTime = datetime.now()\n",
    "_id = str(currentDateAndTime.year)+\"_\"+str(currentDateAndTime.month)+\"_\"+str(currentDateAndTime.day)+\"_\"+str(currentDateAndTime.hour)+\"_\"+str(currentDateAndTime.minute)+\"_\"+str(currentDateAndTime.second)\n",
    "_id =  _id +\"Language constained training\"\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 0.00000005)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "\n",
    "logSotfMax = torch.nn.LogSoftmax(dim=1)\n",
    "nll_loss = torch.nn.NLLLoss()\n",
    "\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "dataloader_dev = DataLoader(dev_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "mapping = dataloader_train.dataset.args_roles\n",
    "\n",
    "auto_model.eval()\n",
    "\n",
    "EPOCHS = 1\n",
    "patience_counter = 0\n",
    "patience = 5\n",
    "max_val_loss = 9999\n",
    "f1_score_max = 0\n",
    "output_path = \"saved\"\n",
    "model_name = \"model_\"+_id+\".pth\"\n",
    "PATH = os.path.join(output_path,model_name)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    #TRAINING\n",
    "    p = []\n",
    "    g = []\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_train):\n",
    "        #print(sample_batched)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "       \n",
    "        #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "        #------------------FILTERING SUB-WORDS----------------------\n",
    "        subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "        word_emebedding = []\n",
    "        for i in range(n):\n",
    "            subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "            flag = subtoken_mask[0,i,0]\n",
    "            if flag :\n",
    "                continue\n",
    "            else :\n",
    "                word_emebedding.append(subwords_embedding)\n",
    "        word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "        #-------------------------FORWARD/BACKWARD----------------------------------\n",
    "        x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "            perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "            predicate_index = sample_batched[\"predicate_index\"],\n",
    "            pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "            predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])        \n",
    "        b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "        loss = nll_loss(logSotfMax(x),gt)\n",
    "        total_loss = total_loss + loss\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "        counter += 1 \n",
    "            \n",
    "\n",
    "        #-------------------------RESULT STORING----------------------------------\n",
    "        predicted = torch.argmax(x, dim=1).cpu()\n",
    "        b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "        print(predicted.size())\n",
    "        print(b,n)\n",
    "\n",
    "        p += predicted.tolist()\n",
    "        g += gt.tolist()\n",
    "    \n",
    "\n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    print(\"Epochs n.\", epoch)\n",
    "    print(\"F1 train:\",f1_score(g, p, average=None))\n",
    "    scheduler.step()\n",
    "    avg_train_loss = total_loss/counter\n",
    "    writer.add_scalar(\"Loss/train\", avg_train_loss, epoch)\n",
    "\n",
    "\n",
    "\n",
    "    #EVALUATION\n",
    "    p = []\n",
    "    g = []\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_dev):\n",
    "    \n",
    "      #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "            #------------------FILTERING SUB-WORDS----------------------\n",
    "            subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "            word_emebedding = []\n",
    "            for i in range(n): \n",
    "                subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "                flag = subtoken_mask[0,i,0]\n",
    "                if flag :\n",
    "                    continue\n",
    "                else :\n",
    "                    word_emebedding.append(subwords_embedding)\n",
    "            word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "            #-------------------------FORWARD----------------------------------\n",
    "            x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "                        perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "                        predicate_index = sample_batched[\"predicate_index\"],\n",
    "                        pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "                        predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])   \n",
    "\n",
    "\n",
    "            b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "            loss = nll_loss(logSotfMax(x),gt)\n",
    "            total_loss = total_loss + loss\n",
    "            #-------------------------RESULT STORING----------------------------------\n",
    "            predicted = torch.argmax(x, dim=1)\n",
    "            p += predicted.tolist()\n",
    "            g += gt.tolist()\n",
    "            counter += 1 \n",
    "    \n",
    "    #-------------------------RESULTS----------------------------------\n",
    "\n",
    "    avg_eval_loss = total_loss/counter\n",
    "\n",
    "    if avg_eval_loss < max_val_loss:\n",
    "        max_val_loss = avg_eval_loss\n",
    "    else :\n",
    "        patience_counter += 1\n",
    "    \n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    if patience_counter >= patience :  \n",
    "\n",
    "\n",
    "        print(\"Early stopping at epoch : \",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 eval :\",f1_avg)\n",
    "        break\n",
    "    else :\n",
    "        print(\"EPOCHS :\",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 eval :\",f1_avg)\n",
    "    \n",
    "\n",
    "    writer.add_scalar(\"Loss/validation\", avg_eval_loss, epoch)\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"Eval_EN/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"Eval_EN/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if f1_avg > f1_score_max:\n",
    "        f1_score_max = f1_avg\n",
    "        torch.save(model.state_dict(),PATH)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spanish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Spanish Dataset\n",
    "bert-base-multilingual-cased\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|| 625/625 [00:00<00:00, 137kB/s]\n",
      "Downloading: 100%|| 714M/714M [00:09<00:00, 73.2MB/s] \n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model class is      : <class 'transformers.models.bert.modeling_bert.BertModel'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|| 29.0/29.0 [00:00<00:00, 8.39kB/s]\n",
      "Downloading: 100%|| 996k/996k [00:01<00:00, 810kB/s] \n",
      "Downloading: 100%|| 1.96M/1.96M [00:01<00:00, 1.00MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model class is      : <class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import random\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "\n",
    "auto_model = AutoModel.from_pretrained(\"bert-base-multilingual-cased\",output_hidden_states=True)\n",
    "print(f\"\\nmodel class is      : {type(auto_model)}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "print(f\"\\nmodel class is      : {type(tokenizer)}\")\n",
    "\n",
    "\n",
    "\n",
    "class SRL(Dataset):\n",
    " \n",
    "    def __init__(self,language,path,args_roles = None,pos_list = None) -> None:\n",
    "\n",
    "        self.path_root = 'data'\n",
    "        self.load_data(language,path)\n",
    "        if args_roles is None :\n",
    "            self.args_roles,self.list_broken_id = self.list_arg_roles()\n",
    "        else : \n",
    "            self.args_roles = args_roles\n",
    "            _,self.list_broken_id = self.list_arg_roles()\n",
    "        \n",
    "\n",
    "        if pos_list is None :\n",
    "            self.pos_list,_ = self.list_pos()\n",
    "            self.pos_list.append(\"Nothing\")\n",
    "        else : \n",
    "            self.pos_list = pos_list\n",
    "        \n",
    "\n",
    "\n",
    "        self.predicate_dis,_ = self.list_predicate_roles()\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(self.pos_list)\n",
    "\n",
    "        self.predicate_dis.append(\"Nothing\")\n",
    "\n",
    "    def load_data(self,language,mode):\n",
    "        \n",
    "        mode = mode+\".json\"\n",
    "        path = os.path.join(self.path_root,language,mode)\n",
    "        data_file = open(path)\n",
    "       \n",
    "        data_ = json.load(data_file)\n",
    "\n",
    "        list_data = []\n",
    "\n",
    "        for data in data_:\n",
    "            list_data.append(data_[data])\n",
    "        \n",
    "\n",
    "        self.data = list_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, id : int):\n",
    "\n",
    "        flag = False\n",
    "        if id in self.list_broken_id :\n",
    "            flag = True\n",
    "            while flag == True:\n",
    "\n",
    "                rand_id = random.randint(0, len(self.data)-1)\n",
    "                \n",
    "                if rand_id in self.list_broken_id :\n",
    "                    pass\n",
    "                else :\n",
    "                    flag = False\n",
    "                    id = rand_id        \n",
    "\n",
    "\n",
    "        data = self.pre_processing(self.data[id])\n",
    "        data = self.processig(data)\n",
    "        return data\n",
    "        \n",
    "    def pre_processing(self, data:dict):\n",
    "        data_list = []\n",
    "        for role in data[\"roles\"]:\n",
    "            dictionary = dict()\n",
    "            dictionary[\"words\"] = data[\"words\"]\n",
    "            dictionary[\"role\"] = data[\"roles\"][role]\n",
    "            dictionary[\"pre_idx\"] = role\n",
    "            dictionary[\"pos_tags\"] = data[\"pos_tags\"]\n",
    "            dictionary[\"predicate_meaning\"] = data[\"predicates\"]\n",
    "            data_list.append(dictionary)    \n",
    "        return data_list\n",
    "    \n",
    "    def processig(self,data_list:list):\n",
    "        \n",
    "        for dictionary in data_list:\n",
    "\n",
    "            #dictionary[\"words\"] = data[\"words\"]\n",
    "            dictionary[\"gt_arg_identification\"] = self.arg_id(dictionary[\"role\"])\n",
    "            dictionary[\"gt_arg_classification\"] = self.arg_class(dictionary[\"role\"])\n",
    "            dictionary[\"pos_idx\"] = self.pos_idx(dictionary[\"pos_tags\"])\n",
    "            dictionary[\"predicate_meaning_idx\"] = self.predicate_meaning_idx(dictionary[\"predicate_meaning\"])\n",
    "        \n",
    "        return data_list\n",
    "   \n",
    "    def list_arg_roles(self):\n",
    "        list_roles = []\n",
    "        list_broken_id = []\n",
    "        for i,element in enumerate(self.data):\n",
    "            flag = True\n",
    "            try : roles = element[\"roles\"]\n",
    "            except : flag = False\n",
    "            if flag :\n",
    "                for e in roles:\n",
    "                    sentence = element[\"roles\"][e]\n",
    "\n",
    "                    for word in sentence:\n",
    "                        \n",
    "                        list_roles.append(word)\n",
    "                list_roles = list(set(list_roles))\n",
    "            else : \n",
    "                list_broken_id.append(i)\n",
    "        return list_roles,list_broken_id\n",
    "\n",
    "    def list_predicate_roles(self):\n",
    "        list_predicate_roles = []\n",
    "        list_broken_id = []\n",
    "        for i,element in enumerate(self.data):\n",
    "            flag = True\n",
    "            try : predicates = element[\"predicates\"]\n",
    "            except : flag = False\n",
    "            if flag :\n",
    "                for pre in predicates:\n",
    "                    list_predicate_roles.append(pre)\n",
    "                list_predicate_roles = list(set(list_predicate_roles))\n",
    "            else : \n",
    "                list_broken_id.append(i)\n",
    "        return list_predicate_roles,list_broken_id\n",
    "\n",
    "    def list_pos(self):\n",
    "        list_pos = []\n",
    "        list_broken_id = []\n",
    "        for i,element in enumerate(self.data):\n",
    "            flag = True\n",
    "            try : pos = element[\"pos_tags\"]\n",
    "            except : flag = False\n",
    "            if flag :\n",
    "                for e in pos:\n",
    "                    list_pos.append(e)\n",
    "                list_pos = list(set(list_pos))\n",
    "            else : \n",
    "                list_broken_id.append(i)\n",
    "        return list_pos,list_broken_id\n",
    "  \n",
    "    def arg_class(self,role:list):\n",
    "        list_idxs = []\n",
    "        for element in role:\n",
    "            list_idxs.append(self.args_roles.index(element))\n",
    "        \n",
    "\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "\n",
    "    def arg_id(self,role:dict):\n",
    "        list_idxs = []\n",
    "        for element in role:\n",
    "            if element == \"_\":\n",
    "                list_idxs.append(0)\n",
    "            else :\n",
    "                list_idxs.append(1)\n",
    "\n",
    "        \n",
    "\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "\n",
    "    def pos_idx(self,pos_tags:dict):\n",
    "        list_idxs = []\n",
    "        list_idxs.append(self.pos_list.index(\"Nothing\"))\n",
    "\n",
    "        for element in pos_tags:\n",
    "            list_idxs.append(self.pos_list.index(element))\n",
    "        \n",
    "        list_idxs.append(self.pos_list.index(\"Nothing\"))\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "    \n",
    "    def predicate_meaning_idx(self,predicate_meaning_tags:dict):\n",
    "        list_idxs = []\n",
    "        list_idxs.append(self.predicate_dis.index(\"Nothing\"))\n",
    "\n",
    "        for element in predicate_meaning_tags:\n",
    "            list_idxs.append(self.predicate_dis.index(element))\n",
    "        \n",
    "        list_idxs.append(self.predicate_dis.index(\"Nothing\"))\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64) \n",
    "\n",
    "    \n",
    "# here we define our collate function\n",
    "def collate_fn(batch) -> Dict[str, torch.Tensor]:\n",
    "    #print(batch)\n",
    "    input = dict() \n",
    "    batch_sentence = [] \n",
    "    #print(len(batch))\n",
    "    for period in batch:\n",
    "        for sentence in period :\n",
    "        \n",
    "            #print(len(sentence[0][\"words\"]))\n",
    "            pre_idx = int(sentence[\"pre_idx\"])\n",
    "            \n",
    "\n",
    "            predicate = sentence[\"words\"][pre_idx]\n",
    "\n",
    "            text = \" \".join(sentence[\"words\"])\n",
    "            tokens: list[str] = text.split()\n",
    "            predicate: list[str] = predicate.split()\n",
    "\n",
    "            #text = sentence[0][\"words\"]\n",
    "            \n",
    "            t = (tokens,predicate)\n",
    "\n",
    "            batch_sentence.append(t)\n",
    "            #print(batch_sentence)\n",
    "\n",
    "    batch_output = tokenizer.batch_encode_plus(batch_sentence,padding=True,is_split_into_words=True, truncation=True,return_offsets_mapping=True, return_tensors=\"pt\")\n",
    "    #print(batch_output.keys())\n",
    "\n",
    "\n",
    "    gt = dict()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    for period in batch:\n",
    "\n",
    "        list_positional_predicate_encoding = []\n",
    "        list_arg_gt = []\n",
    "        list_predicate_index = [] \n",
    "        list_pos_index = [] \n",
    "        list_predicate_meaning_index = []\n",
    "\n",
    "        for sentence in period:\n",
    "            #positional_encoding\n",
    "            #+2 per il CLS iniziale ad SEP finale\n",
    "            sentence_words_lenght =  len(sentence[\"words\"])\n",
    "            positional_predicate_encoding = torch.zeros(1,sentence_words_lenght+2)\n",
    "            #+1 per il CLS iniziale\n",
    "            pre_idx = int(sentence[\"pre_idx\"])\n",
    "            positional_predicate_encoding[:,pre_idx+1] = 1\n",
    "            list_positional_predicate_encoding.append(positional_predicate_encoding)\n",
    "            #print(\"positional_prefix_encoding\",positional_predicate_encoding)\n",
    "            list_predicate_index.append(pre_idx)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            pos = torch.unsqueeze(sentence[\"pos_idx\"],dim = 0)\n",
    "            list_pos_index.append(pos)\n",
    "            predicate_meaning_idxs = torch.unsqueeze(sentence[\"predicate_meaning_idx\"],dim = 0)\n",
    "            list_predicate_meaning_index.append(predicate_meaning_idxs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #note CLS and SEP are discharder after Bi-LSTM, the Classifier takes in input only wokrds hidden state embedding\n",
    "            arg_gt = torch.unsqueeze(sentence[\"gt_arg_classification\"],dim = 0)\n",
    "            list_arg_gt.append(arg_gt)\n",
    "        \n",
    "\n",
    "    list_arg_gt = torch.cat(list_arg_gt,dim = 0)\n",
    "    list_pos_index = torch.cat(list_pos_index,dim = 0)\n",
    "    list_predicate_meaning_index = torch.cat(list_predicate_meaning_index,dim = 0)\n",
    "    list_positional_predicate_encoding = torch.cat(list_positional_predicate_encoding,dim = 0)\n",
    "    gt[\"arg_gt\"] = list_arg_gt\n",
    "    input[\"predicate_index\"] = list_predicate_index\n",
    "    input[\"pos_index\"] = list_pos_index.long()\n",
    "    input[\"predicate_meaning_idx\"] = list_predicate_meaning_index.long()\n",
    "    offset = batch_output.pop(\"offset_mapping\")\n",
    "    input[\"BERT_input\"] = batch_output\n",
    "    input[\"positional_encoding\"] = list_positional_predicate_encoding.long()\n",
    "    input[\"offset_mapping\"] = offset\n",
    "    input[\"gt\"] = gt\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    return input\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SCONJ', 'ADV', 'PUNCT', 'NOUN', 'DET', 'NUM', 'PART', 'ADP', 'CCONJ', 'SYM', 'INTJ', 'PRON', 'X', 'ADJ', 'AUX', 'PROPN', 'VERB', 'Nothing']\n",
      "['SCONJ', 'ADV', 'PUNCT', 'NOUN', 'DET', 'NUM', 'PART', 'ADP', 'CCONJ', 'SYM', 'INTJ', 'PRON', 'X', 'ADJ', 'AUX', 'PROPN', 'VERB', 'Nothing']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#train_dataset = SRL(\"EN\",\"train\")\n",
    "\n",
    "#note here we are directly loading args_roles mapping as computed before the the dasetet where we have perfomerd \n",
    "#EN and ES dataset should have the same consistency in generating \n",
    "train_dataset = SRL(\"ES\",\"train\",train_dataset.args_roles,train_dataset.pos_list)\n",
    "#same mapping should be used in both the dataset\n",
    "dev_dataset = SRL(\"ES\",\"dev\",train_dataset.args_roles,train_dataset.pos_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English-Spanish attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Arg_Classifier(\n",
       "  (bi_lstm_portable): LSTM(132, 50, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (embedding_predicate_flag): Embedding(2, 32, max_norm=True)\n",
       "  (embedding_predicate): Embedding(304, False, max_norm=True)\n",
       "  (embedding_pos): Embedding(18, 100, max_norm=True)\n",
       "  (bi_lstm): LSTM(900, 50, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (dropout_language_constraint): Dropout(p=0.6, inplace=False)\n",
       "  (dropout_in_classifier): Dropout(p=0.4, inplace=False)\n",
       "  (Relu): ReLU()\n",
       "  (Sigmoid): Sigmoid()\n",
       "  (linear0): Linear(in_features=300, out_features=675, bias=True)\n",
       "  (linear1): Linear(in_features=675, out_features=135, bias=True)\n",
       "  (linear2): Linear(in_features=135, out_features=27, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#note that parameter EN is only used for tracking on which dataset was/is trained, and activate loading of the pretrained head\n",
    "#load the fine-tuned model over english\n",
    "PATH = \"/media/mv/Volume/Download/TEST_EXP/nlp2022-hw2-main-master/saved/model_2022_12_19_15_25_3.pth\"\n",
    "model = Arg_Classifier(\"ES\",cfg)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.train().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs n. 0\n",
      "F1 train: [0.         0.06857143 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.43322476 0.\n",
      " 0.         0.9834875  0.         0.         0.         0.04166667\n",
      " 0.         0.         0.44676806 0.         0.         0.\n",
      " 0.04545455 0.        ]\n",
      "F1 avg train: 0.9492945394245353\n",
      "identification {'true_positives': 932, 'false_positives': 224, 'false_negatives': 1222, 'precision': 0.8062283737024222, 'recall': 0.43268337975858867, 'f1': 0.5631419939577039}\n",
      "classification_result {'true_positives': 517, 'false_positives': 639, 'false_negatives': 1637, 'precision': 0.4472318339100346, 'recall': 0.24001857010213556, 'f1': 0.31238670694864046}\n",
      "EPOCHS : 0\n",
      "F1 eval : [0.         0.01474926 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.51171393 0.         0.\n",
      " 0.98792859 0.         0.         0.         0.         0.\n",
      " 0.         0.56926316 0.         0.         0.         0.\n",
      " 0.03058104 0.        ]\n",
      "F1 avg eval : 0.9593153965026872\n",
      "identification {'true_positives': 2520, 'false_positives': 304, 'false_negatives': 2377, 'precision': 0.8923512747875354, 'recall': 0.5146007759852971, 'f1': 0.6527651858567541}\n",
      "classification_result {'true_positives': 1516, 'false_positives': 1308, 'false_negatives': 3381, 'precision': 0.5368271954674221, 'recall': 0.30957729221972635, 'f1': 0.39269524672969824}\n",
      "SAVED : saved/model_2022_12_19_15_43_47.pth\n",
      "Epochs n. 1\n",
      "F1 train: [0.         0.04487179 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.49515287 0.\n",
      " 0.03960396 0.98669401 0.         0.         0.         0.\n",
      " 0.05405405 0.         0.62367491 0.         0.         0.\n",
      " 0.16296296 0.        ]\n",
      "F1 avg train: 0.9560521205196473\n",
      "identification {'true_positives': 1142, 'false_positives': 153, 'false_negatives': 1015, 'precision': 0.8818532818532818, 'recall': 0.5294390356977283, 'f1': 0.6616454229432214}\n",
      "classification_result {'true_positives': 706, 'false_positives': 589, 'false_negatives': 1451, 'precision': 0.5451737451737452, 'recall': 0.3273064441353732, 'f1': 0.40903823870220163}\n",
      "EPOCHS : 1\n",
      "F1 eval : [0.         0.05633803 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.53375196 0.         0.23893805\n",
      " 0.98906397 0.         0.         0.         0.         0.\n",
      " 0.         0.66339992 0.         0.         0.         0.\n",
      " 0.23661972 0.        ]\n",
      "F1 avg eval : 0.9630999535978669\n",
      "identification {'true_positives': 2802, 'false_positives': 342, 'false_negatives': 2076, 'precision': 0.8912213740458015, 'recall': 0.5744157441574416, 'f1': 0.6985789080029918}\n",
      "classification_result {'true_positives': 1819, 'false_positives': 1325, 'false_negatives': 3059, 'precision': 0.5785623409669212, 'recall': 0.3728987289872899, 'f1': 0.45350286711543264}\n",
      "SAVED : saved/model_2022_12_19_15_43_47.pth\n",
      "Epochs n. 2\n",
      "F1 train: [0.         0.1260745  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.51880276 0.\n",
      " 0.18018018 0.9877483  0.         0.         0.06896552 0.08695652\n",
      " 0.         0.01639344 0.66898349 0.         0.         0.\n",
      " 0.16438356 0.        ]\n",
      "F1 avg train: 0.9587598124977363\n",
      "identification {'true_positives': 1220, 'false_positives': 145, 'false_negatives': 914, 'precision': 0.8937728937728938, 'recall': 0.5716963448922212, 'f1': 0.6973420977422121}\n",
      "classification_result {'true_positives': 771, 'false_positives': 594, 'false_negatives': 1363, 'precision': 0.5648351648351648, 'recall': 0.3612933458294283, 'f1': 0.4406973420977422}\n",
      "EPOCHS : 2\n",
      "F1 eval : [0.         0.18030842 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.51496218 0.         0.23684211\n",
      " 0.98977735 0.         0.         0.         0.         0.\n",
      " 0.12048193 0.67692308 0.         0.         0.         0.\n",
      " 0.44343891 0.        ]\n",
      "F1 avg eval : 0.9653840914463983\n",
      "identification {'true_positives': 2913, 'false_positives': 299, 'false_negatives': 1976, 'precision': 0.9069115815691158, 'recall': 0.5958273675598282, 'f1': 0.7191704727811382}\n",
      "classification_result {'true_positives': 1884, 'false_positives': 1328, 'false_negatives': 3005, 'precision': 0.5865504358655044, 'recall': 0.3853548782982205, 'f1': 0.46512776200469086}\n",
      "SAVED : saved/model_2022_12_19_15_43_47.pth\n",
      "Epochs n. 3\n",
      "F1 train: [0.         0.12707182 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.52977099 0.\n",
      " 0.14678899 0.98865647 0.         0.         0.         0.05\n",
      " 0.05263158 0.23776224 0.69881557 0.         0.         0.\n",
      " 0.33532934 0.        ]\n",
      "F1 avg train: 0.9614860164480786\n",
      "identification {'true_positives': 1291, 'false_positives': 143, 'false_negatives': 857, 'precision': 0.900278940027894, 'recall': 0.601024208566108, 'f1': 0.7208263539921831}\n",
      "classification_result {'true_positives': 838, 'false_positives': 596, 'false_negatives': 1310, 'precision': 0.5843793584379359, 'recall': 0.39013035381750466, 'f1': 0.4678950307091011}\n",
      "EPOCHS : 3\n",
      "F1 eval : [0.         0.09269663 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.54641089 0.         0.41134752\n",
      " 0.99001122 0.         0.         0.         0.         0.\n",
      " 0.17714286 0.69167905 0.         0.         0.         0.\n",
      " 0.30526316 0.        ]\n",
      "F1 avg eval : 0.9656290042429252\n",
      "identification {'true_positives': 3022, 'false_positives': 333, 'false_negatives': 1884, 'precision': 0.9007451564828614, 'recall': 0.6159804321239298, 'f1': 0.7316305532017915}\n",
      "classification_result {'true_positives': 1994, 'false_positives': 1361, 'false_negatives': 2912, 'precision': 0.5943368107302534, 'recall': 0.40644109253974725, 'f1': 0.48275027236412055}\n",
      "SAVED : saved/model_2022_12_19_15_43_47.pth\n",
      "Epochs n. 4\n",
      "F1 train: [0.         0.26506024 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.54602675 0.\n",
      " 0.33587786 0.98898928 0.         0.         0.33333333 0.18604651\n",
      " 0.17777778 0.32098765 0.71099744 0.         0.         0.\n",
      " 0.28571429 0.        ]\n",
      "F1 avg train: 0.9638150633855087\n",
      "identification {'true_positives': 1331, 'false_positives': 145, 'false_negatives': 825, 'precision': 0.9017615176151762, 'recall': 0.6173469387755102, 'f1': 0.7329295154185022}\n",
      "classification_result {'true_positives': 902, 'false_positives': 574, 'false_negatives': 1254, 'precision': 0.6111111111111112, 'recall': 0.41836734693877553, 'f1': 0.49669603524229083}\n",
      "EPOCHS : 4\n",
      "F1 eval : [0.         0.23195266 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.53813968 0.         0.43835616\n",
      " 0.99035729 0.         0.         0.08       0.02777778 0.\n",
      " 0.29230769 0.70888974 0.         0.         0.         0.\n",
      " 0.39702233 0.        ]\n",
      "F1 avg eval : 0.9674899849998956\n",
      "identification {'true_positives': 3074, 'false_positives': 316, 'false_negatives': 1808, 'precision': 0.9067846607669616, 'recall': 0.6296599754199099, 'f1': 0.7432301740812379}\n",
      "classification_result {'true_positives': 2067, 'false_positives': 1323, 'false_negatives': 2815, 'precision': 0.6097345132743363, 'recall': 0.4233920524375256, 'f1': 0.49975822050290136}\n",
      "SAVED : saved/model_2022_12_19_15_43_47.pth\n",
      "Epochs n. 5\n",
      "F1 train: [0.         0.35652174 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.53135314 0.\n",
      " 0.32061069 0.98921559 0.         0.         0.25       0.35714286\n",
      " 0.37209302 0.28742515 0.74049028 0.         0.         0.\n",
      " 0.3583815  0.        ]\n",
      "F1 avg train: 0.9650298208947482\n",
      "identification {'true_positives': 1362, 'false_positives': 150, 'false_negatives': 793, 'precision': 0.9007936507936508, 'recall': 0.6320185614849188, 'f1': 0.7428415598581946}\n",
      "classification_result {'true_positives': 940, 'false_positives': 572, 'false_negatives': 1215, 'precision': 0.6216931216931217, 'recall': 0.4361948955916473, 'f1': 0.5126806653940551}\n",
      "EPOCHS : 5\n",
      "F1 eval : [0.         0.28806584 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.53364738 0.         0.43278689\n",
      " 0.99034448 0.         0.         0.04       0.02666667 0.\n",
      " 0.25520833 0.7063922  0.         0.         0.         0.\n",
      " 0.4195122  0.        ]\n",
      "F1 avg eval : 0.9677381084824558\n",
      "identification {'true_positives': 3147, 'false_positives': 394, 'false_negatives': 1748, 'precision': 0.8887319966111268, 'recall': 0.6429009193054137, 'f1': 0.7460881934566145}\n",
      "classification_result {'true_positives': 2114, 'false_positives': 1427, 'false_negatives': 2781, 'precision': 0.5970064953402994, 'recall': 0.43186925434116447, 'f1': 0.5011853959222381}\n",
      "SAVED : saved/model_2022_12_19_15_43_47.pth\n",
      "Epochs n. 6\n",
      "F1 train: [0.         0.39308855 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.57708162 0.\n",
      " 0.37313433 0.98982221 0.         0.         0.35294118 0.26923077\n",
      " 0.18867925 0.3699422  0.74936709 0.         0.         0.\n",
      " 0.41176471 0.        ]\n",
      "F1 avg train: 0.9669054810260753\n",
      "identification {'true_positives': 1402, 'false_positives': 153, 'false_negatives': 728, 'precision': 0.9016077170418006, 'recall': 0.6582159624413145, 'f1': 0.760922659430122}\n",
      "classification_result {'true_positives': 995, 'false_positives': 560, 'false_negatives': 1135, 'precision': 0.639871382636656, 'recall': 0.4671361502347418, 'f1': 0.5400271370420624}\n",
      "EPOCHS : 6\n",
      "F1 eval : [0.         0.28819068 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.55555556 0.         0.44936709\n",
      " 0.99056446 0.         0.         0.07843137 0.05194805 0.\n",
      " 0.30695444 0.70076726 0.         0.         0.         0.\n",
      " 0.41849148 0.        ]\n",
      "F1 avg eval : 0.9682914580165815\n",
      "identification {'true_positives': 3221, 'false_positives': 421, 'false_negatives': 1662, 'precision': 0.884404173531027, 'recall': 0.6596354699979521, 'f1': 0.7556598240469208}\n",
      "classification_result {'true_positives': 2177, 'false_positives': 1465, 'false_negatives': 2706, 'precision': 0.5977484898407468, 'recall': 0.44583248003276676, 'f1': 0.5107331378299121}\n",
      "SAVED : saved/model_2022_12_19_15_43_47.pth\n",
      "Epochs n. 7\n",
      "F1 train: [0.         0.4371134  0.04166667 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.58512397 0.\n",
      " 0.48684211 0.99065827 0.         0.         0.26666667 0.41935484\n",
      " 0.37288136 0.44565217 0.78093645 0.         0.         0.\n",
      " 0.42352941 0.        ]\n",
      "F1 avg train: 0.9692693539843206\n",
      "identification {'true_positives': 1472, 'false_positives': 140, 'false_negatives': 676, 'precision': 0.913151364764268, 'recall': 0.6852886405959032, 'f1': 0.7829787234042553}\n",
      "classification_result {'true_positives': 1070, 'false_positives': 542, 'false_negatives': 1078, 'precision': 0.6637717121588089, 'recall': 0.49813780260707635, 'f1': 0.5691489361702128}\n",
      "Early stopping at epoch :  7\n",
      "F1 eval : [0.         0.40221694 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.52036199 0.         0.43558282\n",
      " 0.99021366 0.         0.         0.07017544 0.02739726 0.05194805\n",
      " 0.32618026 0.70820896 0.         0.         0.         0.\n",
      " 0.50107991 0.        ]\n",
      "F1 avg eval : 0.9684451747560112\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "currentDateAndTime = datetime.now()\n",
    "_id = str(currentDateAndTime.year)+\"_\"+str(currentDateAndTime.month)+\"_\"+str(currentDateAndTime.day)+\"_\"+str(currentDateAndTime.hour)+\"_\"+str(currentDateAndTime.minute)+\"_\"+str(currentDateAndTime.second)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#optimizer = torch.optim.Adam(model.parameters(),lr = 0.000005)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "\n",
    "logSotfMax = torch.nn.LogSoftmax(dim=1)\n",
    "nll_loss = torch.nn.NLLLoss()\n",
    "\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "dataloader_dev = DataLoader(dev_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "mapping = dataloader_train.dataset.args_roles\n",
    "\n",
    "auto_model.eval()\n",
    "auto_model.cuda()\n",
    "\n",
    "EPOCHS = 200\n",
    "patience_counter = 0\n",
    "patience = 5\n",
    "max_val_loss = 9999\n",
    "f1_score_max = 0\n",
    "output_path = \"saved\"\n",
    "model_name = \"model_\"+_id+\".pth\"\n",
    "PATH = os.path.join(output_path,model_name)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    #TRAINING\n",
    "    p = []\n",
    "    g = []\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_train):\n",
    "        #print(sample_batched)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "       \n",
    "        #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "        #------------------FILTERING SUB-WORDS----------------------\n",
    "        subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "        word_emebedding = []\n",
    "        for i in range(n):\n",
    "            subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "            flag = subtoken_mask[0,i,0]\n",
    "            if flag :\n",
    "                continue\n",
    "            else :\n",
    "                word_emebedding.append(subwords_embedding)\n",
    "        word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "        #-------------------------FORWARD/BACKWARD----------------------------------\n",
    "        x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "            perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "            predicate_index = sample_batched[\"predicate_index\"],\n",
    "            pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "            predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])        \n",
    "        b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "        loss = nll_loss(logSotfMax(x),gt)\n",
    "        total_loss = total_loss + loss\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "        counter += 1 \n",
    "            \n",
    "\n",
    "        #-------------------------RESULT STORING----------------------------------\n",
    "        predicted = torch.argmax(x, dim=1)\n",
    "        p += predicted.tolist()\n",
    "        g += gt.tolist()\n",
    "    \n",
    "\n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    scheduler.step()\n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    print(\"Epochs n.\", epoch)\n",
    "    print(\"F1 train:\",f1)\n",
    "    print(\"F1 avg train:\",f1_avg)\n",
    "    \n",
    "    avg_train_loss = total_loss/counter\n",
    "    writer.add_scalar(\"EN_Loss_ES/train\", avg_train_loss, epoch)\n",
    "\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"EN_Train_ES/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"EN_Train_ES/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "    #EVALUATION\n",
    "    p = []\n",
    "    g = []\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_dev):\n",
    "    \n",
    "      #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "            #------------------FILTERING SUB-WORDS----------------------\n",
    "            subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "            word_emebedding = []\n",
    "            for i in range(n): \n",
    "                subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "                flag = subtoken_mask[0,i,0]\n",
    "                if flag :\n",
    "                    continue\n",
    "                else :\n",
    "                    word_emebedding.append(subwords_embedding)\n",
    "            word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "            #-------------------------FORWARD----------------------------------\n",
    "            x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "                        perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "                        predicate_index = sample_batched[\"predicate_index\"],\n",
    "                        pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "                        predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])   \n",
    "\n",
    "\n",
    "            b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "            loss = nll_loss(logSotfMax(x),gt)\n",
    "            total_loss = total_loss + loss\n",
    "            #-------------------------RESULT STORING----------------------------------\n",
    "            predicted = torch.argmax(x, dim=1)\n",
    "            p += predicted.tolist()\n",
    "            g += gt.tolist()\n",
    "            counter += 1 \n",
    "    \n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    avg_eval_loss = total_loss/counter\n",
    "\n",
    "    if avg_eval_loss < max_val_loss:\n",
    "        max_val_loss = avg_eval_loss\n",
    "    else :\n",
    "        patience_counter += 1\n",
    "    \n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    if patience_counter >= patience :  \n",
    "\n",
    "\n",
    "        print(\"Early stopping at epoch : \",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 avg eval :\",f1_avg)\n",
    "        break\n",
    "    else :\n",
    "        print(\"EPOCHS :\",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 avg eval :\",f1_avg)\n",
    "    \n",
    "\n",
    "    writer.add_scalar(\"EN_Loss_ES/validation\", avg_eval_loss, epoch)\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"EN_Eval_ES/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"EN_Eval_ES/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if f1_avg > f1_score_max:\n",
    "        f1_score_max = f1_avg\n",
    "        print(\"SAVED :\",PATH)\n",
    "        torch.save(model.state_dict(),PATH)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare without Transfert Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs n. 0\n",
      "F1 train: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.17606602 0.\n",
      " 0.         0.97629743 0.         0.         0.         0.\n",
      " 0.         0.         0.020558   0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9326034676010221\n",
      "identification {'true_positives': 157, 'false_positives': 106, 'false_negatives': 1994, 'precision': 0.596958174904943, 'recall': 0.07298930729893073, 'f1': 0.1300745650372825}\n",
      "classification_result {'true_positives': 71, 'false_positives': 192, 'false_negatives': 2080, 'precision': 0.26996197718631176, 'recall': 0.03300790330079033, 'f1': 0.0588235294117647}\n",
      "EPOCHS : 0\n",
      "F1 eval : [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.4334126  0.         0.\n",
      " 0.98445385 0.         0.         0.         0.         0.\n",
      " 0.         0.12718204 0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9491122608744599\n",
      "identification {'true_positives': 1795, 'false_positives': 361, 'false_negatives': 3080, 'precision': 0.8325602968460112, 'recall': 0.3682051282051282, 'f1': 0.5105959322998151}\n",
      "classification_result {'true_positives': 831, 'false_positives': 1325, 'false_negatives': 4044, 'precision': 0.3854359925788497, 'recall': 0.17046153846153847, 'f1': 0.2363817380173517}\n",
      "SAVED : saved/model_2022_12_19_15_46_45WT.pth\n",
      "Epochs n. 1\n",
      "F1 train: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.37237762 0.\n",
      " 0.         0.98162789 0.         0.         0.         0.\n",
      " 0.         0.         0.45136922 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9464986838059342\n",
      "identification {'true_positives': 757, 'false_positives': 228, 'false_negatives': 1394, 'precision': 0.7685279187817259, 'recall': 0.3519293351929335, 'f1': 0.48278061224489804}\n",
      "classification_result {'true_positives': 452, 'false_positives': 533, 'false_negatives': 1699, 'precision': 0.4588832487309645, 'recall': 0.2101348210134821, 'f1': 0.288265306122449}\n",
      "EPOCHS : 1\n",
      "F1 eval : [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.44625316 0.         0.\n",
      " 0.98687232 0.         0.         0.         0.         0.\n",
      " 0.         0.5125399  0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.956798159637657\n",
      "identification {'true_positives': 2450, 'false_positives': 487, 'false_negatives': 2434, 'precision': 0.8341845420497106, 'recall': 0.5016380016380017, 'f1': 0.6265183480373354}\n",
      "classification_result {'true_positives': 1357, 'false_positives': 1580, 'false_negatives': 3527, 'precision': 0.4620360912495744, 'recall': 0.27784602784602785, 'f1': 0.34701444828027106}\n",
      "SAVED : saved/model_2022_12_19_15_46_45WT.pth\n",
      "Epochs n. 2\n",
      "F1 train: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.42304657 0.\n",
      " 0.         0.98405717 0.         0.         0.         0.\n",
      " 0.         0.         0.59271523 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9511966030647121\n",
      "identification {'true_positives': 1009, 'false_positives': 251, 'false_negatives': 1141, 'precision': 0.8007936507936508, 'recall': 0.46930232558139534, 'f1': 0.5917888563049853}\n",
      "classification_result {'true_positives': 626, 'false_positives': 634, 'false_negatives': 1524, 'precision': 0.49682539682539684, 'recall': 0.2911627906976744, 'f1': 0.3671554252199414}\n",
      "EPOCHS : 2\n",
      "F1 eval : [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.50244886 0.         0.\n",
      " 0.98871696 0.         0.         0.         0.         0.\n",
      " 0.         0.62439418 0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9607667700722805\n",
      "identification {'true_positives': 2745, 'false_positives': 376, 'false_negatives': 2144, 'precision': 0.8795257930150593, 'recall': 0.5614645121701779, 'f1': 0.6853932584269662}\n",
      "classification_result {'true_positives': 1645, 'false_positives': 1476, 'false_negatives': 3244, 'precision': 0.5270746555591157, 'recall': 0.33646962569032524, 'f1': 0.4107365792759052}\n",
      "SAVED : saved/model_2022_12_19_15_46_45WT.pth\n",
      "Epochs n. 3\n",
      "F1 train: [0.         0.00673401 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.47250194 0.\n",
      " 0.         0.98621672 0.         0.         0.         0.\n",
      " 0.         0.         0.65463495 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9552013584210328\n",
      "identification {'true_positives': 1115, 'false_positives': 189, 'false_negatives': 1020, 'precision': 0.8550613496932515, 'recall': 0.522248243559719, 'f1': 0.6484443152079092}\n",
      "classification_result {'true_positives': 705, 'false_positives': 599, 'false_negatives': 1430, 'precision': 0.5406441717791411, 'recall': 0.33021077283372363, 'f1': 0.4100029078220413}\n",
      "EPOCHS : 3\n",
      "F1 eval : [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.52492492 0.         0.\n",
      " 0.98865671 0.         0.         0.         0.         0.\n",
      " 0.         0.65524781 0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.961047227094713\n",
      "identification {'true_positives': 2820, 'false_positives': 434, 'false_negatives': 2069, 'precision': 0.8666256914566687, 'recall': 0.5768050726119861, 'f1': 0.692619427729338}\n",
      "classification_result {'true_positives': 1773, 'false_positives': 1481, 'false_negatives': 3116, 'precision': 0.5448678549477566, 'recall': 0.36265084884434445, 'f1': 0.4354660444553604}\n",
      "SAVED : saved/model_2022_12_19_15_46_45WT.pth\n",
      "Epochs n. 4\n",
      "F1 train: [0.         0.04458599 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.496997   0.\n",
      " 0.         0.98690944 0.         0.         0.         0.\n",
      " 0.         0.         0.69745223 0.         0.         0.\n",
      " 0.03278689 0.        ]\n",
      "F1 avg train: 0.9568220490755146\n",
      "identification {'true_positives': 1199, 'false_positives': 190, 'false_negatives': 956, 'precision': 0.86321094312455, 'recall': 0.5563805104408353, 'f1': 0.6766365688487584}\n",
      "classification_result {'true_positives': 778, 'false_positives': 611, 'false_negatives': 1377, 'precision': 0.5601151907847373, 'recall': 0.36102088167053364, 'f1': 0.43905191873589167}\n",
      "EPOCHS : 4\n",
      "F1 eval : [0.         0.12724758 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.52681091 0.         0.\n",
      " 0.98877624 0.         0.         0.         0.         0.\n",
      " 0.1994302  0.62095238 0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9620486244446943\n",
      "identification {'true_positives': 2761, 'false_positives': 368, 'false_negatives': 2114, 'precision': 0.8823905401086609, 'recall': 0.5663589743589743, 'f1': 0.6899050474762618}\n",
      "classification_result {'true_positives': 1736, 'false_positives': 1393, 'false_negatives': 3139, 'precision': 0.5548098434004475, 'recall': 0.3561025641025641, 'f1': 0.43378310844577717}\n",
      "SAVED : saved/model_2022_12_19_15_46_45WT.pth\n",
      "Epochs n. 5\n",
      "F1 train: [0.         0.10465116 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.50687023 0.\n",
      " 0.13333333 0.9878918  0.         0.         0.         0.\n",
      " 0.         0.22360248 0.73070866 0.         0.         0.\n",
      " 0.07874016 0.        ]\n",
      "F1 avg train: 0.9598061645717502\n",
      "identification {'true_positives': 1282, 'false_positives': 197, 'false_negatives': 858, 'precision': 0.8668018931710615, 'recall': 0.5990654205607476, 'f1': 0.7084830063553468}\n",
      "classification_result {'true_positives': 844, 'false_positives': 635, 'false_negatives': 1296, 'precision': 0.5706558485463151, 'recall': 0.394392523364486, 'f1': 0.4664271898314451}\n",
      "EPOCHS : 5\n",
      "F1 eval : [0.         0.13746631 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.53320743 0.         0.17757009\n",
      " 0.98958928 0.         0.         0.         0.         0.\n",
      " 0.30170316 0.63988095 0.         0.         0.         0.\n",
      " 0.14857143 0.        ]\n",
      "F1 avg eval : 0.9642106494627098\n",
      "identification {'true_positives': 2946, 'false_positives': 377, 'false_negatives': 1919, 'precision': 0.8865482997291604, 'recall': 0.6055498458376156, 'f1': 0.719589643380557}\n",
      "classification_result {'true_positives': 1865, 'false_positives': 1458, 'false_negatives': 3000, 'precision': 0.5612398435148962, 'recall': 0.38335046248715315, 'f1': 0.4555446995603322}\n",
      "SAVED : saved/model_2022_12_19_15_46_45WT.pth\n",
      "Epochs n. 6\n",
      "F1 train: [0.         0.14130435 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.52245509 0.\n",
      " 0.12173913 0.98921672 0.         0.         0.         0.\n",
      " 0.         0.36871508 0.7476489  0.         0.         0.\n",
      " 0.2027027  0.        ]\n",
      "F1 avg train: 0.962424113115842\n",
      "identification {'true_positives': 1391, 'false_positives': 188, 'false_negatives': 752, 'precision': 0.8809373020899304, 'recall': 0.6490900606626225, 'f1': 0.7474476088124664}\n",
      "classification_result {'true_positives': 907, 'false_positives': 672, 'false_negatives': 1236, 'precision': 0.5744141861937936, 'recall': 0.42323845076994865, 'f1': 0.48737238044062337}\n",
      "EPOCHS : 6\n",
      "F1 eval : [0.         0.12980132 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.53291536 0.         0.22624434\n",
      " 0.99017995 0.         0.         0.         0.         0.\n",
      " 0.32727273 0.67338241 0.         0.         0.         0.\n",
      " 0.17241379 0.        ]\n",
      "F1 avg eval : 0.9653579129244246\n",
      "identification {'true_positives': 3147, 'false_positives': 435, 'false_negatives': 1732, 'precision': 0.8785594639865997, 'recall': 0.6450092232014757, 'f1': 0.7438837016901076}\n",
      "classification_result {'true_positives': 1980, 'false_positives': 1602, 'false_negatives': 2899, 'precision': 0.5527638190954773, 'recall': 0.4058208649313384, 'f1': 0.468029783713509}\n",
      "SAVED : saved/model_2022_12_19_15_46_45WT.pth\n",
      "Epochs n. 7\n",
      "F1 train: [0.         0.25358852 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.54813665 0.\n",
      " 0.21666667 0.98998556 0.         0.         0.         0.\n",
      " 0.         0.36548223 0.78713629 0.         0.         0.\n",
      " 0.29761905 0.        ]\n",
      "F1 avg train: 0.9651852556529101\n",
      "identification {'true_positives': 1464, 'false_positives': 180, 'false_negatives': 701, 'precision': 0.8905109489051095, 'recall': 0.6762124711316397, 'f1': 0.768705697033342}\n",
      "classification_result {'true_positives': 994, 'false_positives': 650, 'false_negatives': 1171, 'precision': 0.6046228710462287, 'recall': 0.4591224018475751, 'f1': 0.5219217642425834}\n",
      "EPOCHS : 7\n",
      "F1 eval : [0.         0.18411097 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.53446115 0.         0.25514403\n",
      " 0.98985055 0.         0.         0.         0.         0.\n",
      " 0.3411215  0.66666667 0.         0.         0.         0.\n",
      " 0.23463687 0.        ]\n",
      "F1 avg eval : 0.9655376391929702\n",
      "identification {'true_positives': 2987, 'false_positives': 356, 'false_negatives': 1883, 'precision': 0.8935088244092133, 'recall': 0.613347022587269, 'f1': 0.7273834165347619}\n",
      "classification_result {'true_positives': 1937, 'false_positives': 1406, 'false_negatives': 2933, 'precision': 0.5794196829195334, 'recall': 0.397741273100616, 'f1': 0.4716912212346281}\n",
      "SAVED : saved/model_2022_12_19_15_46_45WT.pth\n",
      "Epochs n. 8\n",
      "F1 train: [0.         0.3359375  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.55980472 0.\n",
      " 0.37762238 0.99119752 0.         0.         0.         0.24\n",
      " 0.         0.41025641 0.81418658 0.         0.         0.\n",
      " 0.40437158 0.        ]\n",
      "F1 avg train: 0.9684429583499209\n",
      "identification {'true_positives': 1551, 'false_positives': 169, 'false_negatives': 602, 'precision': 0.9017441860465116, 'recall': 0.7203901532745007, 'f1': 0.8009295120061968}\n",
      "classification_result {'true_positives': 1068, 'false_positives': 652, 'false_negatives': 1085, 'precision': 0.6209302325581395, 'recall': 0.49605202043660007, 'f1': 0.5515104570100696}\n",
      "Early stopping at epoch :  8\n",
      "F1 eval : [0.         0.18858561 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.54355401 0.         0.38513514\n",
      " 0.98989465 0.         0.         0.         0.         0.\n",
      " 0.30523918 0.67118886 0.         0.         0.         0.\n",
      " 0.325      0.        ]\n",
      "F1 avg eval : 0.9662967881783213\n"
     ]
    }
   ],
   "source": [
    "model = Arg_Classifier(\"ES\",cfg).cuda()\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "currentDateAndTime = datetime.now()\n",
    "_id = str(currentDateAndTime.year)+\"_\"+str(currentDateAndTime.month)+\"_\"+str(currentDateAndTime.day)+\"_\"+str(currentDateAndTime.hour)+\"_\"+str(currentDateAndTime.minute)+\"_\"+str(currentDateAndTime.second)+\"WT\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#optimizer = torch.optim.Adam(model.parameters(),lr = 0.000005)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "\n",
    "logSotfMax = torch.nn.LogSoftmax(dim=1)\n",
    "nll_loss = torch.nn.NLLLoss()\n",
    "\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "dataloader_dev = DataLoader(dev_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "mapping = dataloader_train.dataset.args_roles\n",
    "\n",
    "auto_model.eval()\n",
    "auto_model.cuda()\n",
    "\n",
    "EPOCHS = 200\n",
    "patience_counter = 0\n",
    "patience = 5\n",
    "max_val_loss = 9999\n",
    "f1_score_max = 0\n",
    "output_path = \"saved\"\n",
    "model_name = \"model_\"+_id+\".pth\"\n",
    "PATH = os.path.join(output_path,model_name)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    #TRAINING\n",
    "    p = []\n",
    "    g = []\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_train):\n",
    "        #print(sample_batched)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "       \n",
    "        #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "        #------------------FILTERING SUB-WORDS----------------------\n",
    "        subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "        word_emebedding = []\n",
    "        for i in range(n):\n",
    "            subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "            flag = subtoken_mask[0,i,0]\n",
    "            if flag :\n",
    "                continue\n",
    "            else :\n",
    "                word_emebedding.append(subwords_embedding)\n",
    "        word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "        #-------------------------FORWARD/BACKWARD----------------------------------\n",
    "        x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "            perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "            predicate_index = sample_batched[\"predicate_index\"],\n",
    "            pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "            predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])        \n",
    "        b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "        loss = nll_loss(logSotfMax(x),gt)\n",
    "        total_loss = total_loss + loss\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "        counter += 1 \n",
    "            \n",
    "\n",
    "        #-------------------------RESULT STORING----------------------------------\n",
    "        predicted = torch.argmax(x, dim=1)\n",
    "        p += predicted.tolist()\n",
    "        g += gt.tolist()\n",
    "    \n",
    "\n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    scheduler.step()\n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    print(\"Epochs n.\", epoch)\n",
    "    print(\"F1 train:\",f1)\n",
    "    print(\"F1 avg train:\",f1_avg)\n",
    "    \n",
    "    avg_train_loss = total_loss/counter\n",
    "    writer.add_scalar(\"Loss_ES/train\", avg_train_loss, epoch)\n",
    "\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"Train_ES/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"Train_ES/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "    #EVALUATION\n",
    "    p = []\n",
    "    g = []\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_dev):\n",
    "    \n",
    "      #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "            #------------------FILTERING SUB-WORDS----------------------\n",
    "            subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "            word_emebedding = []\n",
    "            for i in range(n): \n",
    "                subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "                flag = subtoken_mask[0,i,0]\n",
    "                if flag :\n",
    "                    continue\n",
    "                else :\n",
    "                    word_emebedding.append(subwords_embedding)\n",
    "            word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "            #-------------------------FORWARD----------------------------------\n",
    "            x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "                        perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "                        predicate_index = sample_batched[\"predicate_index\"],\n",
    "                        pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "                        predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])   \n",
    "\n",
    "\n",
    "            b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "            loss = nll_loss(logSotfMax(x),gt)\n",
    "            total_loss = total_loss + loss\n",
    "            #-------------------------RESULT STORING----------------------------------\n",
    "            predicted = torch.argmax(x, dim=1)\n",
    "            p += predicted.tolist()\n",
    "            g += gt.tolist()\n",
    "            counter += 1 \n",
    "    \n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    avg_eval_loss = total_loss/counter\n",
    "\n",
    "    if avg_eval_loss < max_val_loss:\n",
    "        max_val_loss = avg_eval_loss\n",
    "    else :\n",
    "        patience_counter += 1\n",
    "    \n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    if patience_counter >= patience :  \n",
    "\n",
    "\n",
    "        print(\"Early stopping at epoch : \",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 avg eval :\",f1_avg)\n",
    "        break\n",
    "    else :\n",
    "        print(\"EPOCHS :\",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 avg eval :\",f1_avg)\n",
    "    \n",
    "\n",
    "    writer.add_scalar(\"Loss_ES/validation\", avg_eval_loss, epoch)\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"Eval_ES/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"Eval_ES/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if f1_avg > f1_score_max:\n",
    "        f1_score_max = f1_avg\n",
    "        print(\"SAVED :\",PATH)\n",
    "        torch.save(model.state_dict(),PATH)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### French"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New French Dataset\n",
    "bert-base-multilingual-cased\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load the configuration of 'bert-base-multili ngual-cased'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'bert-base-multili ngual-cased' is the correct path to a directory containing a config.json file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py:614\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=611'>612</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=612'>613</a>\u001b[0m     \u001b[39m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=613'>614</a>\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=614'>615</a>\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=615'>616</a>\u001b[0m         configuration_file,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=616'>617</a>\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=617'>618</a>\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=618'>619</a>\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=619'>620</a>\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=620'>621</a>\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=621'>622</a>\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=622'>623</a>\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=623'>624</a>\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=624'>625</a>\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=625'>626</a>\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=626'>627</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=627'>628</a>\u001b[0m     commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/utils/hub.py:409\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/utils/hub.py?line=406'>407</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/utils/hub.py?line=407'>408</a>\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/utils/hub.py?line=408'>409</a>\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/utils/hub.py?line=409'>410</a>\u001b[0m         path_or_repo_id,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/utils/hub.py?line=410'>411</a>\u001b[0m         filename,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/utils/hub.py?line=411'>412</a>\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/utils/hub.py?line=412'>413</a>\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/utils/hub.py?line=413'>414</a>\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/utils/hub.py?line=414'>415</a>\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/utils/hub.py?line=415'>416</a>\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/utils/hub.py?line=416'>417</a>\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/utils/hub.py?line=417'>418</a>\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/utils/hub.py?line=418'>419</a>\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/utils/hub.py?line=419'>420</a>\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/utils/hub.py?line=420'>421</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/utils/hub.py?line=422'>423</a>\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py?line=112'>113</a>\u001b[0m \u001b[39mif\u001b[39;00m arg_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrepo_id\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py?line=113'>114</a>\u001b[0m     validate_repo_id(arg_value)\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py?line=115'>116</a>\u001b[0m \u001b[39melif\u001b[39;00m arg_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtoken\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m arg_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:172\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py?line=170'>171</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m REPO_ID_REGEX\u001b[39m.\u001b[39mmatch(repo_id):\n\u001b[0;32m--> <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py?line=171'>172</a>\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py?line=172'>173</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRepo id must use alphanumeric chars or \u001b[39m\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m--\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39m..\u001b[39m\u001b[39m'\u001b[39m\u001b[39m are\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py?line=173'>174</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m forbidden, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m cannot start or end the name, max length is 96:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py?line=174'>175</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py?line=175'>176</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py?line=177'>178</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m--\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m repo_id \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m..\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m repo_id:\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: 'bert-base-multili ngual-cased'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 12\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict\n\u001b[0;32m---> 12\u001b[0m auto_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbert-base-multili ngual-cased\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mmodel class is      : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(auto_model)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-multilingual-cased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:434\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py?line=431'>432</a>\u001b[0m hub_kwargs \u001b[39m=\u001b[39m {name: kwargs\u001b[39m.\u001b[39mpop(name) \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m hub_kwargs_names \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m kwargs}\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py?line=432'>433</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[0;32m--> <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py?line=433'>434</a>\u001b[0m     config, kwargs \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py?line=434'>435</a>\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py?line=435'>436</a>\u001b[0m         return_unused_kwargs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py?line=436'>437</a>\u001b[0m         trust_remote_code\u001b[39m=\u001b[39;49mtrust_remote_code,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py?line=437'>438</a>\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py?line=438'>439</a>\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py?line=439'>440</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py?line=440'>441</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(config, \u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m config\u001b[39m.\u001b[39mauto_map:\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py?line=441'>442</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py:776\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py?line=773'>774</a>\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mname_or_path\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m pretrained_model_name_or_path\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py?line=774'>775</a>\u001b[0m trust_remote_code \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtrust_remote_code\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py?line=775'>776</a>\u001b[0m config_dict, unused_kwargs \u001b[39m=\u001b[39m PretrainedConfig\u001b[39m.\u001b[39;49mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py?line=776'>777</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mAutoConfig\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py?line=777'>778</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py:559\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=556'>557</a>\u001b[0m original_kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=557'>558</a>\u001b[0m \u001b[39m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=558'>559</a>\u001b[0m config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=559'>560</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=560'>561</a>\u001b[0m     original_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config_dict[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py:635\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=631'>632</a>\u001b[0m         \u001b[39mraise\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=632'>633</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=633'>634</a>\u001b[0m         \u001b[39m# For any other exception, we throw a generic error.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=634'>635</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=635'>636</a>\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load the configuration of \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. If you were trying to load it\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=636'>637</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m from \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, make sure you don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt have a local directory with the same\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=637'>638</a>\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m name. Otherwise, make sure \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is the correct path to a directory\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=638'>639</a>\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m containing a \u001b[39m\u001b[39m{\u001b[39;00mconfiguration_file\u001b[39m}\u001b[39;00m\u001b[39m file\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=639'>640</a>\u001b[0m         )\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=641'>642</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=642'>643</a>\u001b[0m     \u001b[39m# Load config dict\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/transformers/configuration_utils.py?line=643'>644</a>\u001b[0m     config_dict \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_dict_from_json_file(resolved_config_file)\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load the configuration of 'bert-base-multili ngual-cased'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'bert-base-multili ngual-cased' is the correct path to a directory containing a config.json file"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import random\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SRL(Dataset):\n",
    " \n",
    "    def __init__(self,language,path,args_roles = None) -> None:\n",
    "\n",
    "        self.path_root = 'data'\n",
    "        self.load_data(language,path)\n",
    "        if args_roles is None :\n",
    "            self.args_roles,self.list_broken_id = self.list_arg_roles()\n",
    "        else : \n",
    "            self.args_roles = args_roles\n",
    "            _,self.list_broken_id = self.list_arg_roles()\n",
    "\n",
    "        self.pos_list,_ = self.list_pos()\n",
    "        print(self.args_roles)\n",
    "        self.predicate_dis,_ = self.list_predicate_roles()\n",
    "        self.pos_list.append(\"Nothing\")\n",
    "        self.predicate_dis.append(\"Nothing\")\n",
    "\n",
    "    def load_data(self,language,mode):\n",
    "        \n",
    "        mode = mode+\".json\"\n",
    "        path = os.path.join(self.path_root,language,mode)\n",
    "        data_file = open(path)\n",
    "       \n",
    "        data_ = json.load(data_file)\n",
    "\n",
    "        list_data = []\n",
    "\n",
    "        for data in data_:\n",
    "            list_data.append(data_[data])\n",
    "        \n",
    "\n",
    "        self.data = list_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, id : int):\n",
    "\n",
    "        flag = False\n",
    "        if id in self.list_broken_id :\n",
    "            flag = True\n",
    "            while flag == True:\n",
    "\n",
    "                rand_id = random.randint(0, len(self.data)-1)\n",
    "                \n",
    "                if rand_id in self.list_broken_id :\n",
    "                    pass\n",
    "                else :\n",
    "                    flag = False\n",
    "                    id = rand_id        \n",
    "\n",
    "\n",
    "        data = self.pre_processing(self.data[id])\n",
    "        data = self.processig(data)\n",
    "        return data\n",
    "        \n",
    "    def pre_processing(self, data:dict):\n",
    "        data_list = []\n",
    "        for role in data[\"roles\"]:\n",
    "            dictionary = dict()\n",
    "            dictionary[\"words\"] = data[\"words\"]\n",
    "            dictionary[\"role\"] = data[\"roles\"][role]\n",
    "            dictionary[\"pre_idx\"] = role\n",
    "            dictionary[\"pos_tags\"] = data[\"pos_tags\"]\n",
    "            dictionary[\"predicate_meaning\"] = data[\"predicates\"]\n",
    "            data_list.append(dictionary)    \n",
    "        return data_list\n",
    "    \n",
    "    def processig(self,data_list:list):\n",
    "        \n",
    "        for dictionary in data_list:\n",
    "\n",
    "            #dictionary[\"words\"] = data[\"words\"]\n",
    "            dictionary[\"gt_arg_identification\"] = self.arg_id(dictionary[\"role\"])\n",
    "            dictionary[\"gt_arg_classification\"] = self.arg_class(dictionary[\"role\"])\n",
    "            dictionary[\"pos_idx\"] = self.pos_idx(dictionary[\"pos_tags\"])\n",
    "            dictionary[\"predicate_meaning_idx\"] = self.predicate_meaning_idx(dictionary[\"predicate_meaning\"])\n",
    "        \n",
    "        return data_list\n",
    "   \n",
    "    def list_arg_roles(self):\n",
    "        list_roles = []\n",
    "        list_broken_id = []\n",
    "        for i,element in enumerate(self.data):\n",
    "            flag = True\n",
    "            try : roles = element[\"roles\"]\n",
    "            except : flag = False\n",
    "            if flag :\n",
    "                for e in roles:\n",
    "                    sentence = element[\"roles\"][e]\n",
    "\n",
    "                    for word in sentence:\n",
    "                        \n",
    "                        list_roles.append(word)\n",
    "                list_roles = list(set(list_roles))\n",
    "            else : \n",
    "                list_broken_id.append(i)\n",
    "        return list_roles,list_broken_id\n",
    "\n",
    "    def list_predicate_roles(self):\n",
    "        list_predicate_roles = []\n",
    "        list_broken_id = []\n",
    "        for i,element in enumerate(self.data):\n",
    "            flag = True\n",
    "            try : predicates = element[\"predicates\"]\n",
    "            except : flag = False\n",
    "            if flag :\n",
    "                for pre in predicates:\n",
    "                    list_predicate_roles.append(pre)\n",
    "                list_predicate_roles = list(set(list_predicate_roles))\n",
    "            else : \n",
    "                list_broken_id.append(i)\n",
    "        return list_predicate_roles,list_broken_id\n",
    "\n",
    "    def list_pos(self):\n",
    "        list_pos = []\n",
    "        list_broken_id = []\n",
    "        for i,element in enumerate(self.data):\n",
    "            flag = True\n",
    "            try : pos = element[\"pos_tags\"]\n",
    "            except : flag = False\n",
    "            if flag :\n",
    "                for e in pos:\n",
    "                    list_pos.append(e)\n",
    "                list_pos = list(set(list_pos))\n",
    "            else : \n",
    "                list_broken_id.append(i)\n",
    "        return list_pos,list_broken_id\n",
    "  \n",
    "    def arg_class(self,role:list):\n",
    "        list_idxs = []\n",
    "        for element in role:\n",
    "            list_idxs.append(self.args_roles.index(element))\n",
    "        \n",
    "\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "\n",
    "    def arg_id(self,role:dict):\n",
    "        list_idxs = []\n",
    "        for element in role:\n",
    "            if element == \"_\":\n",
    "                list_idxs.append(0)\n",
    "            else :\n",
    "                list_idxs.append(1)\n",
    "\n",
    "        \n",
    "\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "\n",
    "    def pos_idx(self,pos_tags:dict):\n",
    "        list_idxs = []\n",
    "        list_idxs.append(self.pos_list.index(\"Nothing\"))\n",
    "\n",
    "        for element in pos_tags:\n",
    "            list_idxs.append(self.pos_list.index(element))\n",
    "        \n",
    "        list_idxs.append(self.pos_list.index(\"Nothing\"))\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "    \n",
    "    def predicate_meaning_idx(self,predicate_meaning_tags:dict):\n",
    "        list_idxs = []\n",
    "        list_idxs.append(self.predicate_dis.index(\"Nothing\"))\n",
    "\n",
    "        for element in predicate_meaning_tags:\n",
    "            list_idxs.append(self.predicate_dis.index(element))\n",
    "        \n",
    "        list_idxs.append(self.predicate_dis.index(\"Nothing\"))\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64) \n",
    "\n",
    "    \n",
    "# here we define our collate function\n",
    "def collate_fn(batch) -> Dict[str, torch.Tensor]:\n",
    "    #print(batch)\n",
    "    input = dict() \n",
    "    batch_sentence = [] \n",
    "    #print(len(batch))\n",
    "    for period in batch:\n",
    "        for sentence in period :\n",
    "        \n",
    "            #print(len(sentence[0][\"words\"]))\n",
    "            pre_idx = int(sentence[\"pre_idx\"])\n",
    "            \n",
    "\n",
    "            predicate = sentence[\"words\"][pre_idx]\n",
    "\n",
    "            text = \" \".join(sentence[\"words\"])\n",
    "            tokens: list[str] = text.split()\n",
    "            predicate: list[str] = predicate.split()\n",
    "\n",
    "            #text = sentence[0][\"words\"]\n",
    "            \n",
    "            t = (tokens,predicate)\n",
    "\n",
    "            batch_sentence.append(t)\n",
    "            #print(batch_sentence)\n",
    "\n",
    "    batch_output = tokenizer.batch_encode_plus(batch_sentence,padding=True,is_split_into_words=True, truncation=True,return_offsets_mapping=True, return_tensors=\"pt\")\n",
    "    #print(batch_output.keys())\n",
    "\n",
    "\n",
    "    gt = dict()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    for period in batch:\n",
    "\n",
    "        list_positional_predicate_encoding = []\n",
    "        list_arg_gt = []\n",
    "        list_predicate_index = [] \n",
    "        list_pos_index = [] \n",
    "        list_predicate_meaning_index = []\n",
    "\n",
    "        for sentence in period:\n",
    "            #positional_encoding\n",
    "            #+2 per il CLS iniziale ad SEP finale\n",
    "            sentence_words_lenght =  len(sentence[\"words\"])\n",
    "            positional_predicate_encoding = torch.zeros(1,sentence_words_lenght+2)\n",
    "            #+1 per il CLS iniziale\n",
    "            pre_idx = int(sentence[\"pre_idx\"])\n",
    "            positional_predicate_encoding[:,pre_idx+1] = 1\n",
    "            list_positional_predicate_encoding.append(positional_predicate_encoding)\n",
    "            #print(\"positional_prefix_encoding\",positional_predicate_encoding)\n",
    "            list_predicate_index.append(pre_idx)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            pos = torch.unsqueeze(sentence[\"pos_idx\"],dim = 0)\n",
    "            list_pos_index.append(pos)\n",
    "            predicate_meaning_idxs = torch.unsqueeze(sentence[\"predicate_meaning_idx\"],dim = 0)\n",
    "            list_predicate_meaning_index.append(predicate_meaning_idxs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #note CLS and SEP are discharder after Bi-LSTM, the Classifier takes in input only wokrds hidden state embedding\n",
    "            arg_gt = torch.unsqueeze(sentence[\"gt_arg_classification\"],dim = 0)\n",
    "            list_arg_gt.append(arg_gt)\n",
    "        \n",
    "\n",
    "    list_arg_gt = torch.cat(list_arg_gt,dim = 0)\n",
    "    list_pos_index = torch.cat(list_pos_index,dim = 0)\n",
    "    list_predicate_meaning_index = torch.cat(list_predicate_meaning_index,dim = 0)\n",
    "    list_positional_predicate_encoding = torch.cat(list_positional_predicate_encoding,dim = 0)\n",
    "    gt[\"arg_gt\"] = list_arg_gt\n",
    "    input[\"predicate_index\"] = list_predicate_index\n",
    "    input[\"pos_index\"] = list_pos_index.long()\n",
    "    input[\"predicate_meaning_idx\"] = list_predicate_meaning_index.long()\n",
    "    offset = batch_output.pop(\"offset_mapping\")\n",
    "    input[\"BERT_input\"] = batch_output\n",
    "    input[\"positional_encoding\"] = list_positional_predicate_encoding.long()\n",
    "    input[\"offset_mapping\"] = offset\n",
    "    input[\"gt\"] = gt\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    return\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes from 3 to 4 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#train_dataset = SRL(\"EN\",\"train\")\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#note here we are directly loading args_roles mapping as computed before the the dasetet where we have perfomerd \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#EN and ES dataset should have the same consistency in generating \u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mSRL\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFR\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs_roles\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#same mapping should be used in both the dataset\u001b[39;00m\n\u001b[1;32m      7\u001b[0m dev_dataset \u001b[38;5;241m=\u001b[39m SRL(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdev\u001b[39m\u001b[38;5;124m\"\u001b[39m,train_dataset\u001b[38;5;241m.\u001b[39margs_roles,train_dataset\u001b[38;5;241m.\u001b[39mpos_list)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes from 3 to 4 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "\n",
    "#train_dataset = SRL(\"EN\",\"train\")\n",
    "\n",
    "#note here we are directly loading args_roles mapping as computed before the the dasetet where we have perfomerd \n",
    "#EN and ES dataset should have the same consistency in generating \n",
    "train_dataset = SRL(\"FR\",\"train\",train_dataset.args_roles,train_dataset.pos_list)\n",
    "#same mapping should be used in both the dataset\n",
    "dev_dataset = SRL(\"FR\",\"dev\",train_dataset.args_roles,train_dataset.pos_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English-French attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Arg_Classifier(\n",
       "  (bi_lstm_portable): LSTM(132, 50, num_layers=2, batch_first=True, dropout=0.4, bidirectional=True)\n",
       "  (embedding_predicate_flag): Embedding(2, 32, max_norm=True)\n",
       "  (embedding_predicate): Embedding(304, False, max_norm=True)\n",
       "  (embedding_pos): Embedding(18, 100, max_norm=True)\n",
       "  (bi_lstm): LSTM(900, 50, num_layers=2, batch_first=True, dropout=0.4, bidirectional=True)\n",
       "  (dropout_language_constraint): Dropout(p=0.6, inplace=False)\n",
       "  (dropout_in_classifier): Dropout(p=0.4, inplace=False)\n",
       "  (Relu): ReLU()\n",
       "  (Sigmoid): Sigmoid()\n",
       "  (linear0): Linear(in_features=300, out_features=675, bias=True)\n",
       "  (linear1): Linear(in_features=675, out_features=135, bias=True)\n",
       "  (linear2): Linear(in_features=135, out_features=27, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#note that parameter EN is only used for tracking on which dataset was/is trained, and activate loading of the pretrained head\n",
    "#load the fine-tuned model over english\n",
    "PATH = \"saved/model_2022_12_18_21_33_45.pth\"\n",
    "model = Arg_Classifier(\"ES\",cfg)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.train().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs n. 0\n",
      "F1 train: [0.         0.41519926 0.00668896 0.98159286 0.         0.38427948\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9458893102533754\n",
      "identification {'true_positives': 657, 'false_positives': 127, 'false_negatives': 1495, 'precision': 0.8380102040816326, 'recall': 0.30529739776951675, 'f1': 0.4475476839237057}\n",
      "classification_result {'true_positives': 401, 'false_positives': 383, 'false_negatives': 1751, 'precision': 0.5114795918367347, 'recall': 0.18633828996282528, 'f1': 0.2731607629427793}\n",
      "EPOCHS : 0\n",
      "F1 eval : [0.         0.50540098 0.         0.98476308 0.25635359 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.951819105330271\n",
      "identification {'true_positives': 1784, 'false_positives': 270, 'false_negatives': 3098, 'precision': 0.8685491723466408, 'recall': 0.3654240065546907, 'f1': 0.5144175317185699}\n",
      "classification_result {'true_positives': 1004, 'false_positives': 1050, 'false_negatives': 3878, 'precision': 0.48880233690360275, 'recall': 0.20565342072920934, 'f1': 0.2895040369088812}\n",
      "SAVED : saved/model_2022_12_18_21_55_17.pth\n",
      "Epochs n. 1\n",
      "F1 train: [0.         0.4831553  0.07228916 0.98556403 0.         0.58646617\n",
      " 0.         0.         0.         0.         0.01639344 0.\n",
      " 0.         0.         0.         0.         0.14814815 0.\n",
      " 0.         0.         0.11940299 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9544334404786856\n",
      "identification {'true_positives': 1012, 'false_positives': 127, 'false_negatives': 1133, 'precision': 0.8884986830553117, 'recall': 0.4717948717948718, 'f1': 0.6163215590742995}\n",
      "classification_result {'true_positives': 635, 'false_positives': 504, 'false_negatives': 1510, 'precision': 0.5575065847234416, 'recall': 0.29603729603729606, 'f1': 0.38672350791717425}\n",
      "EPOCHS : 1\n",
      "F1 eval : [0.         0.51134548 0.         0.98473388 0.23535511 0.\n",
      " 0.         0.         0.         0.18390805 0.         0.\n",
      " 0.         0.         0.         0.         0.03448276 0.\n",
      " 0.         0.         0.01286174 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9524853334057606\n",
      "identification {'true_positives': 1901, 'false_positives': 415, 'false_negatives': 3022, 'precision': 0.8208117443868739, 'recall': 0.38614665854153973, 'f1': 0.5252106644564167}\n",
      "classification_result {'true_positives': 1065, 'false_positives': 1251, 'false_negatives': 3858, 'precision': 0.45984455958549225, 'recall': 0.21633150517976843, 'f1': 0.29423953584749274}\n",
      "SAVED : saved/model_2022_12_18_21_55_17.pth\n",
      "Epochs n. 2\n",
      "F1 train: [0.         0.49960349 0.07902736 0.98612618 0.         0.6159292\n",
      " 0.         0.         0.         0.         0.20689655 0.\n",
      " 0.         0.         0.         0.         0.19354839 0.\n",
      " 0.         0.         0.25333333 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.956746441350736\n",
      "identification {'true_positives': 1113, 'false_positives': 182, 'false_negatives': 1033, 'precision': 0.8594594594594595, 'recall': 0.5186393289841565, 'f1': 0.6469049694856146}\n",
      "classification_result {'true_positives': 722, 'false_positives': 573, 'false_negatives': 1424, 'precision': 0.5575289575289575, 'recall': 0.3364398881640261, 'f1': 0.41964545190351643}\n",
      "EPOCHS : 2\n",
      "F1 eval : [0.         0.51954603 0.09116809 0.98472378 0.14238411 0.\n",
      " 0.         0.         0.         0.23268698 0.         0.\n",
      " 0.         0.         0.         0.         0.0361991  0.\n",
      " 0.         0.         0.16374269 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9521032155241269\n",
      "identification {'true_positives': 1926, 'false_positives': 421, 'false_negatives': 2944, 'precision': 0.8206220707285897, 'recall': 0.395482546201232, 'f1': 0.5337397810724678}\n",
      "classification_result {'true_positives': 1059, 'false_positives': 1288, 'false_negatives': 3811, 'precision': 0.4512143161482744, 'recall': 0.21745379876796714, 'f1': 0.293473742552307}\n",
      "Epochs n. 3\n",
      "F1 train: [0.         0.4892772  0.16853933 0.98702312 0.         0.66314864\n",
      " 0.         0.         0.         0.         0.27329193 0.\n",
      " 0.         0.         0.         0.         0.19512195 0.\n",
      " 0.         0.         0.28187919 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9589274068393385\n",
      "identification {'true_positives': 1178, 'false_positives': 168, 'false_negatives': 963, 'precision': 0.87518573551263, 'recall': 0.5502101821578702, 'f1': 0.6756524232864928}\n",
      "classification_result {'true_positives': 770, 'false_positives': 576, 'false_negatives': 1371, 'precision': 0.5720653789004457, 'recall': 0.3596450256889304, 'f1': 0.4416403785488959}\n",
      "EPOCHS : 3\n",
      "F1 eval : [0.         0.52654867 0.12273361 0.98440412 0.1816387  0.\n",
      " 0.         0.         0.         0.25464191 0.         0.\n",
      " 0.         0.         0.         0.         0.18587361 0.\n",
      " 0.         0.         0.33248082 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9535452873154773\n",
      "identification {'true_positives': 2046, 'false_positives': 625, 'false_negatives': 2839, 'precision': 0.7660052414825907, 'recall': 0.41883316274309107, 'f1': 0.5415563790365272}\n",
      "classification_result {'true_positives': 1199, 'false_positives': 1472, 'false_negatives': 3686, 'precision': 0.4488955447397978, 'recall': 0.24544524053224157, 'f1': 0.31736368448914776}\n",
      "SAVED : saved/model_2022_12_18_21_55_17.pth\n",
      "Epochs n. 4\n",
      "F1 train: [0.         0.49594814 0.22051282 0.98784859 0.         0.70686767\n",
      " 0.         0.         0.         0.         0.34285714 0.\n",
      " 0.         0.         0.         0.         0.17241379 0.\n",
      " 0.         0.         0.32051282 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9609269395326534\n",
      "identification {'true_positives': 1255, 'false_positives': 162, 'false_negatives': 899, 'precision': 0.8856739590684545, 'recall': 0.5826369545032498, 'f1': 0.7028843461215346}\n",
      "classification_result {'true_positives': 836, 'false_positives': 581, 'false_negatives': 1318, 'precision': 0.5899788285109386, 'recall': 0.3881151346332405, 'f1': 0.46821618594231307}\n",
      "EPOCHS : 4\n",
      "F1 eval : [0.         0.51295681 0.14188267 0.98365281 0.14747859 0.\n",
      " 0.         0.         0.         0.23561644 0.         0.\n",
      " 0.         0.         0.         0.         0.06278027 0.\n",
      " 0.         0.         0.38834951 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9521465915526715\n",
      "identification {'true_positives': 1914, 'false_positives': 649, 'false_negatives': 2978, 'precision': 0.7467811158798283, 'recall': 0.3912510220768602, 'f1': 0.5134808853118713}\n",
      "classification_result {'true_positives': 1109, 'false_positives': 1454, 'false_negatives': 3783, 'precision': 0.43269605930550137, 'recall': 0.22669664758789862, 'f1': 0.29751844399731725}\n",
      "Epochs n. 5\n",
      "F1 train: [0.         0.52560873 0.3531746  0.98907995 0.         0.73452769\n",
      " 0.         0.         0.         0.         0.3583815  0.\n",
      " 0.         0.         0.         0.         0.32061069 0.\n",
      " 0.         0.         0.38323353 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.964453422178617\n",
      "identification {'true_positives': 1364, 'false_positives': 157, 'false_negatives': 814, 'precision': 0.8967784352399737, 'recall': 0.6262626262626263, 'f1': 0.7374966207082995}\n",
      "classification_result {'true_positives': 937, 'false_positives': 584, 'false_negatives': 1241, 'precision': 0.6160420775805391, 'recall': 0.43021120293847565, 'f1': 0.5066234117329007}\n",
      "Early stopping at epoch :  5\n",
      "F1 eval : [0.         0.51528998 0.23322332 0.98259752 0.04374703 0.\n",
      " 0.         0.         0.         0.25668449 0.         0.\n",
      " 0.         0.         0.         0.         0.21201413 0.\n",
      " 0.         0.         0.45248869 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9508463436472356\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "currentDateAndTime = datetime.now()\n",
    "_id = str(currentDateAndTime.year)+\"_\"+str(currentDateAndTime.month)+\"_\"+str(currentDateAndTime.day)+\"_\"+str(currentDateAndTime.hour)+\"_\"+str(currentDateAndTime.minute)+\"_\"+str(currentDateAndTime.second)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#optimizer = torch.optim.Adam(model.parameters(),lr = 0.000005)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "\n",
    "logSotfMax = torch.nn.LogSoftmax(dim=1)\n",
    "nll_loss = torch.nn.NLLLoss()\n",
    "\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "dataloader_dev = DataLoader(dev_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "mapping = dataloader_train.dataset.args_roles\n",
    "\n",
    "auto_model.eval()\n",
    "auto_model.cuda()\n",
    "\n",
    "EPOCHS = 200\n",
    "patience_counter = 0\n",
    "patience = 5\n",
    "max_val_loss = 9999\n",
    "f1_score_max = 0\n",
    "output_path = \"saved\"\n",
    "model_name = \"model_\"+_id+\".pth\"\n",
    "PATH = os.path.join(output_path,model_name)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    #TRAINING\n",
    "    p = []\n",
    "    g = []\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_train):\n",
    "        #print(sample_batched)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "       \n",
    "        #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "        #------------------FILTERING SUB-WORDS----------------------\n",
    "        subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "        word_emebedding = []\n",
    "        for i in range(n):\n",
    "            subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "            flag = subtoken_mask[0,i,0]\n",
    "            if flag :\n",
    "                continue\n",
    "            else :\n",
    "                word_emebedding.append(subwords_embedding)\n",
    "        word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "        #-------------------------FORWARD/BACKWARD----------------------------------\n",
    "        x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "            perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "            predicate_index = sample_batched[\"predicate_index\"],\n",
    "            pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "            predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])        \n",
    "        b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "        loss = nll_loss(logSotfMax(x),gt)\n",
    "        total_loss = total_loss + loss\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "        counter += 1 \n",
    "            \n",
    "\n",
    "        #-------------------------RESULT STORING----------------------------------\n",
    "        predicted = torch.argmax(x, dim=1)\n",
    "        p += predicted.tolist()\n",
    "        g += gt.tolist()\n",
    "    \n",
    "\n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    scheduler.step()\n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    print(\"Epochs n.\", epoch)\n",
    "    print(\"F1 train:\",f1)\n",
    "    print(\"F1 avg train:\",f1_avg)\n",
    "    \n",
    "    avg_train_loss = total_loss/counter\n",
    "    writer.add_scalar(\"EN_Loss_ES/train\", avg_train_loss, epoch)\n",
    "\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"EN_Train_ES/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"EN_Train_ES/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "    #EVALUATION\n",
    "    p = []\n",
    "    g = []\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_dev):\n",
    "    \n",
    "      #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "            #------------------FILTERING SUB-WORDS----------------------\n",
    "            subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "            word_emebedding = []\n",
    "            for i in range(n): \n",
    "                subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "                flag = subtoken_mask[0,i,0]\n",
    "                if flag :\n",
    "                    continue\n",
    "                else :\n",
    "                    word_emebedding.append(subwords_embedding)\n",
    "            word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "            #-------------------------FORWARD----------------------------------\n",
    "            x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "                        perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "                        predicate_index = sample_batched[\"predicate_index\"],\n",
    "                        pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "                        predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])   \n",
    "\n",
    "\n",
    "            b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "            loss = nll_loss(logSotfMax(x),gt)\n",
    "            total_loss = total_loss + loss\n",
    "            #-------------------------RESULT STORING----------------------------------\n",
    "            predicted = torch.argmax(x, dim=1)\n",
    "            p += predicted.tolist()\n",
    "            g += gt.tolist()\n",
    "            counter += 1 \n",
    "    \n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    avg_eval_loss = total_loss/counter\n",
    "\n",
    "    if avg_eval_loss < max_val_loss:\n",
    "        max_val_loss = avg_eval_loss\n",
    "    else :\n",
    "        patience_counter += 1\n",
    "    \n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    if patience_counter >= patience :  \n",
    "\n",
    "\n",
    "        print(\"Early stopping at epoch : \",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 avg eval :\",f1_avg)\n",
    "        break\n",
    "    else :\n",
    "        print(\"EPOCHS :\",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 avg eval :\",f1_avg)\n",
    "    \n",
    "\n",
    "    writer.add_scalar(\"EN_Loss_ES/validation\", avg_eval_loss, epoch)\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"EN_Eval_ES/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"EN_Eval_ES/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if f1_avg > f1_score_max:\n",
    "        f1_score_max = f1_avg\n",
    "        print(\"SAVED :\",PATH)\n",
    "        torch.save(model.state_dict(),PATH)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare without Transfert Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs n. 0\n",
      "F1 train: [0.         0.19946809 0.         0.9768764  0.         0.0295421\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9336015019908596\n",
      "identification {'true_positives': 172, 'false_positives': 71, 'false_negatives': 1987, 'precision': 0.7078189300411523, 'recall': 0.07966651227420102, 'f1': 0.14321398834304747}\n",
      "classification_result {'true_positives': 85, 'false_positives': 158, 'false_negatives': 2074, 'precision': 0.3497942386831276, 'recall': 0.03937007874015748, 'f1': 0.070774354704413}\n",
      "EPOCHS : 0\n",
      "F1 eval : [0.         0.23416618 0.         0.9777718  0.00265252 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9389493973815146\n",
      "identification {'true_positives': 1030, 'false_positives': 1096, 'false_negatives': 3858, 'precision': 0.48447789275634995, 'recall': 0.2107201309328969, 'f1': 0.2936983176504135}\n",
      "classification_result {'true_positives': 405, 'false_positives': 1721, 'false_negatives': 4483, 'precision': 0.1904985888993415, 'recall': 0.08285597381342062, 'f1': 0.11548331907613345}\n",
      "SAVED : saved/model_2022_12_18_22_2_58.pth\n",
      "Epochs n. 1\n",
      "F1 train: [0.         0.3872549  0.         0.98172752 0.         0.42610365\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9464882649310143\n",
      "identification {'true_positives': 798, 'false_positives': 268, 'false_negatives': 1337, 'precision': 0.7485928705440901, 'recall': 0.3737704918032787, 'f1': 0.4985941893158388}\n",
      "classification_result {'true_positives': 459, 'false_positives': 607, 'false_negatives': 1676, 'precision': 0.43058161350844276, 'recall': 0.21498829039812647, 'f1': 0.2867853795688848}\n",
      "EPOCHS : 1\n",
      "F1 eval : [0.         0.23666396 0.         0.97658545 0.00478469 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9376621220958927\n",
      "identification {'true_positives': 1127, 'false_positives': 1421, 'false_negatives': 3759, 'precision': 0.4423076923076923, 'recall': 0.23065902578796563, 'f1': 0.3032015065913371}\n",
      "classification_result {'true_positives': 441, 'false_positives': 2107, 'false_negatives': 4445, 'precision': 0.17307692307692307, 'recall': 0.09025787965616046, 'f1': 0.11864406779661017}\n",
      "Epochs n. 2\n",
      "F1 train: [0.         0.44530046 0.         0.98438053 0.         0.60050463\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.952145225902215\n",
      "identification {'true_positives': 1022, 'false_positives': 243, 'false_negatives': 1124, 'precision': 0.807905138339921, 'recall': 0.4762348555452004, 'f1': 0.5992377601876282}\n",
      "classification_result {'true_positives': 646, 'false_positives': 619, 'false_negatives': 1500, 'precision': 0.5106719367588933, 'recall': 0.30102516309412863, 'f1': 0.37877455291703316}\n",
      "EPOCHS : 2\n",
      "F1 eval : [0.         0.26013195 0.         0.9780147  0.0071599  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9395047264553521\n",
      "identification {'true_positives': 1022, 'false_positives': 1031, 'false_negatives': 3848, 'precision': 0.4978080857282026, 'recall': 0.20985626283367556, 'f1': 0.2952477249747219}\n",
      "classification_result {'true_positives': 420, 'false_positives': 1633, 'false_negatives': 4450, 'precision': 0.20457866536775451, 'recall': 0.08624229979466119, 'f1': 0.12133468149646107}\n",
      "SAVED : saved/model_2022_12_18_22_2_58.pth\n",
      "Epochs n. 3\n",
      "F1 train: [0.         0.47011952 0.02555911 0.98541674 0.         0.63305785\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9541615111910255\n",
      "identification {'true_positives': 1065, 'false_positives': 200, 'false_negatives': 1089, 'precision': 0.841897233201581, 'recall': 0.49442896935933145, 'f1': 0.6229891781222578}\n",
      "classification_result {'true_positives': 682, 'false_positives': 583, 'false_negatives': 1472, 'precision': 0.5391304347826087, 'recall': 0.3166202414113278, 'f1': 0.39894706054401874}\n",
      "EPOCHS : 3\n",
      "F1 eval : [0.         0.28145455 0.02816901 0.97774314 0.00762112 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9396751709187108\n",
      "identification {'true_positives': 871, 'false_positives': 950, 'false_negatives': 4019, 'precision': 0.4783086216364635, 'recall': 0.17811860940695295, 'f1': 0.2595738340038742}\n",
      "classification_result {'true_positives': 404, 'false_positives': 1417, 'false_negatives': 4486, 'precision': 0.22185612300933552, 'recall': 0.08261758691206544, 'f1': 0.12039934436000595}\n",
      "SAVED : saved/model_2022_12_18_22_2_58.pth\n",
      "Epochs n. 4\n",
      "F1 train: [0.         0.51370399 0.05590062 0.98651971 0.         0.69414674\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.07843137 0.\n",
      " 0.         0.         0.03333333 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9570333876174468\n",
      "identification {'true_positives': 1139, 'false_positives': 185, 'false_negatives': 994, 'precision': 0.8602719033232629, 'recall': 0.5339896858884201, 'f1': 0.658952849291293}\n",
      "classification_result {'true_positives': 764, 'false_positives': 560, 'false_negatives': 1369, 'precision': 0.5770392749244713, 'recall': 0.35818096577590247, 'f1': 0.4420017356089094}\n",
      "EPOCHS : 4\n",
      "F1 eval : [0.         0.27561328 0.01457726 0.9769951  0.00616333 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9388430347981687\n",
      "identification {'true_positives': 843, 'false_positives': 1092, 'false_negatives': 4038, 'precision': 0.4356589147286822, 'recall': 0.1727105101413645, 'f1': 0.24735915492957747}\n",
      "classification_result {'true_positives': 393, 'false_positives': 1542, 'false_negatives': 4488, 'precision': 0.20310077519379846, 'recall': 0.08051628764597418, 'f1': 0.11531690140845072}\n",
      "Epochs n. 5\n",
      "F1 train: [0.         0.51257862 0.12105263 0.98804271 0.         0.72871917\n",
      " 0.         0.         0.         0.         0.0620155  0.\n",
      " 0.         0.         0.         0.         0.12612613 0.\n",
      " 0.         0.         0.01652893 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9595282512313569\n",
      "identification {'true_positives': 1263, 'false_positives': 167, 'false_negatives': 879, 'precision': 0.8832167832167832, 'recall': 0.5896358543417367, 'f1': 0.7071668533034715}\n",
      "classification_result {'true_positives': 819, 'false_positives': 611, 'false_negatives': 1323, 'precision': 0.5727272727272728, 'recall': 0.38235294117647056, 'f1': 0.4585666293393057}\n",
      "Early stopping at epoch :  5\n",
      "F1 eval : [0.         0.30775194 0.03133903 0.97723285 0.00687623 0.\n",
      " 0.         0.         0.         0.04590164 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9394300496737596\n"
     ]
    }
   ],
   "source": [
    "model = Arg_Classifier(\"FR\",cfg).cuda()\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "currentDateAndTime = datetime.now()\n",
    "_id = str(currentDateAndTime.year)+\"_\"+str(currentDateAndTime.month)+\"_\"+str(currentDateAndTime.day)+\"_\"+str(currentDateAndTime.hour)+\"_\"+str(currentDateAndTime.minute)+\"_\"+str(currentDateAndTime.second)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#optimizer = torch.optim.Adam(model.parameters(),lr = 0.000005)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "\n",
    "logSotfMax = torch.nn.LogSoftmax(dim=1)\n",
    "nll_loss = torch.nn.NLLLoss()\n",
    "\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "dataloader_dev = DataLoader(dev_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "mapping = dataloader_train.dataset.args_roles\n",
    "\n",
    "auto_model.eval()\n",
    "auto_model.cuda()\n",
    "\n",
    "EPOCHS = 200\n",
    "patience_counter = 0\n",
    "patience = 5\n",
    "max_val_loss = 9999\n",
    "f1_score_max = 0\n",
    "output_path = \"saved\"\n",
    "model_name = \"model_\"+_id+\".pth\"\n",
    "PATH = os.path.join(output_path,model_name)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    #TRAINING\n",
    "    p = []\n",
    "    g = []\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_train):\n",
    "        #print(sample_batched)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "       \n",
    "        #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "        #------------------FILTERING SUB-WORDS----------------------\n",
    "        subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "        word_emebedding = []\n",
    "        for i in range(n):\n",
    "            subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "            flag = subtoken_mask[0,i,0]\n",
    "            if flag :\n",
    "                continue\n",
    "            else :\n",
    "                word_emebedding.append(subwords_embedding)\n",
    "        word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "        #-------------------------FORWARD/BACKWARD----------------------------------\n",
    "        x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "            perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "            predicate_index = sample_batched[\"predicate_index\"],\n",
    "            pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "            predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])        \n",
    "        b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "        loss = nll_loss(logSotfMax(x),gt)\n",
    "        total_loss = total_loss + loss\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "        counter += 1 \n",
    "            \n",
    "\n",
    "        #-------------------------RESULT STORING----------------------------------\n",
    "        predicted = torch.argmax(x, dim=1)\n",
    "        p += predicted.tolist()\n",
    "        g += gt.tolist()\n",
    "    \n",
    "\n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    scheduler.step()\n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    print(\"Epochs n.\", epoch)\n",
    "    print(\"F1 train:\",f1)\n",
    "    print(\"F1 avg train:\",f1_avg)\n",
    "    \n",
    "    avg_train_loss = total_loss/counter\n",
    "    writer.add_scalar(\"Loss_ES/train\", avg_train_loss, epoch)\n",
    "\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"Train_ES/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"Train_ES/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "    #EVALUATION\n",
    "    p = []\n",
    "    g = []\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_dev):\n",
    "    \n",
    "      #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "            #------------------FILTERING SUB-WORDS----------------------\n",
    "            subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "            word_emebedding = []\n",
    "            for i in range(n): \n",
    "                subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "                flag = subtoken_mask[0,i,0]\n",
    "                if flag :\n",
    "                    continue\n",
    "                else :\n",
    "                    word_emebedding.append(subwords_embedding)\n",
    "            word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "            #-------------------------FORWARD----------------------------------\n",
    "            x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "                        perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "                        predicate_index = sample_batched[\"predicate_index\"],\n",
    "                        pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "                        predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])   \n",
    "\n",
    "\n",
    "            b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "            loss = nll_loss(logSotfMax(x),gt)\n",
    "            total_loss = total_loss + loss\n",
    "            #-------------------------RESULT STORING----------------------------------\n",
    "            predicted = torch.argmax(x, dim=1)\n",
    "            p += predicted.tolist()\n",
    "            g += gt.tolist()\n",
    "            counter += 1 \n",
    "    \n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    avg_eval_loss = total_loss/counter\n",
    "\n",
    "    if avg_eval_loss < max_val_loss:\n",
    "        max_val_loss = avg_eval_loss\n",
    "    else :\n",
    "        patience_counter += 1\n",
    "    \n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    if patience_counter >= patience :  \n",
    "\n",
    "\n",
    "        print(\"Early stopping at epoch : \",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 avg eval :\",f1_avg)\n",
    "        break\n",
    "    else :\n",
    "        print(\"EPOCHS :\",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 avg eval :\",f1_avg)\n",
    "    \n",
    "\n",
    "    writer.add_scalar(\"Loss_ES/validation\", avg_eval_loss, epoch)\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"Eval_ES/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"Eval_ES/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if f1_avg > f1_score_max:\n",
    "        f1_score_max = f1_avg\n",
    "        print(\"SAVED :\",PATH)\n",
    "        torch.save(model.state_dict(),PATH)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('nlp2022-hw3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ea0fd027ae2f380e05c4e3293ede0f85a80d4d466dd9309da3a748502c958571"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
