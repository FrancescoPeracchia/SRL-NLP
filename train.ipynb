{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT and TOKENIZER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model class is      : <class 'transformers.models.bert.modeling_bert.BertModel'>\n",
      "\n",
      "model class is      : <class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel,AutoTokenizer\n",
    "\n",
    "auto_model = AutoModel.from_pretrained(\"bert-base-cased\",output_hidden_states=True)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nmodel class is      : {type(auto_model)}\")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "print(f\"\\nmodel class is      : {type(tokenizer)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Role Labelling Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import random\n",
    "from typing import Dict\n",
    "\n",
    "class SRL(Dataset):\n",
    " \n",
    "    def __init__(self,language,path) -> None:\n",
    "\n",
    "        self.path_root = 'data'\n",
    "        self.load_data(language,path)\n",
    "        self.args_roles,self.list_broken_id = self.list_arg_roles()\n",
    "        self.pos_list,_ = self.list_pos()\n",
    "        self.predicate_dis,_ = self.list_predicate_roles()\n",
    "        self.pos_list.append(\"Nothing\")\n",
    "        self.predicate_dis.append(\"Nothing\")\n",
    "\n",
    "    def load_data(self,language,mode):\n",
    "        \n",
    "        mode = mode+\".json\"\n",
    "        path = os.path.join(self.path_root,language,mode)\n",
    "        data_file = open(path)\n",
    "       \n",
    "        data_ = json.load(data_file)\n",
    "\n",
    "        list_data = []\n",
    "\n",
    "        for data in data_:\n",
    "            list_data.append(data_[data])\n",
    "        \n",
    "\n",
    "        self.data = list_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, id : int):\n",
    "\n",
    "        flag = False\n",
    "        if id in self.list_broken_id :\n",
    "            flag = True\n",
    "            while flag == True:\n",
    "\n",
    "                rand_id = random.randint(0, len(self.data))\n",
    "                \n",
    "                if rand_id in self.list_broken_id :\n",
    "                    pass\n",
    "                else :\n",
    "                    flag = False\n",
    "                    id = rand_id        \n",
    "\n",
    "\n",
    "        data = self.pre_processing(self.data[id])\n",
    "        data = self.processig(data)\n",
    "        return data\n",
    "        \n",
    "    def pre_processing(self, data:dict):\n",
    "        data_list = []\n",
    "        for role in data[\"roles\"]:\n",
    "            dictionary = dict()\n",
    "            dictionary[\"words\"] = data[\"words\"]\n",
    "            dictionary[\"role\"] = data[\"roles\"][role]\n",
    "            dictionary[\"pre_idx\"] = role\n",
    "            dictionary[\"pos_tags\"] = data[\"pos_tags\"]\n",
    "            dictionary[\"predicate_meaning\"] = data[\"predicates\"]\n",
    "            data_list.append(dictionary)    \n",
    "        return data_list\n",
    "    \n",
    "    def processig(self,data_list:list):\n",
    "        \n",
    "        for dictionary in data_list:\n",
    "\n",
    "            #dictionary[\"words\"] = data[\"words\"]\n",
    "            dictionary[\"gt_arg_identification\"] = self.arg_id(dictionary[\"role\"])\n",
    "            dictionary[\"gt_arg_classification\"] = self.arg_class(dictionary[\"role\"])\n",
    "            dictionary[\"pos_idx\"] = self.pos_idx(dictionary[\"pos_tags\"])\n",
    "            dictionary[\"predicate_meaning_idx\"] = self.predicate_meaning_idx(dictionary[\"predicate_meaning\"])\n",
    "        \n",
    "        return data_list\n",
    "   \n",
    "    def list_arg_roles(self):\n",
    "        list_roles = []\n",
    "        list_broken_id = []\n",
    "        for i,element in enumerate(self.data):\n",
    "            flag = True\n",
    "            try : roles = element[\"roles\"]\n",
    "            except : flag = False\n",
    "            if flag :\n",
    "                for e in roles:\n",
    "                    sentence = element[\"roles\"][e]\n",
    "\n",
    "                    for word in sentence:\n",
    "                        \n",
    "                        list_roles.append(word)\n",
    "                list_roles = list(set(list_roles))\n",
    "            else : \n",
    "                list_broken_id.append(i)\n",
    "        return list_roles,list_broken_id\n",
    "\n",
    "    def list_predicate_roles(self):\n",
    "        list_predicate_roles = []\n",
    "        list_broken_id = []\n",
    "        for i,element in enumerate(self.data):\n",
    "            flag = True\n",
    "            try : predicates = element[\"predicates\"]\n",
    "            except : flag = False\n",
    "            if flag :\n",
    "                for pre in predicates:\n",
    "                    list_predicate_roles.append(pre)\n",
    "                list_predicate_roles = list(set(list_predicate_roles))\n",
    "            else : \n",
    "                list_broken_id.append(i)\n",
    "        return list_predicate_roles,list_broken_id\n",
    "\n",
    "    def list_pos(self):\n",
    "        list_pos = []\n",
    "        list_broken_id = []\n",
    "        for i,element in enumerate(self.data):\n",
    "            flag = True\n",
    "            try : pos = element[\"pos_tags\"]\n",
    "            except : flag = False\n",
    "            if flag :\n",
    "                for e in pos:\n",
    "                    list_pos.append(e)\n",
    "                list_pos = list(set(list_pos))\n",
    "            else : \n",
    "                list_broken_id.append(i)\n",
    "        return list_pos,list_broken_id\n",
    "  \n",
    "    def arg_class(self,role:list):\n",
    "        list_idxs = []\n",
    "        for element in role:\n",
    "            list_idxs.append(self.args_roles.index(element))\n",
    "        \n",
    "\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "\n",
    "    def arg_id(self,role:dict):\n",
    "        list_idxs = []\n",
    "        for element in role:\n",
    "            if element == \"_\":\n",
    "                list_idxs.append(0)\n",
    "            else :\n",
    "                list_idxs.append(1)\n",
    "\n",
    "        \n",
    "\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "\n",
    "    def pos_idx(self,pos_tags:dict):\n",
    "        list_idxs = []\n",
    "        list_idxs.append(self.pos_list.index(\"Nothing\"))\n",
    "\n",
    "        for element in pos_tags:\n",
    "            list_idxs.append(self.pos_list.index(element))\n",
    "        \n",
    "        list_idxs.append(self.pos_list.index(\"Nothing\"))\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "    \n",
    "    def predicate_meaning_idx(self,predicate_meaning_tags:dict):\n",
    "        list_idxs = []\n",
    "        list_idxs.append(self.predicate_dis.index(\"Nothing\"))\n",
    "\n",
    "        for element in predicate_meaning_tags:\n",
    "            list_idxs.append(self.predicate_dis.index(element))\n",
    "        \n",
    "        list_idxs.append(self.predicate_dis.index(\"Nothing\"))\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64) \n",
    "\n",
    "    \n",
    "# here we define our collate function\n",
    "def collate_fn(batch) -> Dict[str, torch.Tensor]:\n",
    "    #print(batch)\n",
    "    input = dict() \n",
    "    batch_sentence = [] \n",
    "    #print(len(batch))\n",
    "    for period in batch:\n",
    "        for sentence in period :\n",
    "        \n",
    "            #print(len(sentence[0][\"words\"]))\n",
    "            pre_idx = int(sentence[\"pre_idx\"])\n",
    "            \n",
    "\n",
    "            predicate = sentence[\"words\"][pre_idx]\n",
    "\n",
    "            text = \" \".join(sentence[\"words\"])\n",
    "            tokens: list[str] = text.split()\n",
    "            predicate: list[str] = predicate.split()\n",
    "\n",
    "            #text = sentence[0][\"words\"]\n",
    "            \n",
    "            t = (tokens,predicate)\n",
    "\n",
    "            batch_sentence.append(t)\n",
    "            #print(batch_sentence)\n",
    "\n",
    "    batch_output = tokenizer.batch_encode_plus(batch_sentence,padding=True,is_split_into_words=True, truncation=True,return_offsets_mapping=True, return_tensors=\"pt\")\n",
    "    #print(batch_output.keys())\n",
    "\n",
    "\n",
    "    gt = dict()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    for period in batch:\n",
    "\n",
    "        list_positional_predicate_encoding = []\n",
    "        list_arg_gt = []\n",
    "        list_predicate_index = [] \n",
    "        list_pos_index = [] \n",
    "        list_predicate_meaning_index = []\n",
    "\n",
    "        for sentence in period:\n",
    "            #positional_encoding\n",
    "            #+2 per il CLS iniziale ad SEP finale\n",
    "            sentence_words_lenght =  len(sentence[\"words\"])\n",
    "            positional_predicate_encoding = torch.zeros(1,sentence_words_lenght+2)\n",
    "            #+1 per il CLS iniziale\n",
    "            pre_idx = int(sentence[\"pre_idx\"])\n",
    "            positional_predicate_encoding[:,pre_idx+1] = 1\n",
    "            list_positional_predicate_encoding.append(positional_predicate_encoding)\n",
    "            #print(\"positional_prefix_encoding\",positional_predicate_encoding)\n",
    "            list_predicate_index.append(pre_idx)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            pos = torch.unsqueeze(sentence[\"pos_idx\"],dim = 0)\n",
    "            list_pos_index.append(pos)\n",
    "            predicate_meaning_idxs = torch.unsqueeze(sentence[\"predicate_meaning_idx\"],dim = 0)\n",
    "            list_predicate_meaning_index.append(predicate_meaning_idxs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #note CLS and SEP are discharder after Bi-LSTM, the Classifier takes in input only wokrds hidden state embedding\n",
    "            arg_gt = torch.unsqueeze(sentence[\"gt_arg_classification\"],dim = 0)\n",
    "            list_arg_gt.append(arg_gt)\n",
    "        \n",
    "\n",
    "    list_arg_gt = torch.cat(list_arg_gt,dim = 0)\n",
    "    list_pos_index = torch.cat(list_pos_index,dim = 0)\n",
    "    list_predicate_meaning_index = torch.cat(list_predicate_meaning_index,dim = 0)\n",
    "    list_positional_predicate_encoding = torch.cat(list_positional_predicate_encoding,dim = 0)\n",
    "    gt[\"arg_gt\"] = list_arg_gt\n",
    "    input[\"predicate_index\"] = list_predicate_index\n",
    "    input[\"pos_index\"] = list_pos_index.long()\n",
    "    input[\"predicate_meaning_idx\"] = list_predicate_meaning_index.long()\n",
    "    offset = batch_output.pop(\"offset_mapping\")\n",
    "    input[\"BERT_input\"] = batch_output\n",
    "    input[\"positional_encoding\"] = list_positional_predicate_encoding.long()\n",
    "    input[\"offset_mapping\"] = offset\n",
    "    input[\"gt\"] = gt\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    return input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SRL(\"EN\",\"train\")\n",
    "dev_dataset = SRL(\"EN\",\"dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "embeddings = dict()\n",
    "\n",
    "embeddings[\"predicate_flag_embedding_output_dim\"] = 32\n",
    "#defined in initial exploration of the dataset\n",
    "embeddings[\"pos_embedding_input_dim\"] = 0\n",
    "embeddings[\"pos_embedding_output_dim\"] = 100\n",
    "#-------------------------------------------------\n",
    "embeddings[\"predicate_embedding_input_dim\"] = 0\n",
    "embeddings[\"predicate_embedding_output_dim\"] = False\n",
    "#defined in initial exploration of the dataset\n",
    "n_classes = 0\n",
    "\n",
    "\n",
    "\n",
    "bilstm = dict()\n",
    "bilstm[\"n_layers\"] = 2\n",
    "bilstm[\"output_dim\"] = 50\n",
    "dropouts = [0.4,0.3,0.3]\n",
    "\n",
    "language_portable = True\n",
    "predicate_meaning = True\n",
    "pos = True\n",
    "\n",
    "cfg = dict()\n",
    "cfg[\"embeddings\"] = embeddings\n",
    "cfg[\"n_classes\"] = n_classes\n",
    "cfg[\"bilstm\"] = bilstm\n",
    "cfg[\"language_portable\"] = language_portable\n",
    "cfg[\"dropouts\"] = dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arg_Classifier(\n",
      "  (bi_lstm_portable): LSTM(132, 200, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (embedding_predicate_flag): Embedding(2, 32, max_norm=True)\n",
      "  (embedding_predicate): Embedding(304, False, max_norm=True)\n",
      "  (embedding_pos): Embedding(18, 100, max_norm=True)\n",
      "  (bi_lstm): LSTM(900, 200, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (dropout_in_classifier): Dropout(p=0.4, inplace=False)\n",
      "  (Relu): ReLU()\n",
      "  (Sigmoid): Sigmoid()\n",
      "  (linear0): Linear(in_features=1200, out_features=675, bias=True)\n",
      "  (linear1): Linear(in_features=675, out_features=135, bias=True)\n",
      "  (linear2): Linear(in_features=135, out_features=27, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#from mmcv import Config\n",
    "from hw2.stud.arg import Arg_Classifier \n",
    "\n",
    "\n",
    "\n",
    "#cfg = Config.fromfile('/home/francesco/Desktop/nlp2022-hw2-main/hw2/stud/configs/model.py')\n",
    "\n",
    "cfg[\"embeddings\"][\"pos_embedding_input_dim\"] = len(train_dataset.pos_list)\n",
    "cfg[\"embeddings\"][\"predicate_embedding_input_dim\"] = len(train_dataset.predicate_dis)\n",
    "cfg[\"n_classes\"] = len(train_dataset.args_roles)\n",
    "\n",
    "\n",
    "model = Arg_Classifier(\"EN\",cfg).cuda()\n",
    "print(model)\n",
    "\n",
    "automodel = auto_model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Argument Identification and Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS 0\n",
      "F1 eval: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.97899419 0.020558   0.         0.         0.         0.\n",
      " 0.         0.         0.53513717 0.         0.23474178 0.\n",
      " 0.         0.21026895 0.        ]\n",
      "F1 eval: 0.9399024836010591\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "\n",
    "logSotfMax = torch.nn.LogSoftmax(dim=1)\n",
    "nll_loss = torch.nn.NLLLoss()\n",
    "\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "dataloader_dev = DataLoader(dev_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "auto_model.eval()\n",
    "\n",
    "EPOCHS = 200\n",
    "patience_counter = 0\n",
    "patience = 5\n",
    "max_val_loss = 9999\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    #TRAINING\n",
    "    p = []\n",
    "    g = []\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_train):\n",
    "        #print(sample_batched)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "       \n",
    "        #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "        #------------------FILTERING SUB-WORDS----------------------\n",
    "        subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "        word_emebedding = []\n",
    "        for i in range(n): \n",
    "            subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "            flag = subtoken_mask[0,i,0]\n",
    "            if flag :\n",
    "                continue\n",
    "            else :\n",
    "                word_emebedding.append(subwords_embedding)\n",
    "        word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "        #-------------------------FORWARD/BACKWARD----------------------------------\n",
    "        x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "            perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "            predicate_index = sample_batched[\"predicate_index\"],\n",
    "            pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "            predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])        \n",
    "        b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "        loss = nll_loss(logSotfMax(x),gt)\n",
    "        total_loss = total_loss + loss\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "        counter += 1 \n",
    "            \n",
    "\n",
    "        #-------------------------RESULT STORING----------------------------------\n",
    "        predicted = torch.argmax(x, dim=1)\n",
    "        p += predicted.tolist()\n",
    "        g += gt.tolist()\n",
    "    \n",
    "\n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    #print(\"Epochs n.\", epoch)\n",
    "    #print(\"F1 train:\",f1_score(g, p, average=None))\n",
    "    scheduler.step()\n",
    "    avg_train_loss = total_loss/counter\n",
    "    writer.add_scalar(\"Loss/train\", avg_train_loss, epoch)\n",
    "\n",
    "\n",
    "\n",
    "    #EVALUATION\n",
    "    p = []\n",
    "    g = []\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_dev):\n",
    "    \n",
    "      #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "        #------------------FILTERING SUB-WORDS----------------------\n",
    "        subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "        word_emebedding = []\n",
    "        for i in range(n): \n",
    "            subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "            flag = subtoken_mask[0,i,0]\n",
    "            if flag :\n",
    "                continue\n",
    "            else :\n",
    "                word_emebedding.append(subwords_embedding)\n",
    "        word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "        #-------------------------FORWARD----------------------------------\n",
    "        x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "                    perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "                    predicate_index = sample_batched[\"predicate_index\"],\n",
    "                    pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "                    predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])   \n",
    "\n",
    "\n",
    "        b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "        loss = nll_loss(logSotfMax(x),gt)\n",
    "        total_loss = total_loss + loss\n",
    "        #-------------------------RESULT STORING----------------------------------\n",
    "        predicted = torch.argmax(x, dim=1)\n",
    "        p += predicted.tolist()\n",
    "        g += gt.tolist()\n",
    "        counter += 1 \n",
    "    \n",
    "    #-------------------------RESULTS----------------------------------\n",
    "\n",
    "    avg_eval_loss = total_loss/counter\n",
    "\n",
    "    if avg_eval_loss < max_val_loss:\n",
    "        max_val_loss = avg_eval_loss\n",
    "    else :\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= patience :  \n",
    "\n",
    "        print(\"F1 eval:\",f1_score(g, p, average=None))\n",
    "        print(\"F1 eval:\",f1_score(g, p, average=\"weighted\"))\n",
    "        break\n",
    "    else :\n",
    "        print(\"EPOCHS\",epoch)\n",
    "        print(\"F1 eval:\",f1_score(g, p, average=None))\n",
    "        print(\"F1 eval:\",f1_score(g, p, average=\"weighted\"))\n",
    "    \n",
    "\n",
    "    writer.add_scalar(\"Loss/validation\", avg_eval_loss, epoch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 eval: [0.         0.         0.         0.2        0.33112583 0.\n",
      " 0.59271804 0.14492754 0.         0.18867925 0.         0.\n",
      " 0.03846154 0.10526316 0.         0.17582418 0.58670989 0.47942755\n",
      " 0.56698742 0.74213836 0.16949153 0.54212454 0.         0.14285714\n",
      " 0.99005174 0.         0.20224719]\n",
      "F1 eval: 0.9690091993467825\n"
     ]
    }
   ],
   "source": [
    "#EVALUATION\n",
    "p = []\n",
    "g = []\n",
    "model.eval()\n",
    "for i_batch, sample_batched in enumerate(dataloader_dev):\n",
    "\n",
    "    #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "    input_bert = sample_batched[\"BERT_input\"]\n",
    "    input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "    input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "    input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "    sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "    sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "    #prepare gt\n",
    "    gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "    offset = sample_batched[\"offset_mapping\"]\n",
    "    #-----------------BERT EMBEDDING---------------------------\n",
    "    with torch.no_grad():\n",
    "        output = auto_model(**input_bert)\n",
    "        output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "        b,n,h = output_hidden_states_sum.size()\n",
    "\n",
    "    #------------------FILTERING SUB-WORDS----------------------\n",
    "    subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "    word_emebedding = []\n",
    "    for i in range(n): \n",
    "        subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "        flag = subtoken_mask[0,i,0]\n",
    "        if flag :\n",
    "            continue\n",
    "        else :\n",
    "            word_emebedding.append(subwords_embedding)\n",
    "    word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "    #-------------------------FORWARD----------------------------------\n",
    "    x = model.forward(subwords_embeddings = output_hidden_states_sum,perdicate_positional_encoding = sample_batched[\"positional_encoding\"], predicate_index = sample_batched[\"predicate_index\"],pos_index_encoding = sample_batched[\"pos_index\"])        \n",
    "    b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "    #-------------------------RESULT STORING----------------------------------\n",
    "    predicted = torch.argmax(x, dim=1)\n",
    "    p += predicted.tolist()\n",
    "    g += gt.tolist()\n",
    "\n",
    "#-------------------------RESULTS----------------------------------\n",
    "\n",
    "print(\"F1 eval:\",f1_score(g, p, average=None))\n",
    "print(\"F1 eval:\",f1_score(g, p, average=\"weighted\"))\n",
    "#convert index to string \n",
    "mapping = dataloader_train.dataset.args_roles\n",
    "#filter \"_\" from evaluation\n",
    "\n",
    "\n",
    "gt = [mapping[elem] for elem in g]\n",
    "predictions = [mapping[elem] for elem in p]\n",
    "\n",
    "\n",
    "positions = [elem != \"_\" for elem in gt]\n",
    "gt_filtered = []\n",
    "pre_filtered = []\n",
    "for i,elem in enumerate(gt):\n",
    "    if i in positions:\n",
    "        gt_filtered.append(elem)\n",
    "        pre_filtered.append(predictions[i])\n",
    "\n",
    "\n",
    "v = confusion_matrix(gt, predictions, labels=mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     1     0     0     1     0     0     0\n",
      "      1     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      6     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     1     0     0\n",
      "      0     0     0     0     0     0     1     0     0     0     0     0\n",
      "      2     0     0]\n",
      " [    0     0     0     2     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     3     1     0     0     0     0\n",
      "      7     0     0]\n",
      " [    0     0     0     0    25     0    14     2     0     1     0     0\n",
      "      0     0     0     0     1     8    12     0     0     0     0     6\n",
      "     28     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     1     0     0\n",
      "      0     0     0     0     0     1     1     1     0     0     0     0\n",
      "     12     0     0]\n",
      " [    0     0     0     0     5     0   350     0     0     3     0     0\n",
      "      1     2     0     6     9     3   130    10     1     1     0    14\n",
      "    114     0     1]\n",
      " [    0     0     0     0     0     0     0     5     3     1     0     0\n",
      "      1     0     0     0     2     0    13     3     1     0     0     0\n",
      "     17     0     0]\n",
      " [    1     0     0     1     1     0     0     0     0     2     0     0\n",
      "      3     0     0     0     0     0     2     1     2     0     0     0\n",
      "     25     0     0]\n",
      " [    0     0     0     0     0     0     3     0     0    10     0     0\n",
      "      0     0     0     0     2     4    17     0     0     0     0     0\n",
      "     19     0     0]\n",
      " [    0     0     0     0     0     0     2     0     0     1     0     0\n",
      "      0     0     0     0     0     0     2     1     0     0     0     1\n",
      "      3     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      1     0     0]\n",
      " [    0     0     0     0     0     0     0     0     1     2     0     0\n",
      "      1     0     0     0     0     1     1     2     0     0     0     0\n",
      "     21     0     0]\n",
      " [    0     0     0     0     0     0     9     0     0     0     0     0\n",
      "      0     2     0     0     0     0    12     1     0     1     0     0\n",
      "      9     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     2     2     0     0     0     0\n",
      "      3     0     0]\n",
      " [    0     0     0     0     0     0     2     1     0     0     0     0\n",
      "      1     0     0     8     3     1    23     4     0     0     0     2\n",
      "     16     0     0]\n",
      " [    0     0     0     0     0     0    10     2     0     1     0     0\n",
      "      0     0     0     2   181     4    86     6     0     6     0     2\n",
      "     48     0     0]\n",
      " [    0     0     0     1     4     0     9     2     0     4     0     0\n",
      "      2     0     0     1     1   134    17     9     0     2     0     0\n",
      "    142     0     0]\n",
      " [    0     0     0     0    10     0    62     0     0     8     0     0\n",
      "      1     0     0     4    41    12   766    51     2     6     0    26\n",
      "    298     0     2]\n",
      " [    0     0     0     0     0     0     1     6     0     3     0     0\n",
      "      0     0     1     1     2     7    30  1180     1     0     0     2\n",
      "    380     0    10]\n",
      " [    0     0     0     1     2     0     1     1     1     2     0     0\n",
      "      0     0     0     0     2     1     2     0     5     3     0     0\n",
      "     22     0     1]\n",
      " [    0     0     0     0     0     0     1     1     0     0     0     0\n",
      "      0     0     0     0     1     2    19     8     1    74     0     0\n",
      "     54     0     0]\n",
      " [    0     0     0     0     0     0     1     0     0     0     0     0\n",
      "      0     0     0     0     1     0     7     1     0     0     0     0\n",
      "      4     0     0]\n",
      " [    0     0     0     0     0     0    17     0     0     1     0     0\n",
      "      0     0     0     0     3     2    60     3     0     2     0    15\n",
      "     32     0     0]\n",
      " [    0     0     0     2     7     0    46     3    12    10     0     0\n",
      "     13     0     0     6    20    50   201   252     1    17     2     6\n",
      "  96435     0     3]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      1     0     0]\n",
      " [    0     0     0     0     0     0     3     0     0     0     0     0\n",
      "      0     0     0     2     0     0     6    20     0     0     0     1\n",
      "     22     0     9]]\n"
     ]
    }
   ],
   "source": [
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cycler'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix, ConfusionMatrixDisplay\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m disp \u001b[38;5;241m=\u001b[39m ConfusionMatrixDisplay(confusion_matrix\u001b[38;5;241m=\u001b[39mcm,display_labels\u001b[38;5;241m=\u001b[39mmapping)\n\u001b[1;32m      4\u001b[0m disp\u001b[38;5;241m.\u001b[39mplot()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/matplotlib/__init__.py:113\u001b[0m\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.9/site-packages/matplotlib/__init__.py?line=108'>109</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpackaging\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mversion\u001b[39;00m \u001b[39mimport\u001b[39;00m parse \u001b[39mas\u001b[39;00m parse_version\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.9/site-packages/matplotlib/__init__.py?line=110'>111</a>\u001b[0m \u001b[39m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.9/site-packages/matplotlib/__init__.py?line=111'>112</a>\u001b[0m \u001b[39m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///~/.local/lib/python3.9/site-packages/matplotlib/__init__.py?line=112'>113</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _api, _version, cbook, _docstring, rcsetup\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.9/site-packages/matplotlib/__init__.py?line=113'>114</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcbook\u001b[39;00m \u001b[39mimport\u001b[39;00m sanitize_sequence\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.9/site-packages/matplotlib/__init__.py?line=114'>115</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m \u001b[39mimport\u001b[39;00m MatplotlibDeprecationWarning\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/matplotlib/rcsetup.py:32\u001b[0m\n\u001b[1;32m     <a href='file:///~/.local/lib/python3.9/site-packages/matplotlib/rcsetup.py?line=28'>29</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_enums\u001b[39;00m \u001b[39mimport\u001b[39;00m JoinStyle, CapStyle\n\u001b[1;32m     <a href='file:///~/.local/lib/python3.9/site-packages/matplotlib/rcsetup.py?line=30'>31</a>\u001b[0m \u001b[39m# Don't let the original cycler collide with our validating cycler\u001b[39;00m\n\u001b[0;32m---> <a href='file:///~/.local/lib/python3.9/site-packages/matplotlib/rcsetup.py?line=31'>32</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcycler\u001b[39;00m \u001b[39mimport\u001b[39;00m Cycler, cycler \u001b[39mas\u001b[39;00m ccycler\n\u001b[1;32m     <a href='file:///~/.local/lib/python3.9/site-packages/matplotlib/rcsetup.py?line=34'>35</a>\u001b[0m \u001b[39m# The capitalized forms are needed for ipython at present; this may\u001b[39;00m\n\u001b[1;32m     <a href='file:///~/.local/lib/python3.9/site-packages/matplotlib/rcsetup.py?line=35'>36</a>\u001b[0m \u001b[39m# change for later versions.\u001b[39;00m\n\u001b[1;32m     <a href='file:///~/.local/lib/python3.9/site-packages/matplotlib/rcsetup.py?line=36'>37</a>\u001b[0m interactive_bk \u001b[39m=\u001b[39m [\n\u001b[1;32m     <a href='file:///~/.local/lib/python3.9/site-packages/matplotlib/rcsetup.py?line=37'>38</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mGTK3Agg\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mGTK3Cairo\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mGTK4Agg\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mGTK4Cairo\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='file:///~/.local/lib/python3.9/site-packages/matplotlib/rcsetup.py?line=38'>39</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mMacOSX\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///~/.local/lib/python3.9/site-packages/matplotlib/rcsetup.py?line=43'>44</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mWX\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mWXAgg\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mWXCairo\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='file:///~/.local/lib/python3.9/site-packages/matplotlib/rcsetup.py?line=44'>45</a>\u001b[0m ]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cycler'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=mapping)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "# we will use with Distil-BERT\n",
    "language_model_name = \"distilbert-base-uncased\"\n",
    "# this GPU should be enough for this task to handle 32 samples per batch\n",
    "batch_size = 32\n",
    "# we keep num_workers = min(4 * number of GPUs, number of cores)\n",
    "# tells the data loader how many sub-processes to use for data loading\n",
    "num_workers = 2\n",
    "# optim\n",
    "learning_rate = 2e-4\n",
    "weight_decay = 0.0\n",
    "transformer_learning_rate = 1e-5\n",
    "transformer_weight_decay = 0.0\n",
    "# training\n",
    "epochs = 3\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# load our dataset\n",
    "ner_dataset = load_dataset(\"conll2003\")\n",
    "\n",
    "# let's instantiate the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(language_model_name)\n",
    "\n",
    "# here we define a vocab dict to map the labels to int (and vice versa)\n",
    "label_list = ner_dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "label_to_id = {n: i for i, n in enumerate(ner_dataset[\"train\"].features[\"ner_tags\"].feature.names)}\n",
    "id_to_label = {i: n for n, i in label_to_id.items()}\n",
    "\n",
    "# here we define our collate function\n",
    "def collate_fn(batch) -> Dict[str, torch.Tensor]:\n",
    "    batch_out = tokenizer(\n",
    "        [sentence[\"tokens\"] for sentence in batch],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        # We use this argument because the texts in our dataset are lists of words.\n",
    "        is_split_into_words=True,\n",
    "    )\n",
    "    labels = []\n",
    "    ner_tags = [sentence[\"ner_tags\"] for sentence in batch]\n",
    "    for i, label in enumerate(ner_tags):\n",
    "      # obtains the word_ids of the i-th sentence\n",
    "      word_ids = batch_out.word_ids(batch_index=i)\n",
    "      previous_word_idx = None\n",
    "      label_ids = []\n",
    "      for word_idx in word_ids:\n",
    "        # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "        # ignored in the loss function.\n",
    "        if word_idx is None:\n",
    "          label_ids.append(-100)\n",
    "        # We set the label for the first token of each word.\n",
    "        elif word_idx != previous_word_idx:\n",
    "          label_ids.append(label[word_idx])\n",
    "        # For the other tokens in a word, we set the label to -100 so they are automatically\n",
    "        # ignored in the loss function.\n",
    "        else:\n",
    "          label_ids.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "      labels.append(label_ids)\n",
    "    \n",
    "    # pad the labels with -100\n",
    "    batch_max_length = len(max(labels, key=len))\n",
    "    labels = [l + ([-100] * abs(batch_max_length - len(l))) for l in labels]\n",
    "    batch_out[\"labels\"] = torch.as_tensor(labels)\n",
    "    return batch_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('nlp2022-hw2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "396663868a68a40d4aad1f83db3cf05f3bd24b2a6e9304aa6d34e745f3941ef3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
