{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model class is      : <class 'transformers.models.bert.modeling_bert.BertModel'>\n",
      "\n",
      "model class is      : <class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel,AutoTokenizer\n",
    "import torch\n",
    "auto_model = AutoModel.from_pretrained(\"bert-base-cased\",output_hidden_states=True)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nmodel class is      : {type(auto_model)}\")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "print(f\"\\nmodel class is      : {type(tokenizer)}\")\n",
    "\n",
    "\n",
    "torch.save(auto_model,\"hw2/stud/saved/bert.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert and Tokenizer for English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import random\n",
    "from typing import Dict\n",
    "\n",
    "class SRL(Dataset):\n",
    " \n",
    "    def __init__(self,language,tokenizer,path,args_roles = None,pos_list = None,predicate_dis = None) -> None:\n",
    "        #train\n",
    "        #self.path_root = 'data'\n",
    "        #inference \n",
    "        self.path_root = 'hw2/stud/data'\n",
    "        #self.path_root = 'stud/data'\n",
    "        self.load_data(language,path)\n",
    "        if args_roles is None :\n",
    "            self.args_roles,self.list_broken_id = self.list_arg_roles()\n",
    "            self.args_roles.append(\"UNK\")\n",
    "        else : \n",
    "            self.args_roles = args_roles\n",
    "            _,self.list_broken_id = self.list_arg_roles()\n",
    "        \n",
    "\n",
    "        if pos_list is None :\n",
    "            self.pos_list,_ = self.list_pos()\n",
    "            self.pos_list.append(\"Nothing\")\n",
    "            self.pos_list.append(\"UNK\")\n",
    "        else : \n",
    "            self.pos_list = pos_list\n",
    "        \n",
    "\n",
    "\n",
    "        if predicate_dis is None :\n",
    "            self.predicate_dis,_ = self.list_predicate_roles()\n",
    "            self.predicate_dis.append(\"Nothing\")\n",
    "            self.predicate_dis.append(\"UNK\")\n",
    "        else : \n",
    "            self.predicate_dis = predicate_dis\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def load_data(self,language,mode):\n",
    "        \n",
    "        mode = mode+\".json\"\n",
    "        path = os.path.join(self.path_root,language,mode)\n",
    "        data_file = open(path)\n",
    "       \n",
    "        data_ = json.load(data_file)\n",
    "\n",
    "        list_data = []\n",
    "\n",
    "        for data in data_:\n",
    "            list_data.append(data_[data])\n",
    "        \n",
    "\n",
    "        self.data = list_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, id : int):\n",
    "\n",
    "        flag = False\n",
    "        if id in self.list_broken_id :\n",
    "            flag = True\n",
    "            while flag == True:\n",
    "\n",
    "                rand_id = random.randint(0, len(self.data)-1)\n",
    "                \n",
    "                if rand_id in self.list_broken_id :\n",
    "                    pass\n",
    "                else :\n",
    "                    flag = False\n",
    "                    id = rand_id        \n",
    "\n",
    "\n",
    "        data = self.pre_processing(self.data[id])\n",
    "        data = self.processig(data)\n",
    "        return data\n",
    "        \n",
    "    def pre_processing(self, data:dict):\n",
    "        data_list = []\n",
    "        for role in data[\"roles\"]:\n",
    "            dictionary = dict()\n",
    "            dictionary[\"words\"] = data[\"words\"]\n",
    "            dictionary[\"role\"] = data[\"roles\"][role]\n",
    "            dictionary[\"pre_idx\"] = role\n",
    "            dictionary[\"pos_tags\"] = data[\"pos_tags\"]\n",
    "            dictionary[\"predicate_meaning\"] = data[\"predicates\"]\n",
    "            data_list.append(dictionary)    \n",
    "        return data_list\n",
    "    \n",
    "    def processig(self,data_list:list):\n",
    "        \n",
    "        for dictionary in data_list:\n",
    "\n",
    "            #dictionary[\"words\"] = data[\"words\"]\n",
    "            dictionary[\"gt_arg_identification\"] = self.arg_id(dictionary[\"role\"])\n",
    "            dictionary[\"gt_arg_classification\"] = self.arg_class(dictionary[\"role\"])\n",
    "            dictionary[\"pos_idx\"] = self.pos_idx(dictionary[\"pos_tags\"])\n",
    "            dictionary[\"predicate_meaning_idx\"] = self.predicate_meaning_idx(dictionary[\"predicate_meaning\"])\n",
    "        \n",
    "        return data_list\n",
    "   \n",
    "    def list_arg_roles(self):\n",
    "        list_roles = []\n",
    "        list_broken_id = []\n",
    "        for i,element in enumerate(self.data):\n",
    "            flag = True\n",
    "            try : roles = element[\"roles\"]\n",
    "            except : flag = False\n",
    "            if flag :\n",
    "                for e in roles:\n",
    "                    sentence = element[\"roles\"][e]\n",
    "\n",
    "                    for word in sentence:\n",
    "                        \n",
    "                        list_roles.append(word)\n",
    "                list_roles = list(set(list_roles))\n",
    "            else : \n",
    "                list_broken_id.append(i)\n",
    "        return list_roles,list_broken_id\n",
    "\n",
    "    def list_predicate_roles(self):\n",
    "        list_predicate_roles = []\n",
    "        list_broken_id = []\n",
    "        for i,element in enumerate(self.data):\n",
    "            flag = True\n",
    "            try : predicates = element[\"predicates\"]\n",
    "            except : flag = False\n",
    "            if flag :\n",
    "                for pre in predicates:\n",
    "                    list_predicate_roles.append(pre)\n",
    "                list_predicate_roles = list(set(list_predicate_roles))\n",
    "            else : \n",
    "                list_broken_id.append(i)\n",
    "        return list_predicate_roles,list_broken_id\n",
    "\n",
    "    def list_pos(self):\n",
    "        list_pos = []\n",
    "        list_broken_id = []\n",
    "        for i,element in enumerate(self.data):\n",
    "            flag = True\n",
    "            try : pos = element[\"pos_tags\"]\n",
    "            except : flag = False\n",
    "            if flag :\n",
    "                for e in pos:\n",
    "                    list_pos.append(e)\n",
    "                list_pos = list(set(list_pos))\n",
    "            else : \n",
    "                list_broken_id.append(i)\n",
    "        return list_pos,list_broken_id\n",
    "  \n",
    "    def arg_class(self,role:list):\n",
    "        list_idxs = []\n",
    "        for element in role:\n",
    "            try : list_idxs.append(self.args_roles.index(element))\n",
    "            except : list_idxs.append(self.args_roles.index(\"UNK\"))\n",
    "        \n",
    "\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "\n",
    "    def arg_id(self,role:dict):\n",
    "        list_idxs = []\n",
    "        for element in role:\n",
    "            if element == \"_\":\n",
    "                list_idxs.append(0)\n",
    "            else :\n",
    "                list_idxs.append(1)\n",
    "\n",
    "        \n",
    "\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "\n",
    "    def pos_idx(self,pos_tags:dict):\n",
    "        list_idxs = []\n",
    "        list_idxs.append(self.pos_list.index(\"Nothing\"))\n",
    "\n",
    "        for element in pos_tags:\n",
    "            try :list_idxs.append(self.pos_list.index(element))\n",
    "            except :list_idxs.append(self.pos_list.index(\"UNK\"))\n",
    "        \n",
    "        list_idxs.append(self.pos_list.index(\"Nothing\"))\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "    \n",
    "    def predicate_meaning_idx(self,predicate_meaning_tags:dict):\n",
    "        list_idxs = []\n",
    "        list_idxs.append(self.predicate_dis.index(\"Nothing\"))\n",
    "\n",
    "        for element in predicate_meaning_tags:\n",
    "            try : list_idxs.append(self.predicate_dis.index(element))\n",
    "            except : list_idxs.append(self.predicate_dis.index(\"UNK\"))\n",
    "            \n",
    "        \n",
    "        list_idxs.append(self.predicate_dis.index(\"Nothing\"))\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "   \n",
    "    def role_gen(self,sentence):\n",
    "\n",
    "        base = [\"_\"]*len(sentence[\"predicates\"])\n",
    "        roles_dict = dict()\n",
    "        counter = 0\n",
    "        for i,item in enumerate(sentence[\"predicates\"]):\n",
    "\n",
    "            if item != \"_\":\n",
    "                base = [\"_\"]*len(sentence[\"predicates\"])\n",
    "                sentence[\"roles\"] = 10\n",
    "                roles_dict[str(i)] = base\n",
    "                counter += 1\n",
    "        \n",
    "        if counter == 0:\n",
    "            sentence[\"roles\"] = { }\n",
    "            flag = False\n",
    "            \n",
    "                \n",
    "        else :\n",
    "            sentence[\"roles\"] = roles_dict\n",
    "            flag = True\n",
    "\n",
    "        return sentence,flag\n",
    "        \n",
    "    def prepare_batch(self,sentence):\n",
    "\n",
    "        sentence,flag = self.role_gen(sentence)\n",
    "        \n",
    "        if flag :\n",
    "\n",
    "            data = self.pre_processing(sentence)\n",
    "            data = self.processig(data)\n",
    "            data = [data]\n",
    "            \n",
    "            \n",
    "            input = dict() \n",
    "            gt = dict()\n",
    "            batch_sentence = [] \n",
    "            \n",
    "            for period in data:\n",
    "                for sentence in period :\n",
    "\n",
    "                    \n",
    "                \n",
    "                    #print(len(sentence[0][\"words\"]))\n",
    "                    pre_idx = int(sentence[\"pre_idx\"])\n",
    "                    \n",
    "\n",
    "                    predicate = sentence[\"words\"][pre_idx]\n",
    "\n",
    "                    text = \" \".join(sentence[\"words\"])\n",
    "                    tokens: list[str] = text.split()\n",
    "                    predicate: list[str] = predicate.split()\n",
    "\n",
    "                    #text = sentence[0][\"words\"]\n",
    "                    \n",
    "                    t = (tokens,predicate)\n",
    "\n",
    "                    batch_sentence.append(t)\n",
    "                \n",
    "                \n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "            batch_output = self.tokenizer.batch_encode_plus(batch_sentence,padding=True,is_split_into_words=True, truncation=True,return_offsets_mapping=True, return_tensors=\"pt\")\n",
    "            \n",
    "\n",
    "\n",
    "            for period in data:\n",
    "\n",
    "                list_positional_predicate_encoding = []\n",
    "                list_predicate_index = [] \n",
    "                list_pos_index = [] \n",
    "                list_arg_gt = []\n",
    "                list_predicate_meaning_index = []\n",
    "                list_meaning_predicate_encoding = []\n",
    "\n",
    "                for sentence in period:\n",
    "                    #positional_encoding\n",
    "                    #+2 per il CLS iniziale ad SEP finale\n",
    "                    sentence_words_lenght =  len(sentence[\"words\"])\n",
    "                    positional_predicate_encoding = torch.zeros(1,sentence_words_lenght+2)\n",
    "                    #+1 per il CLS iniziale\n",
    "                    pre_idx = int(sentence[\"pre_idx\"])\n",
    "                    positional_predicate_encoding[:,pre_idx+1] = 1\n",
    "                    list_positional_predicate_encoding.append(positional_predicate_encoding)\n",
    "                    #print(\"positional_prefix_encoding\",positional_predicate_encoding)\n",
    "                    list_predicate_index.append(pre_idx)\n",
    "\n",
    "                    meaning_predicate_encoding = torch.zeros(1,sentence_words_lenght+2)\n",
    "                    pre_idx = int(sentence[\"pre_idx\"])\n",
    "                    #rather then set the flag 0,1 set with class verb\n",
    "                    meaning_predicate_encoding[:,pre_idx+1] = sentence[\"predicate_meaning_idx\"][pre_idx+1]\n",
    "                    list_meaning_predicate_encoding.append(meaning_predicate_encoding)\n",
    "                    \n",
    "\n",
    "                    pos = torch.unsqueeze(sentence[\"pos_idx\"],dim = 0)\n",
    "                    list_pos_index.append(pos)\n",
    "                    predicate_meaning_idxs = torch.unsqueeze(sentence[\"predicate_meaning_idx\"],dim = 0)\n",
    "                    list_predicate_meaning_index.append(predicate_meaning_idxs)\n",
    "\n",
    "\n",
    "                    arg_gt = torch.unsqueeze(sentence[\"gt_arg_classification\"],dim = 0)\n",
    "                    list_arg_gt.append(arg_gt)\n",
    "\n",
    "\n",
    "            list_arg_gt = torch.cat(list_arg_gt,dim = 0)\n",
    "            list_pos_index = torch.cat(list_pos_index,dim = 0)\n",
    "            list_predicate_meaning_index = torch.cat(list_predicate_meaning_index,dim = 0)\n",
    "            list_positional_predicate_encoding = torch.cat(list_positional_predicate_encoding,dim = 0)\n",
    "            list_predicate_meaning_index_bis = torch.cat(list_meaning_predicate_encoding,dim = 0)\n",
    "            gt[\"arg_gt\"] = list_arg_gt\n",
    "            input[\"predicate_index\"] = list_predicate_index\n",
    "            input[\"pos_index\"] = list_pos_index.long()\n",
    "            input[\"predicate_meaning_idx\"] = list_predicate_meaning_index.long()\n",
    "            input[\"predicate_meaning_idx_bis\"] = list_predicate_meaning_index_bis.long()\n",
    "            offset = batch_output.pop(\"offset_mapping\")\n",
    "            input[\"BERT_input\"] = batch_output\n",
    "            input[\"positional_encoding\"] = list_positional_predicate_encoding.long()\n",
    "            input[\"offset_mapping\"] = offset\n",
    "            input[\"gt\"] = gt\n",
    "        \n",
    "        else :\n",
    "            input = sentence\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return input,flag\n",
    "    \n",
    "# here we define our collate function\n",
    "def collate_fn(batch) -> Dict[str, torch.Tensor]:\n",
    "    #print(batch)\n",
    "    input = dict() \n",
    "    batch_sentence = [] \n",
    "    #print(len(batch))\n",
    "    for period in batch:\n",
    "        for sentence in period :\n",
    "        \n",
    "            #print(len(sentence[0][\"words\"]))\n",
    "            pre_idx = int(sentence[\"pre_idx\"])\n",
    "            \n",
    "\n",
    "            predicate = sentence[\"words\"][pre_idx]\n",
    "\n",
    "            text = \" \".join(sentence[\"words\"])\n",
    "            tokens: list[str] = text.split()\n",
    "            predicate: list[str] = predicate.split()\n",
    "\n",
    "            #text = sentence[0][\"words\"]\n",
    "            \n",
    "            t = (tokens,predicate)\n",
    "\n",
    "            batch_sentence.append(t)\n",
    "            #print(batch_sentence)\n",
    "\n",
    "    batch_output = tokenizer.batch_encode_plus(batch_sentence,padding=True,is_split_into_words=True, truncation=True,return_offsets_mapping=True, return_tensors=\"pt\")\n",
    "    #print(batch_output.keys())\n",
    "\n",
    "\n",
    "    gt = dict()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    for period in batch:\n",
    "\n",
    "        list_positional_predicate_encoding = []\n",
    "        list_arg_gt = []\n",
    "        list_predicate_index = [] \n",
    "        list_pos_index = [] \n",
    "        list_predicate_meaning_index = []\n",
    "        list_meaning_predicate_encoding = []\n",
    "\n",
    "        for sentence in period:\n",
    "            #positional_encoding\n",
    "            #+2 per il CLS iniziale ad SEP finale\n",
    "            sentence_words_lenght =  len(sentence[\"words\"])\n",
    "            positional_predicate_encoding = torch.zeros(1,sentence_words_lenght+2)\n",
    "            #+1 per il CLS iniziale\n",
    "            pre_idx = int(sentence[\"pre_idx\"])\n",
    "            positional_predicate_encoding[:,pre_idx+1] = 1\n",
    "            list_positional_predicate_encoding.append(positional_predicate_encoding)\n",
    "            #print(\"positional_prefix_encoding\",positional_predicate_encoding)\n",
    "            list_predicate_index.append(pre_idx)\n",
    "\n",
    "\n",
    "            meaning_predicate_encoding = torch.zeros(1,sentence_words_lenght+2)\n",
    "            pre_idx = int(sentence[\"pre_idx\"])\n",
    "            #rather then set the flag 0,1 set with class verb\n",
    "            meaning_predicate_encoding[:,pre_idx+1] = sentence[\"predicate_meaning_idx\"][pre_idx+1]\n",
    "            list_meaning_predicate_encoding.append(meaning_predicate_encoding)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            pos = torch.unsqueeze(sentence[\"pos_idx\"],dim = 0)\n",
    "            list_pos_index.append(pos)\n",
    "            predicate_meaning_idxs = torch.unsqueeze(sentence[\"predicate_meaning_idx\"],dim = 0)\n",
    "            list_predicate_meaning_index.append(predicate_meaning_idxs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #note CLS and SEP are discharder after Bi-LSTM, the Classifier takes in input only wokrds hidden state embedding\n",
    "            arg_gt = torch.unsqueeze(sentence[\"gt_arg_classification\"],dim = 0)\n",
    "            list_arg_gt.append(arg_gt)\n",
    "        \n",
    "\n",
    "    list_arg_gt = torch.cat(list_arg_gt,dim = 0)\n",
    "    list_pos_index = torch.cat(list_pos_index,dim = 0)\n",
    "    list_predicate_meaning_index = torch.cat(list_predicate_meaning_index,dim = 0)\n",
    "    list_predicate_meaning_index_bis = torch.cat(list_meaning_predicate_encoding,dim = 0)\n",
    "    list_positional_predicate_encoding = torch.cat(list_positional_predicate_encoding,dim = 0)\n",
    "    gt[\"arg_gt\"] = list_arg_gt\n",
    "    input[\"predicate_index\"] = list_predicate_index\n",
    "    input[\"pos_index\"] = list_pos_index.long()\n",
    "    input[\"predicate_meaning_idx\"] = list_predicate_meaning_index.long()\n",
    "    input[\"predicate_meaning_idx_bis\"] = list_predicate_meaning_index_bis.long()\n",
    "    offset = batch_output.pop(\"offset_mapping\")\n",
    "    input[\"BERT_input\"] = batch_output\n",
    "    input[\"positional_encoding\"] = list_positional_predicate_encoding.long()\n",
    "    input[\"offset_mapping\"] = offset\n",
    "    input[\"gt\"] = gt\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    return input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SRL(\"EN\",tokenizer,\"train\")\n",
    "#same mapping should be used in both the dataset\n",
    "dev_dataset = SRL(\"EN\",tokenizer,\"dev\",train_dataset.args_roles,train_dataset.pos_list,train_dataset.predicate_dis)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'args_roles': ['recipient', 'experiencer', 'product', 'purpose', 'value', 'patient', 'beneficiary', 'asset', 'theme', 'location', 'time', '_', 'goal', 'agent', 'cause', 'topic', 'extent', 'source', 'co-agent', 'attribute', 'co-patient', 'destination', 'stimulus', 'co-theme', 'result', 'instrument', 'material', 'UNK'], 'pos_list': ['PROPN', 'AUX', 'CCONJ', 'NOUN', 'SYM', 'ADV', 'X', 'NUM', 'INTJ', 'PUNCT', 'DET', 'ADP', 'SCONJ', 'PART', 'PRON', 'ADJ', 'VERB', 'Nothing', 'UNK'], 'predicate_dis': ['ORIENT', 'SPEND-TIME_PASS-TIME', 'INFORM', 'TAKE-INTO-ACCOUNT_CONSIDER', 'REDUCE_DIMINISH', 'DIRECT_AIM_MANEUVER', 'STEAL_DEPRIVE', 'DISCARD', 'VISIT', 'ESTABLISH', 'CANCEL_ELIMINATE', 'DOWNPLAY_HUMILIATE', 'OVERCOME_SURPASS', 'MEASURE_EVALUATE', 'COMBINE_MIX_UNITE', 'GUESS', 'LOWER', 'TOLERATE', 'NEGOTIATE', 'ADJUST_CORRECT', 'BORDER', 'CAUSE-MENTAL-STATE', 'LEAD_GOVERN', 'TEACH', 'ACCOMPANY', 'ABSTAIN_AVOID_REFRAIN', 'REFLECT', 'LEARN', 'GO-FORWARD', 'CAGE_IMPRISON', 'RECORD', 'PUNISH', 'DISMISS_FIRE-SMN', 'SHARE', 'CITE', 'BENEFIT_EXPLOIT', 'PROMOTE', 'LIBERATE_ALLOW_AFFORD', 'DERIVE', 'PUT_APPLY_PLACE_PAVE', 'ACHIEVE', 'DIVERSIFY', 'FILL', 'INVERT_REVERSE', 'ATTACH', 'INSERT', 'UNDERGO-EXPERIENCE', 'MEET', 'RELY', 'SHARPEN', 'TRAVEL', 'RAISE', 'DELAY', 'SPEAK', 'PLAN_SCHEDULE', 'HURT_HARM_ACHE', 'POSSESS', 'OBLIGE_FORCE', 'COME-AFTER_FOLLOW-IN-TIME', 'PRESERVE', 'FIND', 'INVERT_REVERSE-', 'RESIGN_RETIRE', 'SOLVE', 'HAVE-A-FUNCTION_SERVE', 'RESIST', 'RISK', 'TAKE', 'MATCH', 'SLOW-DOWN', 'CARRY-OUT-ACTION', 'HARMONIZE', 'GROUND_BASE_FOUND', 'ORDER', 'ENTER', 'COMPARE', 'PARTICIPATE', 'BEGIN', 'CALCULATE_ESTIMATE', 'FAIL_LOSE', 'TAKE-SHELTER', 'PERCEIVE', 'OBTAIN', 'COST', 'CONSUME_SPEND', 'PUBLICIZE', 'PAINT', 'LEAVE_DEPART_RUN-AWAY', 'DEFEAT', 'JUSTIFY_EXCUSE', 'WARN', 'GUARANTEE_ENSURE_PROMISE', 'CHASE', 'LEAVE-BEHIND', 'FLOW', 'REPEAT', 'ACCUSE', 'OVERLAP', 'CREATE_MATERIALIZE', 'LIKE', 'APPLY', 'FINISH_CONCLUDE_END', 'ASCRIBE', 'AFFIRM', 'CHOOSE', 'ANSWER', 'ARRIVE', 'AFFECT', 'ATTEND', 'COUNT', 'RESTORE-TO-PREVIOUS/INITIAL-STATE_UNDO_UNWIND', 'PERFORM', 'CO-OPT', 'OPEN', 'SETTLE_CONCILIATE', 'BURDEN_BEAR', 'AROUSE_WAKE_ENLIVEN', 'FOCUS', 'INCITE_INDUCE', 'WAIT', 'REMEMBER', 'ADD', 'COMPENSATE', 'SHOOT_LAUNCH_PROPEL', 'RECOGNIZE_ADMIT_IDENTIFY', 'SPEED-UP', 'REACT', 'CRITICIZE', 'READ', 'BREAK_DETERIORATE', 'AMASS', 'MANAGE', 'EXTEND', 'RETAIN_KEEP_SAVE-MONEY', 'CONTRACT-AN-ILLNESS_INFECT', 'DISBAND_BREAK-UP', 'WASH_CLEAN', 'WORSEN', 'HAPPEN_OCCUR', 'TRANSMIT', 'USE', 'UNDERSTAND', 'DISCUSS', 'SEEM', 'SATISFY_FULFILL', 'DEBASE_ADULTERATE', 'DECIDE_DETERMINE', 'AGREE_ACCEPT', 'FIGHT', 'SHOW', 'SEE', 'MOVE-BACK', 'KNOW', 'REFER', 'STOP', 'VERIFY', 'BEHAVE', 'HIRE', 'COOL', 'EARN', 'SUMMON', 'MEAN', 'MOUNT_ASSEMBLE_PRODUCE', 'INCREASE_ENLARGE_MULTIPLY', 'LOCATE-IN-TIME_DATE', 'EMPTY_UNLOAD', 'ASK_REQUEST', 'ORGANIZE', 'PROPOSE', 'THINK', 'GIVE-BIRTH', 'DRIVE-BACK', 'BEFRIEND', 'REVEAL', 'INFLUENCE', 'BUY', 'CARRY_TRANSPORT', 'WORK', 'SUMMARIZE', 'INTERPRET', 'SEND', 'SEPARATE_FILTER_DETACH', 'DESTROY', 'OFFER', 'ABSORB', 'STRENGTHEN_MAKE-RESISTANT', 'AUTHORIZE_ADMIT', 'EXEMPT', 'OPPOSE_REBEL_DISSENT', 'BELIEVE', 'CORRELATE', 'ATTRACT_SUCK', 'ENCLOSE_WRAP', 'STABILIZE_SUPPORT-PHYSICALLY', '_', 'TRY', 'WATCH_LOOK-OUT', 'WIN', 'COMMUNICATE_CONTACT', 'CATCH', 'LEND', 'ATTACK_BOMB', 'GIVE_GIFT', 'EMPHASIZE', 'COPY', 'MOVE-SOMETHING', 'DISTINGUISH_DIFFER', 'IMAGINE', 'BRING', 'MISS_OMIT_LACK', 'TRANSLATE', 'KILL', 'REMOVE_TAKE-AWAY_KIDNAP', 'ARGUE-IN-DEFENSE', 'RECEIVE', 'EXIST_LIVE', 'FACE_CHALLENGE', 'ENDANGER', 'MOVE-ONESELF', 'REPRESENT', 'EXPLAIN', 'SECURE_FASTEN_TIE', 'FOLLOW-IN-SPACE', 'AMELIORATE', 'CIRCULATE_SPREAD_DISTRIBUTE', 'PUBLISH', 'REFUSE', 'GROW_PLOW', 'QUARREL_POLEMICIZE', 'TURN_CHANGE-DIRECTION', 'WRITE', 'EMIT', 'DECREE_DECLARE', 'CAUSE-SMT', 'EXHAUST', 'DIVIDE', 'PROVE', 'CHANGE-APPEARANCE/STATE', 'INCLUDE-AS', 'HEAR_LISTEN', 'REGRET_SORRY', 'OBEY', 'CHARGE', 'PRECLUDE_FORBID_EXPEL', 'PAY', 'SORT_CLASSIFY_ARRANGE', 'COVER_SPREAD_SURMOUNT', 'COME-FROM', 'COMPETE', 'REQUIRE_NEED_WANT_HOPE', 'PREPARE', 'CONSIDER', 'PRINT', 'REPAIR_REMEDY', 'VIOLATE', 'TREAT', 'PULL', 'EXIST-WITH-FEATURE', 'TAKE-A-SERVICE_RENT', 'WELCOME', 'SEARCH', 'APPROVE_PRAISE', 'ANALYZE', 'RESULT_CONSEQUENCE', 'PRESS_PUSH_FOLD', 'CELEBRATE_PARTY', 'APPEAR', 'LOSE', 'LIE', 'REACH', 'HOST_MEAL_INVITE', 'JOIN_CONNECT', 'DROP', 'METEOROLOGICAL', 'SUBJECTIVE-JUDGING', 'SIMPLIFY', 'IMPLY', 'ALLY_ASSOCIATE_MARRY', 'CONTAIN', 'HIT', 'GIVE-UP_ABOLISH_ABANDON', 'GROUP', 'CHANGE_SWITCH', 'FIT', 'NOURISH_FEED', 'SELL', 'REMAIN', 'FOLLOW_SUPPORT_SPONSOR_FUND', 'LOAD_PROVIDE_CHARGE_FURNISH', 'HELP_HEAL_CARE_CURE', 'SIGNAL_INDICATE', 'OPERATE', 'EMBELLISH', 'STAY_DWELL', 'RESTRAIN', 'BE-LOCATED_BASE', 'REPLACE', 'NAME', 'CONTINUE', 'PROTECT', 'PERSUADE', 'ASSIGN-SMT-TO-SMN', 'SHAPE', 'Nothing', 'UNK']}\n",
      "['PROPN', 'AUX', 'CCONJ', 'NOUN', 'SYM', 'ADV', 'X', 'NUM', 'INTJ', 'PUNCT', 'DET', 'ADP', 'SCONJ', 'PART', 'PRON', 'ADJ', 'VERB', 'Nothing', 'UNK']\n"
     ]
    }
   ],
   "source": [
    "#print(train_dataset.args_roles,train_dataset.pos_list,train_dataset.predicate_dis)\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "data = {\n",
    "    'args_roles' : train_dataset.args_roles,\n",
    "    \"pos_list\" : train_dataset.pos_list,\n",
    "    \"predicate_dis\" : train_dataset.predicate_dis,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open('hw2/stud/saved/vocabulary.json', 'w') as outfile:\n",
    "    json.dump(data, outfile)\n",
    "\n",
    "\n",
    "with open('hw2/stud/saved/vocabulary.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "print(data)\n",
    "pos_list = data['pos_list']\n",
    "args_roles = data['args_roles']\n",
    "predicate_dis = data['predicate_dis']\n",
    "print(pos_list)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "embeddings = dict()\n",
    "\n",
    "embeddings[\"predicate_flag_embedding_output_dim\"] = 32\n",
    "#defined in initial exploration of the dataset\n",
    "embeddings[\"pos_embedding_input_dim\"] = 0\n",
    "embeddings[\"pos_embedding_output_dim\"] = 100\n",
    "#-------------------------------------------------\n",
    "embeddings[\"predicate_embedding_input_dim\"] = 0\n",
    "embeddings[\"predicate_embedding_output_dim\"] = 50\n",
    "#defined in initial exploration of the dataset\n",
    "n_classes = 0\n",
    "\n",
    "\n",
    "\n",
    "bilstm = dict()\n",
    "bilstm[\"n_layers\"] = 2\n",
    "bilstm[\"output_dim\"] = 50\n",
    "dropouts = [0.4,0.3,0.3]\n",
    "\n",
    "language_portable = True\n",
    "predicate_meaning = True\n",
    "pos = True\n",
    "\n",
    "cfg = dict()\n",
    "cfg[\"embeddings\"] = embeddings\n",
    "cfg[\"n_classes\"] = n_classes\n",
    "cfg[\"bilstm\"] = bilstm\n",
    "cfg[\"language_portable\"] = language_portable\n",
    "cfg[\"dropouts\"] = dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arg_Classifier(\n",
      "  (bi_lstm_portable): LSTM(132, 50, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "  (embedding_predicate_flag): Embedding(2, 32, max_norm=True)\n",
      "  (embedding_predicate): Embedding(305, 50, max_norm=True)\n",
      "  (embedding_pos): Embedding(19, 100, max_norm=True)\n",
      "  (bi_lstm): LSTM(950, 50, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "  (dropout_language_constraint): Dropout(p=0.6, inplace=False)\n",
      "  (dropout_in_classifier): Dropout(p=0.4, inplace=False)\n",
      "  (Relu): ReLU()\n",
      "  (Sigmoid): Sigmoid()\n",
      "  (linear0): Linear(in_features=300, out_features=700, bias=True)\n",
      "  (linear1): Linear(in_features=700, out_features=140, bias=True)\n",
      "  (linear2): Linear(in_features=140, out_features=28, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#from mmcv import Config\n",
    "from hw2.stud.arg import Arg_Classifier, Arg_Classifier_from_paper\n",
    "\n",
    "\n",
    "\n",
    "#cfg = Config.fromfile('/home/francesco/Desktop/nlp2022-hw2-main/hw2/stud/configs/model.py')\n",
    "\n",
    "cfg[\"embeddings\"][\"pos_embedding_input_dim\"] = len(train_dataset.pos_list)\n",
    "cfg[\"embeddings\"][\"predicate_embedding_input_dim\"] = len(train_dataset.predicate_dis)\n",
    "cfg[\"n_classes\"] = len(train_dataset.args_roles)\n",
    "\n",
    "\n",
    "model = Arg_Classifier(cfg).cuda()\n",
    "#model = Arg_Classifier_from_paper(\"EN\",cfg).cuda()\n",
    "print(model)\n",
    "\n",
    "automodel = auto_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(gold,pred):\n",
    "    true_positives, false_positives, false_negatives = 0, 0, 0\n",
    "    null_tag = \"_\"\n",
    "\n",
    "    for i,r_g in  enumerate(gold):\n",
    "        r_p = pred[i]\n",
    "\n",
    "        if r_g != null_tag and r_p != null_tag:\n",
    "            true_positives += 1\n",
    "        elif r_g != null_tag and r_p == null_tag:\n",
    "            false_negatives += 1\n",
    "        elif r_g == null_tag and r_p != null_tag:\n",
    "            false_positives += 1\n",
    "\n",
    "    a = true_positives + false_positives\n",
    "    b = true_positives + false_negatives\n",
    "    if a == 0 and b == 0 :        \n",
    "        argument_identification = {\n",
    "            \"true_positives\": 0,\n",
    "            \"false_positives\": 0,\n",
    "            \"false_negatives\": 0,\n",
    "            \"precision\": 0,\n",
    "            \"recall\": 0,\n",
    "            \"f1\": 0,\n",
    "        } \n",
    "\n",
    "    else : \n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        argument_identification = {\n",
    "            \"true_positives\": true_positives,\n",
    "            \"false_positives\": false_positives,\n",
    "            \"false_negatives\": false_negatives,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    true_positives, false_positives, false_negatives = 0, 0, 0\n",
    "    for i,r_g in  enumerate(gold):\n",
    "        r_p = pred[i]\n",
    "\n",
    "        if r_g != null_tag and r_p != null_tag:\n",
    "            if r_g == r_p:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                false_positives += 1\n",
    "                false_negatives += 1\n",
    "        elif r_g != null_tag and r_p == null_tag:\n",
    "                false_negatives += 1\n",
    "        elif r_g == null_tag and r_p != null_tag:\n",
    "                false_positives += 1\n",
    "\n",
    "\n",
    "    a = true_positives + false_positives\n",
    "    b = true_positives + false_negatives\n",
    "    if a == 0 and b == 0 :\n",
    "        argument_classification = {\n",
    "            \"true_positives\": 0,\n",
    "            \"false_positives\": 0,\n",
    "            \"false_negatives\": 0,\n",
    "            \"precision\": 0,\n",
    "            \"recall\": 0,\n",
    "            \"f1\": 0,\n",
    "        } \n",
    "\n",
    "    else : \n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        argument_classification = {\n",
    "            \"true_positives\": true_positives,\n",
    "            \"false_positives\": false_positives,\n",
    "            \"false_negatives\": false_negatives,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "        }\n",
    "\n",
    "\n",
    "    return argument_identification,argument_classification\n",
    "\n",
    "\"\"\"\n",
    "from [1,1,8,8,8,8,8,8,8,8,8,8,8,8,2,2,2,8,8,8......]\n",
    "to from [agent,agent,_,_,_........,]\n",
    "\"\"\"\n",
    "def mapping_args(g,p,mapping):\n",
    "    \n",
    "    \n",
    "    gt = [mapping[elem] for elem in g]\n",
    "    predictions = [mapping[elem] for elem in p]\n",
    "\n",
    "\n",
    "    return gt,predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Argument Identification and Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs n. 0\n",
      "F1 train: [0.04415011 0.         0.         0.         0.         0.17993795\n",
      " 0.         0.         0.44318018 0.         0.         0.98443584\n",
      " 0.06132666 0.64229412 0.         0.25796178 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.02020202 0.         0.         0.        ]\n",
      "F1 avg train: 0.9512728906843513\n",
      "identification {'true_positives': 12671, 'false_positives': 1783, 'false_negatives': 12175, 'precision': 0.8766431437664315, 'recall': 0.5099814859534734, 'f1': 0.6448346055979642}\n",
      "classification_result {'true_positives': 8021, 'false_positives': 6433, 'false_negatives': 16825, 'precision': 0.5549328905493289, 'recall': 0.32282862432584725, 'f1': 0.4081933842239186}\n",
      "EPOCHS : 0\n",
      "F1 eval : [0.30522088 0.         0.         0.         0.         0.1549101\n",
      " 0.         0.         0.51717902 0.         0.         0.98874176\n",
      " 0.23419204 0.70358306 0.         0.25       0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "F1 avg eval : 0.9602759422607752\n",
      "identification {'true_positives': 3178, 'false_positives': 297, 'false_negatives': 1894, 'precision': 0.9145323741007194, 'recall': 0.626577287066246, 'f1': 0.7436527436527436}\n",
      "classification_result {'true_positives': 2023, 'false_positives': 1452, 'false_negatives': 3049, 'precision': 0.582158273381295, 'recall': 0.3988564668769716, 'f1': 0.4733824733824734}\n",
      "SAVED : hw2/stud/saved/model_2022_12_25_17_53_6.pth\n",
      "Epochs n. 1\n",
      "F1 train: [0.3616     0.         0.         0.         0.         0.39076155\n",
      " 0.         0.         0.53280318 0.         0.         0.98838553\n",
      " 0.21258593 0.75258442 0.         0.45637584 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.08668731 0.         0.        ]\n",
      "F1 avg train: 0.9611609161822844\n",
      "identification {'true_positives': 16302, 'false_positives': 1826, 'false_negatives': 8544, 'precision': 0.8992718446601942, 'recall': 0.6561217097319488, 'f1': 0.7586913017173176}\n",
      "classification_result {'true_positives': 11216, 'false_positives': 6912, 'false_negatives': 13630, 'precision': 0.6187113857016769, 'recall': 0.45142075183128066, 'f1': 0.5219900404895984}\n",
      "EPOCHS : 1\n",
      "F1 eval : [0.4962963  0.         0.         0.         0.         0.56996587\n",
      " 0.         0.         0.57142857 0.         0.         0.99021102\n",
      " 0.32036613 0.75862069 0.         0.58601134 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "F1 avg eval : 0.9676497386701178\n",
      "identification {'true_positives': 3492, 'false_positives': 339, 'false_negatives': 1556, 'precision': 0.9115113547376664, 'recall': 0.6917591125198098, 'f1': 0.786575064759545}\n",
      "classification_result {'true_positives': 2569, 'false_positives': 1262, 'false_negatives': 2479, 'precision': 0.6705820934481859, 'recall': 0.5089144215530903, 'f1': 0.5786687690055186}\n",
      "SAVED : hw2/stud/saved/model_2022_12_25_17_53_6.pth\n",
      "Epochs n. 2\n",
      "F1 train: [0.51289398 0.00613497 0.         0.         0.         0.56902718\n",
      " 0.00981997 0.         0.58898939 0.         0.         0.98944699\n",
      " 0.29654837 0.781584   0.         0.53227233 0.         0.\n",
      " 0.         0.06349206 0.         0.07073955 0.         0.03816794\n",
      " 0.10542169 0.         0.        ]\n",
      "F1 avg train: 0.9655235482172327\n",
      "identification {'true_positives': 17214, 'false_positives': 1770, 'false_negatives': 7668, 'precision': 0.906763590391909, 'recall': 0.691825415963347, 'f1': 0.7848447544795514}\n",
      "classification_result {'true_positives': 12760, 'false_positives': 6224, 'false_negatives': 12122, 'precision': 0.6721449641803624, 'recall': 0.5128205128205128, 'f1': 0.5817717594492317}\n",
      "EPOCHS : 2\n",
      "F1 eval : [0.56747405 0.         0.         0.         0.         0.66608544\n",
      " 0.02919708 0.         0.63085871 0.         0.         0.99077608\n",
      " 0.46774194 0.75628059 0.         0.60263653 0.         0.\n",
      " 0.         0.         0.         0.04444444 0.         0.\n",
      " 0.         0.         0.        ]\n",
      "F1 avg eval : 0.9702391350863658\n",
      "identification {'true_positives': 3668, 'false_positives': 396, 'false_negatives': 1395, 'precision': 0.9025590551181102, 'recall': 0.7244716571202844, 'f1': 0.8037690369234142}\n",
      "classification_result {'true_positives': 2824, 'false_positives': 1240, 'false_negatives': 2239, 'precision': 0.6948818897637795, 'recall': 0.5577720718941339, 'f1': 0.6188232716117016}\n",
      "SAVED : hw2/stud/saved/model_2022_12_25_17_53_6.pth\n",
      "Epochs n. 3\n",
      "F1 train: [0.61081081 0.10555556 0.         0.         0.         0.67658764\n",
      " 0.16246499 0.         0.64127075 0.00995025 0.         0.99035376\n",
      " 0.40106477 0.80227226 0.         0.57819905 0.         0.06349206\n",
      " 0.         0.17910448 0.         0.1299435  0.04081633 0.09364548\n",
      " 0.13390313 0.         0.        ]\n",
      "F1 avg train: 0.9692567989523224\n",
      "identification {'true_positives': 18001, 'false_positives': 1759, 'false_negatives': 6831, 'precision': 0.9109817813765182, 'recall': 0.7249114046391752, 'f1': 0.8073645496950126}\n",
      "classification_result {'true_positives': 14060, 'false_positives': 5700, 'false_negatives': 10772, 'precision': 0.7115384615384616, 'recall': 0.5662048969072165, 'f1': 0.6306063867958379}\n",
      "EPOCHS : 3\n",
      "F1 eval : [0.6547619  0.         0.         0.         0.         0.74206672\n",
      " 0.31952663 0.         0.68464581 0.         0.         0.99163818\n",
      " 0.496      0.79552    0.         0.66911765 0.         0.\n",
      " 0.         0.10526316 0.         0.12       0.14925373 0.\n",
      " 0.         0.         0.        ]\n",
      "F1 avg eval : 0.9738546079422555\n",
      "identification {'true_positives': 3809, 'false_positives': 377, 'false_negatives': 1240, 'precision': 0.9099378881987578, 'recall': 0.7544068132303426, 'f1': 0.8249052517596102}\n",
      "classification_result {'true_positives': 3105, 'false_positives': 1081, 'false_negatives': 1944, 'precision': 0.7417582417582418, 'recall': 0.6149732620320856, 'f1': 0.6724417975094749}\n",
      "SAVED : hw2/stud/saved/model_2022_12_25_17_53_6.pth\n",
      "Epochs n. 4\n",
      "F1 train: [0.64996704 0.28235294 0.         0.         0.         0.74697975\n",
      " 0.24575163 0.         0.69172389 0.00970874 0.         0.99097205\n",
      " 0.46553191 0.82100823 0.         0.62526053 0.03389831 0.1015625\n",
      " 0.         0.22159091 0.         0.2152231  0.23602484 0.1122807\n",
      " 0.20838972 0.         0.        ]\n",
      "F1 avg train: 0.9723855609222913\n",
      "identification {'true_positives': 18557, 'false_positives': 1738, 'false_negatives': 6351, 'precision': 0.9143631436314363, 'recall': 0.7450216797815963, 'f1': 0.8210517001083999}\n",
      "classification_result {'true_positives': 15203, 'false_positives': 5092, 'false_negatives': 9705, 'precision': 0.7491007637349101, 'recall': 0.6103661474225148, 'f1': 0.6726544698360728}\n",
      "EPOCHS : 4\n",
      "F1 eval : [0.68944099 0.34666667 0.         0.         0.         0.78640777\n",
      " 0.37714286 0.         0.71832006 0.         0.         0.99191485\n",
      " 0.53992395 0.78717779 0.         0.72512648 0.         0.1\n",
      " 0.         0.19354839 0.         0.25       0.26666667 0.12244898\n",
      " 0.02040816 0.         0.        ]\n",
      "F1 avg eval : 0.9757295244261059\n",
      "identification {'true_positives': 3829, 'false_positives': 327, 'false_negatives': 1253, 'precision': 0.9213185755534168, 'recall': 0.7534435261707989, 'f1': 0.8289673089413293}\n",
      "classification_result {'true_positives': 3213, 'false_positives': 943, 'false_negatives': 1869, 'precision': 0.7730991337824832, 'recall': 0.6322314049586777, 'f1': 0.695605109331024}\n",
      "SAVED : hw2/stud/saved/model_2022_12_25_17_53_6.pth\n",
      "Epochs n. 5\n",
      "F1 train: [0.70507201 0.42283298 0.         0.         0.         0.77705519\n",
      " 0.3216445  0.         0.71760749 0.03755869 0.         0.99163562\n",
      " 0.49857666 0.83264139 0.         0.65666802 0.03333333 0.2244898\n",
      " 0.         0.27989822 0.         0.23880597 0.36923077 0.23952096\n",
      " 0.22857143 0.         0.        ]\n",
      "F1 avg train: 0.9745569681027235\n",
      "identification {'true_positives': 19121, 'false_positives': 1691, 'false_negatives': 5777, 'precision': 0.9187487987699404, 'recall': 0.7679733311912603, 'f1': 0.8366221833296871}\n",
      "classification_result {'true_positives': 15934, 'false_positives': 4878, 'false_negatives': 8964, 'precision': 0.7656159907745531, 'recall': 0.6399710820146196, 'f1': 0.6971778604244149}\n",
      "EPOCHS : 5\n",
      "F1 eval : [0.72240803 0.36585366 0.         0.         0.         0.80880121\n",
      " 0.3908046  0.         0.73303835 0.         0.         0.9922622\n",
      " 0.51676529 0.81493299 0.         0.74570447 0.         0.11764706\n",
      " 0.         0.25396825 0.         0.36923077 0.32098765 0.38095238\n",
      " 0.06       0.         0.        ]\n",
      "F1 avg eval : 0.9770653749081488\n",
      "identification {'true_positives': 3937, 'false_positives': 376, 'false_negatives': 1117, 'precision': 0.9128217018316717, 'recall': 0.7789869410368025, 'f1': 0.8406106544251095}\n",
      "classification_result {'true_positives': 3360, 'false_positives': 953, 'false_negatives': 1694, 'precision': 0.7790401112914445, 'recall': 0.6648199445983379, 'f1': 0.7174121917369488}\n",
      "SAVED : hw2/stud/saved/model_2022_12_25_17_53_6.pth\n",
      "Epochs n. 6\n",
      "F1 train: [0.72162334 0.51778656 0.         0.         0.         0.8066633\n",
      " 0.41743119 0.         0.74510666 0.06511628 0.05405405 0.99206721\n",
      " 0.51155249 0.84532301 0.         0.7022293  0.15625    0.22435897\n",
      " 0.01351351 0.27763496 0.03125    0.31890661 0.46808511 0.23622047\n",
      " 0.29411765 0.         0.        ]\n",
      "F1 avg train: 0.9763653205056335\n",
      "identification {'true_positives': 19455, 'false_positives': 1688, 'false_negatives': 5367, 'precision': 0.9201627016033675, 'recall': 0.7837805172830553, 'f1': 0.8465136516915043}\n",
      "classification_result {'true_positives': 16562, 'false_positives': 4581, 'false_negatives': 8260, 'precision': 0.7833325450503713, 'recall': 0.6672306824591089, 'f1': 0.7206352659632329}\n",
      "EPOCHS : 6\n",
      "F1 eval : [0.71779141 0.35       0.         0.         0.         0.82006369\n",
      " 0.4729064  0.         0.74272931 0.06451613 0.         0.9923472\n",
      " 0.56858847 0.81429031 0.         0.75290216 0.5        0.3\n",
      " 0.2        0.25       0.         0.32352941 0.32       0.31460674\n",
      " 0.21238938 0.         0.        ]\n",
      "F1 avg eval : 0.9778632223979711\n",
      "identification {'true_positives': 3949, 'false_positives': 366, 'false_negatives': 1112, 'precision': 0.9151796060254924, 'recall': 0.7802805769610749, 'f1': 0.8423634812286689}\n",
      "classification_result {'true_positives': 3397, 'false_positives': 918, 'false_negatives': 1664, 'precision': 0.7872537659327926, 'recall': 0.6712112230784429, 'f1': 0.7246160409556314}\n",
      "SAVED : hw2/stud/saved/model_2022_12_25_17_53_6.pth\n",
      "Epochs n. 7\n",
      "F1 train: [0.74677003 0.58823529 0.02020202 0.         0.         0.81868781\n",
      " 0.43107221 0.         0.76817981 0.14285714 0.10526316 0.99236454\n",
      " 0.55657737 0.85177879 0.         0.74039581 0.35443038 0.33819242\n",
      " 0.02580645 0.36855037 0.03174603 0.33175355 0.51122625 0.3\n",
      " 0.3461063  0.         0.        ]\n",
      "F1 avg train: 0.977848669529425\n",
      "identification {'true_positives': 19689, 'false_positives': 1653, 'false_negatives': 5130, 'precision': 0.9225470902445881, 'recall': 0.7933035174664571, 'f1': 0.853057776044713}\n",
      "classification_result {'true_positives': 17084, 'false_positives': 4258, 'false_negatives': 7735, 'precision': 0.8004873020335489, 'recall': 0.688343607719892, 'f1': 0.7401919369164447}\n",
      "EPOCHS : 7\n",
      "F1 eval : [0.72435897 0.40963855 0.         0.         0.         0.82271681\n",
      " 0.49230769 0.         0.75911854 0.17647059 0.         0.99262245\n",
      " 0.5770751  0.8225602  0.         0.78594249 0.4        0.33802817\n",
      " 0.15789474 0.4        0.         0.34666667 0.45238095 0.6\n",
      " 0.46616541 0.         0.        ]\n",
      "F1 avg eval : 0.9793116283135631\n",
      "identification {'true_positives': 4021, 'false_positives': 377, 'false_negatives': 1058, 'precision': 0.9142792178262846, 'recall': 0.7916912778105927, 'f1': 0.8485807745067004}\n",
      "classification_result {'true_positives': 3523, 'false_positives': 875, 'false_negatives': 1556, 'precision': 0.8010459299681674, 'recall': 0.6936404804095294, 'f1': 0.7434842249657064}\n",
      "SAVED : hw2/stud/saved/model_2022_12_25_17_53_6.pth\n",
      "Epochs n. 8\n",
      "F1 train: [0.75597484 0.64179104 0.07476636 0.         0.         0.83271997\n",
      " 0.49081081 0.         0.77964856 0.18852459 0.1025641  0.99273257\n",
      " 0.59377432 0.85766522 0.         0.75451402 0.2278481  0.40547945\n",
      " 0.16568047 0.40268456 0.03125    0.38288288 0.56239601 0.37948718\n",
      " 0.44393593 0.         0.        ]\n",
      "F1 avg train: 0.979198402447946\n",
      "identification {'true_positives': 20080, 'false_positives': 1684, 'false_negatives': 4783, 'precision': 0.9226245175519207, 'recall': 0.8076257893255038, 'f1': 0.8613035365775196}\n",
      "classification_result {'true_positives': 17608, 'false_positives': 4156, 'false_negatives': 7255, 'precision': 0.809042455430987, 'recall': 0.7082009411575433, 'f1': 0.7552705513972592}\n",
      "EPOCHS : 8\n",
      "F1 eval : [0.75471698 0.52747253 0.11764706 0.         0.         0.82900763\n",
      " 0.55696203 0.         0.77124672 0.11428571 0.         0.99284852\n",
      " 0.58627087 0.81955381 0.         0.81672026 0.25       0.32\n",
      " 0.36363636 0.4        0.26666667 0.46753247 0.5        0.57142857\n",
      " 0.375      0.         0.        ]\n",
      "F1 avg eval : 0.9800928610922087\n",
      "identification {'true_positives': 4070, 'false_positives': 398, 'false_negatives': 988, 'precision': 0.9109221128021486, 'recall': 0.8046658758402531, 'f1': 0.8545034642032332}\n",
      "classification_result {'true_positives': 3573, 'false_positives': 895, 'false_negatives': 1485, 'precision': 0.799686660698299, 'recall': 0.7064056939501779, 'f1': 0.7501574637833298}\n",
      "SAVED : hw2/stud/saved/model_2022_12_25_17_53_6.pth\n",
      "Epochs n. 9\n",
      "F1 train: [0.77492163 0.67286245 0.10344828 0.         0.         0.83909976\n",
      " 0.504772   0.         0.79355697 0.24535316 0.19047619 0.99306158\n",
      " 0.60483242 0.86961236 0.03703704 0.78443342 0.325      0.44134078\n",
      " 0.18934911 0.4254386  0.11428571 0.39285714 0.61237785 0.40091116\n",
      " 0.48322148 0.         0.        ]\n",
      "F1 avg train: 0.9803462577855815\n",
      "identification {'true_positives': 20270, 'false_positives': 1620, 'false_negatives': 4538, 'precision': 0.9259936043855642, 'recall': 0.8170751370525637, 'f1': 0.8681313974902566}\n",
      "classification_result {'true_positives': 17958, 'false_positives': 3932, 'false_negatives': 6850, 'precision': 0.8203746002740978, 'recall': 0.7238793937439536, 'f1': 0.7691121675446487}\n",
      "EPOCHS : 9\n",
      "F1 eval : [0.75641026 0.54945055 0.26086957 0.         0.         0.8188348\n",
      " 0.53271028 0.         0.7756654  0.23809524 0.         0.99284663\n",
      " 0.60038241 0.82751938 0.         0.81200632 0.33333333 0.38554217\n",
      " 0.40816327 0.45333333 0.28571429 0.43373494 0.53488372 0.56179775\n",
      " 0.45454545 0.         0.        ]\n",
      "F1 avg eval : 0.9804281058082749\n",
      "identification {'true_positives': 4047, 'false_positives': 371, 'false_negatives': 1014, 'precision': 0.916025350837483, 'recall': 0.7996443390634262, 'f1': 0.8538875408798396}\n",
      "classification_result {'true_positives': 3578, 'false_positives': 840, 'false_negatives': 1483, 'precision': 0.80986871887732, 'recall': 0.7069749061450307, 'f1': 0.7549319548475578}\n",
      "SAVED : hw2/stud/saved/model_2022_12_25_17_53_6.pth\n",
      "Epochs n. 10\n",
      "F1 train: [0.77833126 0.7079646  0.1147541  0.         0.         0.8470074\n",
      " 0.52350427 0.         0.80667553 0.28057554 0.14634146 0.99334786\n",
      " 0.61227661 0.87537749 0.         0.79430498 0.35443038 0.53299492\n",
      " 0.22222222 0.45378151 0.19444444 0.44074844 0.6496     0.39479393\n",
      " 0.49339207 0.         0.        ]\n",
      "F1 avg train: 0.9812403820466332\n",
      "identification {'true_positives': 20585, 'false_positives': 1639, 'false_negatives': 4283, 'precision': 0.9262508999280058, 'recall': 0.8277706289207013, 'f1': 0.8742461564596959}\n",
      "classification_result {'true_positives': 18348, 'false_positives': 3876, 'false_negatives': 6520, 'precision': 0.8255939524838013, 'recall': 0.7378156667202831, 'f1': 0.7792406353520767}\n",
      "EPOCHS : 10\n",
      "F1 eval : [0.77022654 0.58064516 0.31578947 0.         0.         0.83191325\n",
      " 0.59166667 0.         0.78836586 0.2        0.         0.99301719\n",
      " 0.59217877 0.83051907 0.         0.83359253 0.33333333 0.36619718\n",
      " 0.47272727 0.43902439 0.15384615 0.45945946 0.58       0.54945055\n",
      " 0.48888889 0.         0.        ]\n",
      "F1 avg eval : 0.9810816203834374\n",
      "identification {'true_positives': 4153, 'false_positives': 444, 'false_negatives': 906, 'precision': 0.9034152708288014, 'recall': 0.8209132239573038, 'f1': 0.8601905550952775}\n",
      "classification_result {'true_positives': 3696, 'false_positives': 901, 'false_negatives': 1363, 'precision': 0.8040026103980857, 'recall': 0.7305791658430519, 'f1': 0.7655343827671913}\n",
      "SAVED : hw2/stud/saved/model_2022_12_25_17_53_6.pth\n",
      "Epochs n. 11\n",
      "F1 train: [0.80248447 0.7366548  0.16       0.         0.         0.85843525\n",
      " 0.56129686 0.         0.81454491 0.31914894 0.30434783 0.99362183\n",
      " 0.64216634 0.87945739 0.06896552 0.81730052 0.52380952 0.52475248\n",
      " 0.34170854 0.50847458 0.16666667 0.45643154 0.68797565 0.45202559\n",
      " 0.53601695 0.         0.        ]\n",
      "F1 avg train: 0.9822568731138809\n",
      "identification {'true_positives': 20772, 'false_positives': 1574, 'false_negatives': 4094, 'precision': 0.9295623377785733, 'recall': 0.8353575162872999, 'f1': 0.8799457764975006}\n",
      "classification_result {'true_positives': 18690, 'false_positives': 3656, 'false_negatives': 6176, 'precision': 0.8363913004564575, 'recall': 0.7516287299927612, 'f1': 0.791747860713378}\n",
      "EPOCHS : 11\n",
      "F1 eval : [0.77160494 0.61386139 0.32258065 0.         0.         0.83239775\n",
      " 0.56502242 0.         0.783174   0.18604651 0.28571429 0.99283273\n",
      " 0.59491194 0.83124397 0.         0.84896661 0.28571429 0.4109589\n",
      " 0.52830189 0.42857143 0.23529412 0.47058824 0.62626263 0.55913978\n",
      " 0.47222222 0.         0.        ]\n",
      "F1 avg eval : 0.9809972019058053\n",
      "identification {'true_positives': 4074, 'false_positives': 401, 'false_negatives': 987, 'precision': 0.9103910614525139, 'recall': 0.8049792531120332, 'f1': 0.8544463087248322}\n",
      "classification_result {'true_positives': 3645, 'false_positives': 830, 'false_negatives': 1416, 'precision': 0.8145251396648044, 'recall': 0.7202133965619443, 'f1': 0.7644714765100671}\n",
      "Epochs n. 12\n",
      "F1 train: [0.80764488 0.73883162 0.23703704 0.         0.         0.867173\n",
      " 0.57200811 0.         0.83069608 0.4        0.13043478 0.99384493\n",
      " 0.64898116 0.88725326 0.         0.82204957 0.37647059 0.54545455\n",
      " 0.28865979 0.48547718 0.08219178 0.45338983 0.68389058 0.51476793\n",
      " 0.54411765 0.         0.        ]\n",
      "F1 avg train: 0.9829929124787379\n",
      "identification {'true_positives': 20927, 'false_positives': 1552, 'false_negatives': 3926, 'precision': 0.9309577828195205, 'recall': 0.8420311431215547, 'f1': 0.8842643454745204}\n",
      "classification_result {'true_positives': 18964, 'false_positives': 3515, 'false_negatives': 5889, 'precision': 0.8436318341563237, 'recall': 0.7630467146823321, 'f1': 0.8013183469956899}\n",
      "EPOCHS : 12\n",
      "F1 eval : [0.7752443  0.61386139 0.32       0.         0.         0.83295195\n",
      " 0.59663866 0.         0.79570717 0.18181818 0.         0.99311286\n",
      " 0.61172161 0.83174603 0.         0.85582822 0.4        0.37142857\n",
      " 0.52830189 0.41860465 0.23529412 0.44444444 0.60606061 0.63043478\n",
      " 0.49275362 0.         0.        ]\n",
      "F1 avg eval : 0.9814949012848718\n",
      "identification {'true_positives': 4164, 'false_positives': 427, 'false_negatives': 906, 'precision': 0.9069919407536484, 'recall': 0.821301775147929, 'f1': 0.8620225649518684}\n",
      "classification_result {'true_positives': 3726, 'false_positives': 865, 'false_negatives': 1344, 'precision': 0.8115878893487257, 'recall': 0.7349112426035503, 'f1': 0.7713487216644239}\n",
      "SAVED : hw2/stud/saved/model_2022_12_25_17_53_6.pth\n",
      "Epochs n. 13\n",
      "F1 train: [0.81454097 0.76610169 0.21621622 0.         0.         0.86216126\n",
      " 0.6143001  0.         0.83821333 0.38795987 0.19047619 0.99409744\n",
      " 0.64772727 0.89162925 0.12698413 0.83573274 0.51162791 0.5995086\n",
      " 0.44036697 0.52313883 0.25       0.47413793 0.71787297 0.52738337\n",
      " 0.57997936 0.         0.        ]\n",
      "F1 avg train: 0.9837328974394317\n",
      "identification {'true_positives': 21105, 'false_positives': 1467, 'false_negatives': 3795, 'precision': 0.9350079744816587, 'recall': 0.8475903614457831, 'f1': 0.8891557128412537}\n",
      "classification_result {'true_positives': 19197, 'false_positives': 3375, 'false_negatives': 5703, 'precision': 0.8504784688995215, 'recall': 0.7709638554216868, 'f1': 0.8087714863498483}\n",
      "EPOCHS : 13\n",
      "F1 eval : [0.75949367 0.60784314 0.28571429 0.         0.         0.83359253\n",
      " 0.61276596 0.         0.79317035 0.23728814 0.         0.99306875\n",
      " 0.59340659 0.84266169 0.         0.85626911 0.22222222 0.39473684\n",
      " 0.5        0.47191011 0.3        0.49411765 0.6407767  0.52631579\n",
      " 0.46666667 0.         0.        ]\n",
      "F1 avg eval : 0.9815468322949501\n",
      "identification {'true_positives': 4202, 'false_positives': 474, 'false_negatives': 865, 'precision': 0.8986313088109495, 'recall': 0.8292875468719163, 'f1': 0.862567997536693}\n",
      "classification_result {'true_positives': 3756, 'false_positives': 920, 'false_negatives': 1311, 'precision': 0.8032506415739948, 'recall': 0.7412670219064535, 'f1': 0.7710150877553116}\n",
      "SAVED : hw2/stud/saved/model_2022_12_25_17_53_6.pth\n",
      "Epochs n. 14\n",
      "F1 train: [0.83181542 0.77054795 0.25333333 0.         0.         0.87633588\n",
      " 0.62137862 0.         0.84364505 0.38311688 0.34615385 0.99433175\n",
      " 0.66465257 0.8976473  0.10526316 0.84457478 0.55813953 0.5914787\n",
      " 0.41441441 0.55009823 0.1369863  0.53305785 0.75620438 0.55353535\n",
      " 0.58753709 0.         0.        ]\n",
      "F1 avg train: 0.9844404524304502\n",
      "identification {'true_positives': 21250, 'false_positives': 1461, 'false_negatives': 3574, 'precision': 0.9356699396768086, 'recall': 0.8560264260393168, 'f1': 0.8940780477542862}\n",
      "classification_result {'true_positives': 19429, 'false_positives': 3282, 'false_negatives': 5395, 'precision': 0.8554885297873277, 'recall': 0.7826699967773123, 'f1': 0.8174608183443777}\n",
      "EPOCHS : 14\n",
      "F1 eval : [0.77316294 0.6185567  0.22857143 0.         0.         0.83916084\n",
      " 0.62280702 0.         0.79663609 0.22222222 0.         0.9932501\n",
      " 0.59489051 0.83713561 0.         0.85316847 0.33333333 0.41558442\n",
      " 0.50980392 0.4494382  0.22222222 0.48101266 0.62857143 0.54545455\n",
      " 0.48101266 0.         0.        ]\n",
      "F1 avg eval : 0.9818095627798558\n",
      "identification {'true_positives': 4199, 'false_positives': 445, 'false_negatives': 866, 'precision': 0.9041774332472007, 'recall': 0.8290227048371175, 'f1': 0.8649706457925634}\n",
      "classification_result {'true_positives': 3743, 'false_positives': 901, 'false_negatives': 1322, 'precision': 0.8059862187769165, 'recall': 0.7389930898321816, 'f1': 0.7710371819960861}\n",
      "SAVED : hw2/stud/saved/model_2022_12_25_17_53_6.pth\n",
      "Epochs n. 15\n",
      "F1 train: [0.83698297 0.79061977 0.25698324 0.         0.         0.88291032\n",
      " 0.62311558 0.         0.85191793 0.45970149 0.22641509 0.99453287\n",
      " 0.6811215  0.90029404 0.12698413 0.85243501 0.58139535 0.62117647\n",
      " 0.53389831 0.55066922 0.31111111 0.53333333 0.73593074 0.55711423\n",
      " 0.58640777 0.         0.        ]\n",
      "F1 avg train: 0.9850246766540812\n",
      "identification {'true_positives': 21491, 'false_positives': 1501, 'false_negatives': 3362, 'precision': 0.9347164231036882, 'recall': 0.8647245805335372, 'f1': 0.898359285191765}\n",
      "classification_result {'true_positives': 19696, 'false_positives': 3296, 'false_negatives': 5157, 'precision': 0.8566457898399443, 'recall': 0.7924998994085221, 'f1': 0.8233253213501933}\n",
      "Early stopping at epoch :  15\n",
      "F1 eval : [0.76489028 0.67307692 0.41176471 0.         0.         0.84824903\n",
      " 0.60944206 0.         0.79801451 0.26086957 0.28571429 0.99327598\n",
      " 0.59674503 0.83553875 0.         0.86227545 0.33333333 0.4556962\n",
      " 0.58181818 0.43010753 0.34782609 0.48837209 0.64761905 0.57731959\n",
      " 0.49006623 0.         0.        ]\n",
      "F1 avg eval : 0.9820245290429902\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "currentDateAndTime = datetime.now()\n",
    "_id = str(currentDateAndTime.year)+\"_\"+str(currentDateAndTime.month)+\"_\"+str(currentDateAndTime.day)+\"_\"+str(currentDateAndTime.hour)+\"_\"+str(currentDateAndTime.minute)+\"_\"+str(currentDateAndTime.second)\n",
    "\n",
    "\n",
    "\n",
    "#optimizer = torch.optim.Adam(model.parameters(),lr = 0.000005)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "\n",
    "logSotfMax = torch.nn.LogSoftmax(dim=1)\n",
    "nll_loss = torch.nn.NLLLoss()\n",
    "\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "dataloader_dev = DataLoader(dev_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "mapping = dataloader_train.dataset.args_roles\n",
    "\n",
    "auto_model.eval()\n",
    "\n",
    "EPOCHS = 200\n",
    "patience_counter = 0\n",
    "patience = 5\n",
    "max_val_loss = 9999\n",
    "f1_score_max = 0\n",
    "output_path = \"hw2/stud/saved\"\n",
    "model_name = \"model_\"+_id+\".pth\"\n",
    "PATH = os.path.join(output_path,model_name)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    #TRAINING\n",
    "    p = []\n",
    "    g = []\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_train):\n",
    "        #print(sample_batched)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "       \n",
    "        #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        #sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx_bis\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "        #------------------FILTERING SUB-WORDS----------------------\n",
    "        subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "        word_emebedding = []\n",
    "        for i in range(n):\n",
    "            subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "            flag = subtoken_mask[0,i,0]\n",
    "            if flag :\n",
    "                continue\n",
    "            else :\n",
    "                word_emebedding.append(subwords_embedding)\n",
    "        word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "        #-------------------------FORWARD/BACKWARD----------------------------------\n",
    "        x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "            perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "            predicate_index = sample_batched[\"predicate_index\"],\n",
    "            pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "            predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])        \n",
    "        b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "        loss = nll_loss(logSotfMax(x),gt)\n",
    "        total_loss = total_loss + loss\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "        counter += 1 \n",
    "            \n",
    "\n",
    "        #-------------------------RESULT STORING----------------------------------\n",
    "        predicted = torch.argmax(x, dim=1)\n",
    "        p += predicted.tolist()\n",
    "        g += gt.tolist()\n",
    "    \n",
    "\n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    scheduler.step()\n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    print(\"Epochs n.\", epoch)\n",
    "    print(\"F1 train:\",f1)\n",
    "    print(\"F1 avg train:\",f1_avg)\n",
    "    \n",
    "    avg_train_loss = total_loss/counter\n",
    "    writer.add_scalar(\"Loss/train\", avg_train_loss, epoch)\n",
    "\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"Train_EN/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"Train_EN/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "    #EVALUATION\n",
    "    p = []\n",
    "    g = []\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_dev):\n",
    "    \n",
    "      #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        #sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx_bis\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "            #------------------FILTERING SUB-WORDS----------------------\n",
    "            subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "            word_emebedding = []\n",
    "            for i in range(n): \n",
    "                subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "                flag = subtoken_mask[0,i,0]\n",
    "                if flag :\n",
    "                    continue\n",
    "                else :\n",
    "                    word_emebedding.append(subwords_embedding)\n",
    "            word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "            #-------------------------FORWARD----------------------------------\n",
    "            x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "                        perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "                        predicate_index = sample_batched[\"predicate_index\"],\n",
    "                        pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "                        predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])   \n",
    "\n",
    "\n",
    "            b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "            loss = nll_loss(logSotfMax(x),gt)\n",
    "            total_loss = total_loss + loss\n",
    "            #-------------------------RESULT STORING----------------------------------\n",
    "            predicted = torch.argmax(x, dim=1)\n",
    "            p += predicted.tolist()\n",
    "            g += gt.tolist()\n",
    "            counter += 1 \n",
    "    \n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    avg_eval_loss = total_loss/counter\n",
    "\n",
    "    if avg_eval_loss < max_val_loss:\n",
    "        max_val_loss = avg_eval_loss\n",
    "    else :\n",
    "        patience_counter += 1\n",
    "    \n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    if patience_counter >= patience :  \n",
    "\n",
    "\n",
    "        print(\"Early stopping at epoch : \",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 avg eval :\",f1_avg)\n",
    "        break\n",
    "    else :\n",
    "        print(\"EPOCHS :\",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 avg eval :\",f1_avg)\n",
    "    \n",
    "\n",
    "    writer.add_scalar(\"Loss/validation\", avg_eval_loss, epoch)\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"Eval_EN/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"Eval_EN/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if f1_avg > f1_score_max:\n",
    "        f1_score_max = f1_avg\n",
    "        print(\"SAVED :\",PATH)\n",
    "        torch.save(model.state_dict(),PATH)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Transfert Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning over Structural Information over English dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading English Pretrained Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#note that parameter EN is only used for tracking on which dataset was/is trained\n",
    "model = Arg_Classifier(cfg).cuda()\n",
    "model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language constained training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_language_constrains()\n",
    "#model.freeze_parts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs n. 0\n",
      "F1 train: [0.60040568 0.57312253 0.06504065 0.         0.         0.77299011\n",
      " 0.41314554 0.         0.73040578 0.20740741 0.18181818 0.99051611\n",
      " 0.47439353 0.80747382 0.         0.60424469 0.15625    0.29916898\n",
      " 0.17777778 0.32822757 0.11940299 0.26       0.51419032 0.27968338\n",
      " 0.37821782 0.         0.        ]\n",
      "EPOCHS : 0\n",
      "F1 eval : [0.77070064 0.62626263 0.25       0.         0.         0.84567901\n",
      " 0.61135371 0.         0.79694073 0.22222222 0.         0.99324028\n",
      " 0.59744991 0.83655982 0.         0.85139319 0.33333333 0.41558442\n",
      " 0.50980392 0.47191011 0.21052632 0.48192771 0.64220183 0.55670103\n",
      " 0.48407643 0.         0.        ]\n",
      "F1 eval : 0.9818441786344059\n",
      "identification {'true_positives': 4204, 'false_positives': 452, 'false_negatives': 859, 'precision': 0.9029209621993127, 'recall': 0.8303377444203042, 'f1': 0.8651095791748122}\n",
      "classification_result {'true_positives': 3752, 'false_positives': 904, 'false_negatives': 1311, 'precision': 0.8058419243986255, 'recall': 0.7410626111001383, 'f1': 0.7720958946393663}\n",
      "Epochs n. 1\n",
      "F1 train: [0.60910945 0.51046025 0.12403101 0.         0.         0.76842889\n",
      " 0.41923077 0.         0.73668334 0.18587361 0.10526316 0.99055178\n",
      " 0.48715678 0.80543359 0.         0.60606061 0.26086957 0.27810651\n",
      " 0.13259669 0.38427948 0.02941176 0.21052632 0.51741294 0.34010152\n",
      " 0.40647118 0.         0.        ]\n",
      "EPOCHS : 1\n",
      "F1 eval : [0.7788162  0.6185567  0.25       0.         0.         0.84792627\n",
      " 0.61403509 0.         0.79633168 0.22641509 0.         0.99325393\n",
      " 0.59205776 0.83616887 0.         0.85321101 0.28571429 0.42105263\n",
      " 0.5        0.47191011 0.22222222 0.48101266 0.62857143 0.55670103\n",
      " 0.48407643 0.         0.        ]\n",
      "F1 eval : 0.9818903528961789\n",
      "identification {'true_positives': 4222, 'false_positives': 458, 'false_negatives': 856, 'precision': 0.9021367521367522, 'recall': 0.8314296967309964, 'f1': 0.8653412584546014}\n",
      "classification_result {'true_positives': 3769, 'false_positives': 911, 'false_negatives': 1309, 'precision': 0.8053418803418804, 'recall': 0.7422213469870027, 'f1': 0.7724943635990982}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "currentDateAndTime = datetime.now()\n",
    "_id = str(currentDateAndTime.year)+\"_\"+str(currentDateAndTime.month)+\"_\"+str(currentDateAndTime.day)+\"_\"+str(currentDateAndTime.hour)+\"_\"+str(currentDateAndTime.minute)+\"_\"+str(currentDateAndTime.second)\n",
    "_id =  _id +\"Language_constained_training\"\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 0.00000005)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "\n",
    "logSotfMax = torch.nn.LogSoftmax(dim=1)\n",
    "nll_loss = torch.nn.NLLLoss()\n",
    "\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "dataloader_dev = DataLoader(dev_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "mapping = dataloader_train.dataset.args_roles\n",
    "\n",
    "auto_model.eval()\n",
    "\n",
    "EPOCHS = 2\n",
    "patience_counter = 0\n",
    "patience = 5\n",
    "max_val_loss = 9999\n",
    "f1_score_max = 0\n",
    "output_path = \"hw2/stud/saved\"\n",
    "model_name = \"model_\"+_id+\".pth\"\n",
    "PATH = os.path.join(output_path,model_name)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    #TRAINING\n",
    "    p = []\n",
    "    g = []\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_train):\n",
    "        #print(sample_batched)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "       \n",
    "        #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        #sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx_bis\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "        #------------------FILTERING SUB-WORDS----------------------\n",
    "        subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "        word_emebedding = []\n",
    "        for i in range(n):\n",
    "            subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "            flag = subtoken_mask[0,i,0]\n",
    "            if flag :\n",
    "                continue\n",
    "            else :\n",
    "                word_emebedding.append(subwords_embedding)\n",
    "        word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "        #-------------------------FORWARD/BACKWARD----------------------------------\n",
    "        x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "            perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "            predicate_index = sample_batched[\"predicate_index\"],\n",
    "            pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "            predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])        \n",
    "        b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "        loss = nll_loss(logSotfMax(x),gt)\n",
    "        total_loss = total_loss + loss\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "        counter += 1 \n",
    "            \n",
    "\n",
    "        #-------------------------RESULT STORING----------------------------------\n",
    "        predicted = torch.argmax(x, dim=1).cpu()\n",
    "        b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "\n",
    "\n",
    "        p += predicted.tolist()\n",
    "        g += gt.tolist()\n",
    "    \n",
    "\n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    print(\"Epochs n.\", epoch)\n",
    "    print(\"F1 train:\",f1_score(g, p, average=None))\n",
    "    scheduler.step()\n",
    "    avg_train_loss = total_loss/counter\n",
    "    writer.add_scalar(\"Loss/train\", avg_train_loss, epoch)\n",
    "\n",
    "\n",
    "\n",
    "    #EVALUATION\n",
    "    p = []\n",
    "    g = []\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_dev):\n",
    "    \n",
    "      #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        #sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx_bis\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "            #------------------FILTERING SUB-WORDS----------------------\n",
    "            subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "            word_emebedding = []\n",
    "            for i in range(n): \n",
    "                subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "                flag = subtoken_mask[0,i,0]\n",
    "                if flag :\n",
    "                    continue\n",
    "                else :\n",
    "                    word_emebedding.append(subwords_embedding)\n",
    "            word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "            #-------------------------FORWARD----------------------------------\n",
    "            x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "                        perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "                        predicate_index = sample_batched[\"predicate_index\"],\n",
    "                        pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "                        predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])   \n",
    "\n",
    "\n",
    "            b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "            loss = nll_loss(logSotfMax(x),gt)\n",
    "            total_loss = total_loss + loss\n",
    "            #-------------------------RESULT STORING----------------------------------\n",
    "            predicted = torch.argmax(x, dim=1)\n",
    "            p += predicted.tolist()\n",
    "            g += gt.tolist()\n",
    "            counter += 1 \n",
    "    \n",
    "    #-------------------------RESULTS----------------------------------\n",
    "\n",
    "    avg_eval_loss = total_loss/counter\n",
    "\n",
    "    if avg_eval_loss < max_val_loss:\n",
    "        max_val_loss = avg_eval_loss\n",
    "    else :\n",
    "        patience_counter += 1\n",
    "    \n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    if patience_counter >= patience :  \n",
    "\n",
    "\n",
    "        print(\"Early stopping at epoch : \",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 eval :\",f1_avg)\n",
    "        break\n",
    "    else :\n",
    "        print(\"EPOCHS :\",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 eval :\",f1_avg)\n",
    "    \n",
    "\n",
    "    writer.add_scalar(\"Loss/validation\", avg_eval_loss, epoch)\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"Eval_EN/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"Eval_EN/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if f1_avg > f1_score_max:\n",
    "        f1_score_max = f1_avg\n",
    "        torch.save(model.state_dict(),PATH)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spanish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Spanish Dataset\n",
    "bert-base-multilingual-cased\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model class is      : <class 'transformers.models.bert.modeling_bert.BertModel'>\n",
      "\n",
      "model class is      : <class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import random\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "\n",
    "auto_model = AutoModel.from_pretrained(\"bert-base-multilingual-cased\",output_hidden_states=True)\n",
    "print(f\"\\nmodel class is      : {type(auto_model)}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "print(f\"\\nmodel class is      : {type(tokenizer)}\")\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import random\n",
    "from typing import Dict\n",
    "\n",
    "class SRL(Dataset):\n",
    " \n",
    "    def __init__(self,language,tokenizer,path,args_roles = None,pos_list = None,predicate_dis = None) -> None:\n",
    "        #train\n",
    "        #self.path_root = 'data'\n",
    "        #inference \n",
    "        self.path_root = 'hw2/stud/data'\n",
    "        #self.path_root = 'stud/data'\n",
    "        self.load_data(language,path)\n",
    "        if args_roles is None :\n",
    "            self.args_roles,self.list_broken_id = self.list_arg_roles()\n",
    "            self.args_roles.append(\"UNK\")\n",
    "        else : \n",
    "            self.args_roles = args_roles\n",
    "            _,self.list_broken_id = self.list_arg_roles()\n",
    "        \n",
    "\n",
    "        if pos_list is None :\n",
    "            self.pos_list,_ = self.list_pos()\n",
    "            self.pos_list.append(\"Nothing\")\n",
    "            self.pos_list.append(\"UNK\")\n",
    "        else : \n",
    "            self.pos_list = pos_list\n",
    "        \n",
    "\n",
    "\n",
    "        if predicate_dis is None :\n",
    "            self.predicate_dis,_ = self.list_predicate_roles()\n",
    "            self.predicate_dis.append(\"Nothing\")\n",
    "            self.predicate_dis.append(\"UNK\")\n",
    "        else : \n",
    "            self.predicate_dis = predicate_dis\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def load_data(self,language,mode):\n",
    "        \n",
    "        mode = mode+\".json\"\n",
    "        path = os.path.join(self.path_root,language,mode)\n",
    "        data_file = open(path)\n",
    "       \n",
    "        data_ = json.load(data_file)\n",
    "\n",
    "        list_data = []\n",
    "\n",
    "        for data in data_:\n",
    "            list_data.append(data_[data])\n",
    "        \n",
    "\n",
    "        self.data = list_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, id : int):\n",
    "\n",
    "        flag = False\n",
    "        if id in self.list_broken_id :\n",
    "            flag = True\n",
    "            while flag == True:\n",
    "\n",
    "                rand_id = random.randint(0, len(self.data)-1)\n",
    "                \n",
    "                if rand_id in self.list_broken_id :\n",
    "                    pass\n",
    "                else :\n",
    "                    flag = False\n",
    "                    id = rand_id        \n",
    "\n",
    "\n",
    "        data = self.pre_processing(self.data[id])\n",
    "        data = self.processig(data)\n",
    "        return data\n",
    "        \n",
    "    def pre_processing(self, data:dict):\n",
    "        data_list = []\n",
    "        for role in data[\"roles\"]:\n",
    "            dictionary = dict()\n",
    "            dictionary[\"words\"] = data[\"words\"]\n",
    "            dictionary[\"role\"] = data[\"roles\"][role]\n",
    "            dictionary[\"pre_idx\"] = role\n",
    "            dictionary[\"pos_tags\"] = data[\"pos_tags\"]\n",
    "            dictionary[\"predicate_meaning\"] = data[\"predicates\"]\n",
    "            data_list.append(dictionary)    \n",
    "        return data_list\n",
    "    \n",
    "    def processig(self,data_list:list):\n",
    "        \n",
    "        for dictionary in data_list:\n",
    "\n",
    "            #dictionary[\"words\"] = data[\"words\"]\n",
    "            dictionary[\"gt_arg_identification\"] = self.arg_id(dictionary[\"role\"])\n",
    "            dictionary[\"gt_arg_classification\"] = self.arg_class(dictionary[\"role\"])\n",
    "            dictionary[\"pos_idx\"] = self.pos_idx(dictionary[\"pos_tags\"])\n",
    "            dictionary[\"predicate_meaning_idx\"] = self.predicate_meaning_idx(dictionary[\"predicate_meaning\"])\n",
    "        \n",
    "        return data_list\n",
    "   \n",
    "    def list_arg_roles(self):\n",
    "        list_roles = []\n",
    "        list_broken_id = []\n",
    "        for i,element in enumerate(self.data):\n",
    "            flag = True\n",
    "            try : roles = element[\"roles\"]\n",
    "            except : flag = False\n",
    "            if flag :\n",
    "                for e in roles:\n",
    "                    sentence = element[\"roles\"][e]\n",
    "\n",
    "                    for word in sentence:\n",
    "                        \n",
    "                        list_roles.append(word)\n",
    "                list_roles = list(set(list_roles))\n",
    "            else : \n",
    "                list_broken_id.append(i)\n",
    "        return list_roles,list_broken_id\n",
    "\n",
    "    def list_predicate_roles(self):\n",
    "        list_predicate_roles = []\n",
    "        list_broken_id = []\n",
    "        for i,element in enumerate(self.data):\n",
    "            flag = True\n",
    "            try : predicates = element[\"predicates\"]\n",
    "            except : flag = False\n",
    "            if flag :\n",
    "                for pre in predicates:\n",
    "                    list_predicate_roles.append(pre)\n",
    "                list_predicate_roles = list(set(list_predicate_roles))\n",
    "            else : \n",
    "                list_broken_id.append(i)\n",
    "        return list_predicate_roles,list_broken_id\n",
    "\n",
    "    def list_pos(self):\n",
    "        list_pos = []\n",
    "        list_broken_id = []\n",
    "        for i,element in enumerate(self.data):\n",
    "            flag = True\n",
    "            try : pos = element[\"pos_tags\"]\n",
    "            except : flag = False\n",
    "            if flag :\n",
    "                for e in pos:\n",
    "                    list_pos.append(e)\n",
    "                list_pos = list(set(list_pos))\n",
    "            else : \n",
    "                list_broken_id.append(i)\n",
    "        return list_pos,list_broken_id\n",
    "  \n",
    "    def arg_class(self,role:list):\n",
    "        list_idxs = []\n",
    "        for element in role:\n",
    "            try : list_idxs.append(self.args_roles.index(element))\n",
    "            except : list_idxs.append(self.args_roles.index(\"UNK\"))\n",
    "        \n",
    "\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "\n",
    "    def arg_id(self,role:dict):\n",
    "        list_idxs = []\n",
    "        for element in role:\n",
    "            if element == \"_\":\n",
    "                list_idxs.append(0)\n",
    "            else :\n",
    "                list_idxs.append(1)\n",
    "\n",
    "        \n",
    "\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "\n",
    "    def pos_idx(self,pos_tags:dict):\n",
    "        list_idxs = []\n",
    "        list_idxs.append(self.pos_list.index(\"Nothing\"))\n",
    "\n",
    "        for element in pos_tags:\n",
    "            try :list_idxs.append(self.pos_list.index(element))\n",
    "            except :list_idxs.append(self.pos_list.index(\"UNK\"))\n",
    "        \n",
    "        list_idxs.append(self.pos_list.index(\"Nothing\"))\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "    \n",
    "    def predicate_meaning_idx(self,predicate_meaning_tags:dict):\n",
    "        list_idxs = []\n",
    "        list_idxs.append(self.predicate_dis.index(\"Nothing\"))\n",
    "\n",
    "        for element in predicate_meaning_tags:\n",
    "            try : list_idxs.append(self.predicate_dis.index(element))\n",
    "            except : list_idxs.append(self.predicate_dis.index(\"UNK\"))\n",
    "            \n",
    "        \n",
    "        list_idxs.append(self.predicate_dis.index(\"Nothing\"))\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "   \n",
    "    def role_gen(self,sentence):\n",
    "\n",
    "        base = [\"_\"]*len(sentence[\"predicates\"])\n",
    "        roles_dict = dict()\n",
    "        counter = 0\n",
    "        for i,item in enumerate(sentence[\"predicates\"]):\n",
    "\n",
    "            if item != \"_\":\n",
    "                base = [\"_\"]*len(sentence[\"predicates\"])\n",
    "                sentence[\"roles\"] = 10\n",
    "                roles_dict[str(i)] = base\n",
    "                counter += 1\n",
    "        \n",
    "        if counter == 0:\n",
    "            sentence[\"roles\"] = { }\n",
    "            flag = False\n",
    "            \n",
    "                \n",
    "        else :\n",
    "            sentence[\"roles\"] = roles_dict\n",
    "            flag = True\n",
    "\n",
    "        return sentence,flag\n",
    "        \n",
    "    def prepare_batch(self,sentence):\n",
    "\n",
    "        sentence,flag = self.role_gen(sentence)\n",
    "        \n",
    "        if flag :\n",
    "\n",
    "            data = self.pre_processing(sentence)\n",
    "            data = self.processig(data)\n",
    "            data = [data]\n",
    "            \n",
    "            \n",
    "            input = dict() \n",
    "            gt = dict()\n",
    "            batch_sentence = [] \n",
    "            \n",
    "            for period in data:\n",
    "                for sentence in period :\n",
    "\n",
    "                    \n",
    "                \n",
    "                    #print(len(sentence[0][\"words\"]))\n",
    "                    pre_idx = int(sentence[\"pre_idx\"])\n",
    "                    \n",
    "\n",
    "                    predicate = sentence[\"words\"][pre_idx]\n",
    "\n",
    "                    text = \" \".join(sentence[\"words\"])\n",
    "                    tokens: list[str] = text.split()\n",
    "                    predicate: list[str] = predicate.split()\n",
    "\n",
    "                    #text = sentence[0][\"words\"]\n",
    "                    \n",
    "                    t = (tokens,predicate)\n",
    "\n",
    "                    batch_sentence.append(t)\n",
    "                \n",
    "                \n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "            batch_output = self.tokenizer.batch_encode_plus(batch_sentence,padding=True,is_split_into_words=True, truncation=True,return_offsets_mapping=True, return_tensors=\"pt\")\n",
    "            \n",
    "\n",
    "\n",
    "            for period in data:\n",
    "\n",
    "                list_positional_predicate_encoding = []\n",
    "                list_predicate_index = [] \n",
    "                list_pos_index = [] \n",
    "                list_arg_gt = []\n",
    "                list_predicate_meaning_index = []\n",
    "                list_meaning_predicate_encoding = []\n",
    "\n",
    "                for sentence in period:\n",
    "                    #positional_encoding\n",
    "                    #+2 per il CLS iniziale ad SEP finale\n",
    "                    sentence_words_lenght =  len(sentence[\"words\"])\n",
    "                    positional_predicate_encoding = torch.zeros(1,sentence_words_lenght+2)\n",
    "                    #+1 per il CLS iniziale\n",
    "                    pre_idx = int(sentence[\"pre_idx\"])\n",
    "                    positional_predicate_encoding[:,pre_idx+1] = 1\n",
    "                    list_positional_predicate_encoding.append(positional_predicate_encoding)\n",
    "                    #print(\"positional_prefix_encoding\",positional_predicate_encoding)\n",
    "                    list_predicate_index.append(pre_idx)\n",
    "\n",
    "                    meaning_predicate_encoding = torch.zeros(1,sentence_words_lenght+2)\n",
    "                    pre_idx = int(sentence[\"pre_idx\"])\n",
    "                    #rather then set the flag 0,1 set with class verb\n",
    "                    meaning_predicate_encoding[:,pre_idx+1] = sentence[\"predicate_meaning_idx\"][pre_idx+1]\n",
    "                    list_meaning_predicate_encoding.append(meaning_predicate_encoding)\n",
    "                    \n",
    "\n",
    "                    pos = torch.unsqueeze(sentence[\"pos_idx\"],dim = 0)\n",
    "                    list_pos_index.append(pos)\n",
    "                    predicate_meaning_idxs = torch.unsqueeze(sentence[\"predicate_meaning_idx\"],dim = 0)\n",
    "                    list_predicate_meaning_index.append(predicate_meaning_idxs)\n",
    "\n",
    "\n",
    "                    arg_gt = torch.unsqueeze(sentence[\"gt_arg_classification\"],dim = 0)\n",
    "                    list_arg_gt.append(arg_gt)\n",
    "\n",
    "\n",
    "            list_arg_gt = torch.cat(list_arg_gt,dim = 0)\n",
    "            list_pos_index = torch.cat(list_pos_index,dim = 0)\n",
    "            list_predicate_meaning_index = torch.cat(list_predicate_meaning_index,dim = 0)\n",
    "            list_positional_predicate_encoding = torch.cat(list_positional_predicate_encoding,dim = 0)\n",
    "            list_predicate_meaning_index_bis = torch.cat(list_meaning_predicate_encoding,dim = 0)\n",
    "            gt[\"arg_gt\"] = list_arg_gt\n",
    "            input[\"predicate_index\"] = list_predicate_index\n",
    "            input[\"pos_index\"] = list_pos_index.long()\n",
    "            input[\"predicate_meaning_idx\"] = list_predicate_meaning_index.long()\n",
    "            input[\"predicate_meaning_idx_bis\"] = list_predicate_meaning_index_bis.long()\n",
    "            offset = batch_output.pop(\"offset_mapping\")\n",
    "            input[\"BERT_input\"] = batch_output\n",
    "            input[\"positional_encoding\"] = list_positional_predicate_encoding.long()\n",
    "            input[\"offset_mapping\"] = offset\n",
    "            input[\"gt\"] = gt\n",
    "        \n",
    "        else :\n",
    "            input = sentence\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return input,flag\n",
    "    \n",
    "# here we define our collate function\n",
    "def collate_fn(batch) -> Dict[str, torch.Tensor]:\n",
    "    #print(batch)\n",
    "    input = dict() \n",
    "    batch_sentence = [] \n",
    "    #print(len(batch))\n",
    "    for period in batch:\n",
    "        for sentence in period :\n",
    "        \n",
    "            #print(len(sentence[0][\"words\"]))\n",
    "            pre_idx = int(sentence[\"pre_idx\"])\n",
    "            \n",
    "\n",
    "            predicate = sentence[\"words\"][pre_idx]\n",
    "\n",
    "            text = \" \".join(sentence[\"words\"])\n",
    "            tokens: list[str] = text.split()\n",
    "            predicate: list[str] = predicate.split()\n",
    "\n",
    "            #text = sentence[0][\"words\"]\n",
    "            \n",
    "            t = (tokens,predicate)\n",
    "\n",
    "            batch_sentence.append(t)\n",
    "            #print(batch_sentence)\n",
    "\n",
    "    batch_output = tokenizer.batch_encode_plus(batch_sentence,padding=True,is_split_into_words=True, truncation=True,return_offsets_mapping=True, return_tensors=\"pt\")\n",
    "    #print(batch_output.keys())\n",
    "\n",
    "\n",
    "    gt = dict()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    for period in batch:\n",
    "\n",
    "        list_positional_predicate_encoding = []\n",
    "        list_arg_gt = []\n",
    "        list_predicate_index = [] \n",
    "        list_pos_index = [] \n",
    "        list_predicate_meaning_index = []\n",
    "        list_meaning_predicate_encoding = []\n",
    "\n",
    "        for sentence in period:\n",
    "            #positional_encoding\n",
    "            #+2 per il CLS iniziale ad SEP finale\n",
    "            sentence_words_lenght =  len(sentence[\"words\"])\n",
    "            positional_predicate_encoding = torch.zeros(1,sentence_words_lenght+2)\n",
    "            #+1 per il CLS iniziale\n",
    "            pre_idx = int(sentence[\"pre_idx\"])\n",
    "            positional_predicate_encoding[:,pre_idx+1] = 1\n",
    "            list_positional_predicate_encoding.append(positional_predicate_encoding)\n",
    "            #print(\"positional_prefix_encoding\",positional_predicate_encoding)\n",
    "            list_predicate_index.append(pre_idx)\n",
    "\n",
    "\n",
    "            meaning_predicate_encoding = torch.zeros(1,sentence_words_lenght+2)\n",
    "            pre_idx = int(sentence[\"pre_idx\"])\n",
    "            #rather then set the flag 0,1 set with class verb\n",
    "            meaning_predicate_encoding[:,pre_idx+1] = sentence[\"predicate_meaning_idx\"][pre_idx+1]\n",
    "            list_meaning_predicate_encoding.append(meaning_predicate_encoding)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            pos = torch.unsqueeze(sentence[\"pos_idx\"],dim = 0)\n",
    "            list_pos_index.append(pos)\n",
    "            predicate_meaning_idxs = torch.unsqueeze(sentence[\"predicate_meaning_idx\"],dim = 0)\n",
    "            list_predicate_meaning_index.append(predicate_meaning_idxs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #note CLS and SEP are discharder after Bi-LSTM, the Classifier takes in input only wokrds hidden state embedding\n",
    "            arg_gt = torch.unsqueeze(sentence[\"gt_arg_classification\"],dim = 0)\n",
    "            list_arg_gt.append(arg_gt)\n",
    "        \n",
    "\n",
    "    list_arg_gt = torch.cat(list_arg_gt,dim = 0)\n",
    "    list_pos_index = torch.cat(list_pos_index,dim = 0)\n",
    "    list_predicate_meaning_index = torch.cat(list_predicate_meaning_index,dim = 0)\n",
    "    list_predicate_meaning_index_bis = torch.cat(list_meaning_predicate_encoding,dim = 0)\n",
    "    list_positional_predicate_encoding = torch.cat(list_positional_predicate_encoding,dim = 0)\n",
    "    gt[\"arg_gt\"] = list_arg_gt\n",
    "    input[\"predicate_index\"] = list_predicate_index\n",
    "    input[\"pos_index\"] = list_pos_index.long()\n",
    "    input[\"predicate_meaning_idx\"] = list_predicate_meaning_index.long()\n",
    "    input[\"predicate_meaning_idx_bis\"] = list_predicate_meaning_index_bis.long()\n",
    "    offset = batch_output.pop(\"offset_mapping\")\n",
    "    input[\"BERT_input\"] = batch_output\n",
    "    input[\"positional_encoding\"] = list_positional_predicate_encoding.long()\n",
    "    input[\"offset_mapping\"] = offset\n",
    "    input[\"gt\"] = gt\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    return input\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#train_dataset = SRL(\"EN\",\"train\")\n",
    "\n",
    "#note here we are directly loading args_roles mapping as computed before the the dasetet where we have perfomerd \n",
    "#EN and ES dataset should have the same consistency in generating \n",
    "train_dataset = SRL(\"ES\",tokenizer,\"train\",train_dataset.args_roles,train_dataset.pos_list,train_dataset.predicate_dis)\n",
    "#same mapping should be used in both the dataset\n",
    "dev_dataset = SRL(\"ES\",tokenizer,\"dev\",train_dataset.args_roles,train_dataset.pos_list,train_dataset.predicate_dis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English-Spanish attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Arg_Classifier(\n",
       "  (bi_lstm_portable): LSTM(132, 50, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (embedding_predicate_flag): Embedding(2, 32, max_norm=True)\n",
       "  (embedding_predicate): Embedding(305, 50, max_norm=True)\n",
       "  (embedding_pos): Embedding(19, 100, max_norm=True)\n",
       "  (bi_lstm): LSTM(950, 50, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (dropout_language_constraint): Dropout(p=0.6, inplace=False)\n",
       "  (dropout_in_classifier): Dropout(p=0.4, inplace=False)\n",
       "  (Relu): ReLU()\n",
       "  (Sigmoid): Sigmoid()\n",
       "  (linear0): Linear(in_features=300, out_features=700, bias=True)\n",
       "  (linear1): Linear(in_features=700, out_features=140, bias=True)\n",
       "  (linear2): Linear(in_features=140, out_features=28, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#note that parameter EN is only used for tracking on which dataset was/is trained, and activate loading of the pretrained head\n",
    "#load the fine-tuned model over english\n",
    "#path fine tuned\n",
    "PATH_FINE = \"hw2/stud/saved/model_2022_12_25_18_11_13Language_constained_training.pth\"\n",
    "model = Arg_Classifier(cfg)\n",
    "model.load_state_dict(torch.load(PATH_FINE))\n",
    "model.train().cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Freezed layer : bi_lstm_portable and embedding \n"
     ]
    }
   ],
   "source": [
    "#check that it should be false\n",
    "print(model.flag_dropout)\n",
    "model.freeze_parts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs n. 0\n",
      "F1 train: [0.03809524 0.         0.         0.         0.         0.34298441\n",
      " 0.         0.         0.40697674 0.         0.98162182 0.01526718\n",
      " 0.36895161 0.         0.10218978 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9482918882374491\n",
      "identification {'true_positives': 790, 'false_positives': 264, 'false_negatives': 1339, 'precision': 0.7495256166982922, 'recall': 0.371066228276186, 'f1': 0.4963870562362551}\n",
      "classification_result {'true_positives': 480, 'false_positives': 574, 'false_negatives': 1649, 'precision': 0.45540796963946867, 'recall': 0.22545796148426492, 'f1': 0.3016022620169651}\n",
      "EPOCHS : 0\n",
      "F1 eval : [0.         0.         0.         0.         0.         0.49646821\n",
      " 0.         0.         0.55330136 0.         0.         0.98514152\n",
      " 0.         0.45804196 0.         0.30208333 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9591695388377525\n",
      "identification {'true_positives': 2092, 'false_positives': 500, 'false_negatives': 2782, 'precision': 0.8070987654320988, 'recall': 0.42921624948707426, 'f1': 0.5604071792124297}\n",
      "classification_result {'true_positives': 1578, 'false_positives': 1014, 'false_negatives': 3296, 'precision': 0.6087962962962963, 'recall': 0.323758719737382, 'f1': 0.42271631395660325}\n",
      "SAVED : hw2/stud/saved/model_2022_12_25_18_14_41SP.pth\n",
      "Epochs n. 1\n",
      "F1 train: [0.         0.         0.         0.         0.         0.48303393\n",
      " 0.         0.         0.52529183 0.         0.98258786 0.01652893\n",
      " 0.40041929 0.         0.23448276 0.         0.         0.\n",
      " 0.         0.         0.         0.11538462 0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9522274686276908\n",
      "identification {'true_positives': 828, 'false_positives': 205, 'false_negatives': 1321, 'precision': 0.8015488867376573, 'recall': 0.38529548627268495, 'f1': 0.5204274041483343}\n",
      "classification_result {'true_positives': 603, 'false_positives': 430, 'false_negatives': 1546, 'precision': 0.5837366892545982, 'recall': 0.2805956258724988, 'f1': 0.3790069138906348}\n",
      "EPOCHS : 1\n",
      "F1 eval : [0.22907489 0.         0.         0.         0.         0.60361135\n",
      " 0.         0.         0.59259259 0.         0.         0.98615035\n",
      " 0.06984127 0.47183099 0.         0.33678756 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.15789474\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9622045868982566\n",
      "identification {'true_positives': 2266, 'false_positives': 452, 'false_negatives': 2636, 'precision': 0.833701250919794, 'recall': 0.46226030191758466, 'f1': 0.594750656167979}\n",
      "classification_result {'true_positives': 1776, 'false_positives': 942, 'false_negatives': 3126, 'precision': 0.6534216335540839, 'recall': 0.3623011015911873, 'f1': 0.4661417322834646}\n",
      "SAVED : hw2/stud/saved/model_2022_12_25_18_14_41SP.pth\n",
      "Epochs n. 2\n",
      "F1 train: [0.09345794 0.09302326 0.         0.         0.         0.53714286\n",
      " 0.         0.         0.56053384 0.         0.98384482 0.04580153\n",
      " 0.46804326 0.         0.28025478 0.         0.         0.\n",
      " 0.         0.         0.         0.10526316 0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9556714696049403\n",
      "identification {'true_positives': 955, 'false_positives': 225, 'false_negatives': 1180, 'precision': 0.809322033898305, 'recall': 0.44730679156908665, 'f1': 0.5761689291101055}\n",
      "classification_result {'true_positives': 708, 'false_positives': 472, 'false_negatives': 1427, 'precision': 0.6, 'recall': 0.33161592505854803, 'f1': 0.4271493212669683}\n",
      "EPOCHS : 2\n",
      "F1 eval : [0.20264317 0.         0.         0.         0.         0.64744646\n",
      " 0.         0.         0.62406312 0.         0.         0.98698653\n",
      " 0.05194805 0.50415755 0.         0.49890591 0.         0.\n",
      " 0.         0.         0.         0.         0.02739726 0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9642722968420745\n",
      "identification {'true_positives': 2384, 'false_positives': 387, 'false_negatives': 2488, 'precision': 0.8603392277156261, 'recall': 0.48932676518883417, 'f1': 0.6238388067512757}\n",
      "classification_result {'true_positives': 1906, 'false_positives': 865, 'false_negatives': 2966, 'precision': 0.6878383255142548, 'recall': 0.3912151067323481, 'f1': 0.49875703257883025}\n",
      "SAVED : hw2/stud/saved/model_2022_12_25_18_14_41SP.pth\n",
      "Epochs n. 3\n",
      "F1 train: [0.13084112 0.13333333 0.         0.         0.         0.56353591\n",
      " 0.         0.         0.60731949 0.         0.98471172 0.03149606\n",
      " 0.53635505 0.         0.4047619  0.         0.         0.\n",
      " 0.08       0.         0.         0.24137931 0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9589177878991596\n",
      "identification {'true_positives': 1008, 'false_positives': 210, 'false_negatives': 1128, 'precision': 0.8275862068965517, 'recall': 0.47191011235955055, 'f1': 0.6010733452593918}\n",
      "classification_result {'true_positives': 798, 'false_positives': 420, 'false_negatives': 1338, 'precision': 0.6551724137931034, 'recall': 0.37359550561797755, 'f1': 0.47584973166368516}\n",
      "EPOCHS : 3\n",
      "F1 eval : [0.26506024 0.02898551 0.         0.         0.         0.65953947\n",
      " 0.         0.         0.6336478  0.         0.         0.98747787\n",
      " 0.03883495 0.55064935 0.         0.56858847 0.         0.\n",
      " 0.         0.         0.         0.         0.21428571 0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9658612342457401\n",
      "identification {'true_positives': 2497, 'false_positives': 383, 'false_negatives': 2382, 'precision': 0.8670138888888889, 'recall': 0.5117852018856323, 'f1': 0.6436396442840572}\n",
      "classification_result {'true_positives': 2035, 'false_positives': 845, 'false_negatives': 2844, 'precision': 0.7065972222222222, 'recall': 0.41709366673498666, 'f1': 0.5245521330068308}\n",
      "SAVED : hw2/stud/saved/model_2022_12_25_18_14_41SP.pth\n",
      "Epochs n. 4\n",
      "F1 train: [0.31496063 0.30188679 0.         0.         0.         0.59217877\n",
      " 0.         0.         0.63336475 0.         0.9856727  0.08888889\n",
      " 0.557924   0.         0.47126437 0.         0.         0.\n",
      " 0.         0.         0.         0.24561404 0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9615552164824345\n",
      "identification {'true_positives': 1095, 'false_positives': 209, 'false_negatives': 1056, 'precision': 0.8397239263803681, 'recall': 0.5090655509065551, 'f1': 0.6338639652677279}\n",
      "classification_result {'true_positives': 878, 'false_positives': 426, 'false_negatives': 1273, 'precision': 0.6733128834355828, 'recall': 0.4081822408182241, 'f1': 0.5082489146164978}\n",
      "EPOCHS : 4\n",
      "F1 eval : [0.4        0.16       0.         0.         0.         0.66557377\n",
      " 0.         0.         0.64534413 0.         0.         0.98800388\n",
      " 0.19075145 0.58086957 0.         0.56617647 0.         0.\n",
      " 0.         0.         0.         0.         0.25581395 0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9676438174437919\n",
      "identification {'true_positives': 2574, 'false_positives': 339, 'false_negatives': 2309, 'precision': 0.8836251287332647, 'recall': 0.5271349580176121, 'f1': 0.6603386351975371}\n",
      "classification_result {'true_positives': 2130, 'false_positives': 783, 'false_negatives': 2753, 'precision': 0.7312049433573635, 'recall': 0.4362072496416138, 'f1': 0.5464340687532068}\n",
      "SAVED : hw2/stud/saved/model_2022_12_25_18_14_41SP.pth\n",
      "Epochs n. 5\n",
      "F1 train: [0.31147541 0.42307692 0.         0.         0.         0.63587922\n",
      " 0.03921569 0.         0.65279092 0.         0.9862184  0.15172414\n",
      " 0.59836809 0.         0.53488372 0.4        0.         0.\n",
      " 0.         0.         0.         0.3880597  0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9635488179339641\n",
      "identification {'true_positives': 1158, 'false_positives': 212, 'false_negatives': 994, 'precision': 0.8452554744525548, 'recall': 0.5381040892193308, 'f1': 0.6575809199318569}\n",
      "classification_result {'true_positives': 956, 'false_positives': 414, 'false_negatives': 1196, 'precision': 0.6978102189781021, 'recall': 0.44423791821561337, 'f1': 0.5428733674048837}\n",
      "EPOCHS : 5\n",
      "F1 eval : [0.3973064  0.27586207 0.         0.         0.         0.65688672\n",
      " 0.02040816 0.         0.65090623 0.05882353 0.         0.98852902\n",
      " 0.26041667 0.62459547 0.         0.56795132 0.         0.\n",
      " 0.         0.         0.         0.         0.27956989 0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9691197041834351\n",
      "identification {'true_positives': 2762, 'false_positives': 415, 'false_negatives': 2134, 'precision': 0.8693736229146994, 'recall': 0.5641339869281046, 'f1': 0.6842561625170321}\n",
      "classification_result {'true_positives': 2277, 'false_positives': 900, 'false_negatives': 2619, 'precision': 0.71671388101983, 'recall': 0.4650735294117647, 'f1': 0.5641025641025641}\n",
      "SAVED : hw2/stud/saved/model_2022_12_25_18_14_41SP.pth\n",
      "Epochs n. 6\n",
      "F1 train: [0.35555556 0.49180328 0.         0.         0.         0.64902998\n",
      " 0.08       0.         0.6869648  0.         0.98705793 0.25477707\n",
      " 0.64304694 0.         0.48888889 0.         0.         0.\n",
      " 0.14814815 0.         0.         0.47368421 0.         0.04166667\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9661906869076455\n",
      "identification {'true_positives': 1231, 'false_positives': 224, 'false_negatives': 913, 'precision': 0.8460481099656357, 'recall': 0.574160447761194, 'f1': 0.6840789108085579}\n",
      "classification_result {'true_positives': 1034, 'false_positives': 421, 'false_negatives': 1110, 'precision': 0.7106529209621993, 'recall': 0.4822761194029851, 'f1': 0.5746040566824118}\n",
      "EPOCHS : 6\n",
      "F1 eval : [0.42253521 0.08823529 0.1        0.         0.         0.67734187\n",
      " 0.03921569 0.         0.65497076 0.0625     0.         0.98874626\n",
      " 0.26525199 0.62989608 0.         0.54885655 0.         0.\n",
      " 0.         0.         0.         0.         0.19512195 0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9693923161150846\n",
      "identification {'true_positives': 2802, 'false_positives': 394, 'false_negatives': 2094, 'precision': 0.876720901126408, 'recall': 0.5723039215686274, 'f1': 0.6925358378645575}\n",
      "classification_result {'true_positives': 2308, 'false_positives': 888, 'false_negatives': 2588, 'precision': 0.7221526908635795, 'recall': 0.4714052287581699, 'f1': 0.5704399406821552}\n",
      "SAVED : hw2/stud/saved/model_2022_12_25_18_14_41SP.pth\n",
      "Epochs n. 7\n",
      "F1 train: [0.39097744 0.59259259 0.         0.         0.         0.61456483\n",
      " 0.04166667 0.         0.68932956 0.1        0.9873104  0.37804878\n",
      " 0.67548501 0.         0.56353591 0.         0.         0.\n",
      " 0.07407407 0.         0.         0.48484848 0.         0.08163265\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9672122145146038\n",
      "identification {'true_positives': 1247, 'false_positives': 213, 'false_negatives': 893, 'precision': 0.8541095890410959, 'recall': 0.5827102803738318, 'f1': 0.6927777777777777}\n",
      "classification_result {'true_positives': 1066, 'false_positives': 394, 'false_negatives': 1074, 'precision': 0.7301369863013699, 'recall': 0.4981308411214953, 'f1': 0.5922222222222222}\n",
      "EPOCHS : 7\n",
      "F1 eval : [0.44011142 0.26506024 0.0952381  0.         0.         0.67546584\n",
      " 0.03883495 0.         0.65441176 0.         0.         0.98871297\n",
      " 0.28717949 0.62318238 0.         0.56653992 0.         0.\n",
      " 0.         0.09836066 0.         0.         0.31683168 0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9696302486195978\n",
      "identification {'true_positives': 2803, 'false_positives': 425, 'false_negatives': 2068, 'precision': 0.8683395291201983, 'recall': 0.5754465202217204, 'f1': 0.6921842202741079}\n",
      "classification_result {'true_positives': 2303, 'false_positives': 925, 'false_negatives': 2568, 'precision': 0.7134448574969021, 'recall': 0.47279819338944773, 'f1': 0.5687121866897147}\n",
      "SAVED : hw2/stud/saved/model_2022_12_25_18_14_41SP.pth\n",
      "Epochs n. 8\n",
      "F1 train: [0.41134752 0.59259259 0.13333333 0.         0.         0.64912281\n",
      " 0.04       0.         0.70961718 0.2        0.98835232 0.4\n",
      " 0.71012007 0.         0.53488372 0.5        0.22222222 0.\n",
      " 0.45714286 0.         0.         0.58333333 0.         0.03703704\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9696588956814537\n",
      "identification {'true_positives': 1332, 'false_positives': 210, 'false_negatives': 804, 'precision': 0.8638132295719845, 'recall': 0.6235955056179775, 'f1': 0.7243066884176181}\n",
      "classification_result {'true_positives': 1140, 'false_positives': 402, 'false_negatives': 996, 'precision': 0.7392996108949417, 'recall': 0.5337078651685393, 'f1': 0.6199021207177814}\n",
      "EPOCHS : 8\n",
      "F1 eval : [0.44984802 0.18421053 0.08       0.         0.         0.66924266\n",
      " 0.02040816 0.         0.65332266 0.05263158 0.         0.9891394\n",
      " 0.27835052 0.66120015 0.         0.56920078 0.         0.\n",
      " 0.         0.09677419 0.         0.         0.25263158 0.\n",
      " 0.02247191 0.        ]\n",
      "F1 avg eval : 0.9703990669313415\n",
      "identification {'true_positives': 2993, 'false_positives': 509, 'false_negatives': 1887, 'precision': 0.8546544831524843, 'recall': 0.6133196721311476, 'f1': 0.7141493676926748}\n",
      "classification_result {'true_positives': 2436, 'false_positives': 1066, 'false_negatives': 2444, 'precision': 0.6956025128498001, 'recall': 0.49918032786885247, 'f1': 0.581245526127416}\n",
      "SAVED : hw2/stud/saved/model_2022_12_25_18_14_41SP.pth\n",
      "Epochs n. 9\n",
      "F1 train: [0.47222222 0.65517241 0.22222222 0.         0.         0.66077739\n",
      " 0.07692308 0.         0.74528302 0.19047619 0.98930383 0.3902439\n",
      " 0.73738238 0.         0.58947368 0.8        0.         0.\n",
      " 0.5        0.         0.         0.57142857 0.         0.07272727\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9717462991841664\n",
      "identification {'true_positives': 1381, 'false_positives': 167, 'false_negatives': 761, 'precision': 0.8921188630490956, 'recall': 0.6447245564892624, 'f1': 0.7485094850948509}\n",
      "classification_result {'true_positives': 1193, 'false_positives': 355, 'false_negatives': 949, 'precision': 0.770671834625323, 'recall': 0.5569561157796452, 'f1': 0.6466124661246613}\n",
      "EPOCHS : 9\n",
      "F1 eval : [0.46979866 0.21686747 0.15384615 0.         0.         0.66214178\n",
      " 0.11428571 0.         0.65189148 0.05405405 0.         0.98886902\n",
      " 0.29367089 0.66690622 0.         0.59302326 0.         0.\n",
      " 0.         0.06896552 0.         0.         0.24489796 0.\n",
      " 0.04301075 0.        ]\n",
      "F1 avg eval : 0.9704329827054247\n",
      "identification {'true_positives': 3091, 'false_positives': 678, 'false_negatives': 1774, 'precision': 0.8201114353940037, 'recall': 0.6353545734840699, 'f1': 0.7160064859856382}\n",
      "classification_result {'true_positives': 2535, 'false_positives': 1234, 'false_negatives': 2330, 'precision': 0.6725921995224198, 'recall': 0.5210688591983555, 'f1': 0.587213342599027}\n",
      "SAVED : hw2/stud/saved/model_2022_12_25_18_14_41SP.pth\n",
      "Epochs n. 10\n",
      "F1 train: [0.5170068  0.49122807 0.25       0.         0.         0.68070175\n",
      " 0.21818182 0.         0.72589792 0.17391304 0.9889716  0.38095238\n",
      " 0.74011775 0.         0.64948454 0.57142857 0.2        0.\n",
      " 0.3125     0.         0.         0.55737705 0.36363636 0.03508772\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9715279995817089\n",
      "identification {'true_positives': 1381, 'false_positives': 195, 'false_negatives': 765, 'precision': 0.876269035532995, 'recall': 0.6435228331780056, 'f1': 0.7420741536808169}\n",
      "classification_result {'true_positives': 1203, 'false_positives': 373, 'false_negatives': 943, 'precision': 0.7633248730964467, 'recall': 0.5605778191985089, 'f1': 0.6464266523374531}\n",
      "Early stopping at epoch :  10\n",
      "F1 eval : [0.41891892 0.21428571 0.11428571 0.         0.         0.66878981\n",
      " 0.05714286 0.         0.65620329 0.19512195 0.         0.9890698\n",
      " 0.28787879 0.65455951 0.         0.6042885  0.         0.\n",
      " 0.         0.06557377 0.         0.         0.29357798 0.\n",
      " 0.07619048 0.        ]\n",
      "F1 avg eval : 0.9704652373787546\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "currentDateAndTime = datetime.now()\n",
    "_id = str(currentDateAndTime.year)+\"_\"+str(currentDateAndTime.month)+\"_\"+str(currentDateAndTime.day)+\"_\"+str(currentDateAndTime.hour)+\"_\"+str(currentDateAndTime.minute)+\"_\"+str(currentDateAndTime.second)\n",
    "_id = _id +\"SP\"\n",
    "\n",
    "\n",
    "\n",
    "#optimizer = torch.optim.Adam(model.parameters(),lr = 0.000005)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "\n",
    "logSotfMax = torch.nn.LogSoftmax(dim=1)\n",
    "nll_loss = torch.nn.NLLLoss()\n",
    "\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "dataloader_dev = DataLoader(dev_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "mapping = dataloader_train.dataset.args_roles\n",
    "\n",
    "auto_model.eval()\n",
    "auto_model.cuda()\n",
    "\n",
    "EPOCHS = 200\n",
    "patience_counter = 0\n",
    "patience = 5\n",
    "max_val_loss = 9999\n",
    "f1_score_max = 0\n",
    "output_path = \"hw2/stud/saved\"\n",
    "model_name = \"model_\"+_id+\".pth\"\n",
    "PATH = os.path.join(output_path,model_name)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    #TRAINING\n",
    "    p = []\n",
    "    g = []\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_train):\n",
    "        #print(sample_batched)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "       \n",
    "        #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        #sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx_bis\"].cuda()\n",
    "\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "        #------------------FILTERING SUB-WORDS----------------------\n",
    "        subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "        word_emebedding = []\n",
    "        for i in range(n):\n",
    "            subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "            flag = subtoken_mask[0,i,0]\n",
    "            if flag :\n",
    "                continue\n",
    "            else :\n",
    "                word_emebedding.append(subwords_embedding)\n",
    "        word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "        #-------------------------FORWARD/BACKWARD----------------------------------\n",
    "        x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "            perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "            predicate_index = sample_batched[\"predicate_index\"],\n",
    "            pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "            predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])        \n",
    "        b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "        loss = nll_loss(logSotfMax(x),gt)\n",
    "        total_loss = total_loss + loss\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "        counter += 1 \n",
    "            \n",
    "\n",
    "        #-------------------------RESULT STORING----------------------------------\n",
    "        predicted = torch.argmax(x, dim=1)\n",
    "        p += predicted.tolist()\n",
    "        g += gt.tolist()\n",
    "    \n",
    "\n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    scheduler.step()\n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    print(\"Epochs n.\", epoch)\n",
    "    print(\"F1 train:\",f1)\n",
    "    print(\"F1 avg train:\",f1_avg)\n",
    "    \n",
    "    avg_train_loss = total_loss/counter\n",
    "    writer.add_scalar(\"EN_Loss_ES/train\", avg_train_loss, epoch)\n",
    "\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"EN_Train_ES/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"EN_Train_ES/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "    #EVALUATION\n",
    "    p = []\n",
    "    g = []\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_dev):\n",
    "    \n",
    "      #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        #sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx_bis\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "            #------------------FILTERING SUB-WORDS----------------------\n",
    "            subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "            word_emebedding = []\n",
    "            for i in range(n): \n",
    "                subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "                flag = subtoken_mask[0,i,0]\n",
    "                if flag :\n",
    "                    continue\n",
    "                else :\n",
    "                    word_emebedding.append(subwords_embedding)\n",
    "            word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "            #-------------------------FORWARD----------------------------------\n",
    "            x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "                        perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "                        predicate_index = sample_batched[\"predicate_index\"],\n",
    "                        pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "                        predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])   \n",
    "\n",
    "\n",
    "            b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "            loss = nll_loss(logSotfMax(x),gt)\n",
    "            total_loss = total_loss + loss\n",
    "            #-------------------------RESULT STORING----------------------------------\n",
    "            predicted = torch.argmax(x, dim=1)\n",
    "            p += predicted.tolist()\n",
    "            g += gt.tolist()\n",
    "            counter += 1 \n",
    "    \n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    avg_eval_loss = total_loss/counter\n",
    "\n",
    "    if avg_eval_loss < max_val_loss:\n",
    "        max_val_loss = avg_eval_loss\n",
    "    else :\n",
    "        patience_counter += 1\n",
    "    \n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    if patience_counter >= patience :  \n",
    "\n",
    "\n",
    "        print(\"Early stopping at epoch : \",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 avg eval :\",f1_avg)\n",
    "        break\n",
    "    else :\n",
    "        print(\"EPOCHS :\",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 avg eval :\",f1_avg)\n",
    "    \n",
    "\n",
    "    writer.add_scalar(\"EN_Loss_ES/validation\", avg_eval_loss, epoch)\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"EN_Eval_ES/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"EN_Eval_ES/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if f1_avg > f1_score_max:\n",
    "        f1_score_max = f1_avg\n",
    "        print(\"SAVED :\",PATH)\n",
    "        torch.save(model.state_dict(),PATH)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare without Transfert Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs n. 0\n",
      "F1 train: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.17486339 0.         0.97684792 0.\n",
      " 0.0497076  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "F1 avg train: 0.9336283227735425\n",
      "identification {'true_positives': 172, 'false_positives': 77, 'false_negatives': 1968, 'precision': 0.6907630522088354, 'recall': 0.08037383177570094, 'f1': 0.14399330263708668}\n",
      "classification_result {'true_positives': 81, 'false_positives': 168, 'false_negatives': 2059, 'precision': 0.3253012048192771, 'recall': 0.03785046728971963, 'f1': 0.06781079949769779}\n",
      "EPOCHS : 0\n",
      "F1 eval : [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.37042925 0.         0.         0.98274541\n",
      " 0.         0.11848341 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9466507837331721\n",
      "identification {'true_positives': 1210, 'false_positives': 164, 'false_negatives': 3687, 'precision': 0.8806404657933042, 'recall': 0.24709005513579743, 'f1': 0.38590336469462605}\n",
      "classification_result {'true_positives': 566, 'false_positives': 808, 'false_negatives': 4331, 'precision': 0.4119359534206696, 'recall': 0.11558096793955483, 'f1': 0.18051347472492424}\n",
      "Epochs n. 1\n",
      "F1 train: [0.         0.         0.         0.         0.         0.00664452\n",
      " 0.         0.         0.37383178 0.         0.98185087 0.\n",
      " 0.4372549  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.946477159623451\n",
      "identification {'true_positives': 771, 'false_positives': 221, 'false_negatives': 1374, 'precision': 0.7772177419354839, 'recall': 0.3594405594405594, 'f1': 0.4915524386356391}\n",
      "classification_result {'true_positives': 444, 'false_positives': 548, 'false_negatives': 1701, 'precision': 0.4475806451612903, 'recall': 0.206993006993007, 'f1': 0.2830729996812241}\n",
      "EPOCHS : 1\n",
      "F1 eval : [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.45604084 0.         0.         0.987258\n",
      " 0.         0.53419355 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9573680592229238\n",
      "identification {'true_positives': 2548, 'false_positives': 465, 'false_negatives': 2369, 'precision': 0.8456687686691006, 'recall': 0.5182021557860484, 'f1': 0.6426229508196722}\n",
      "classification_result {'true_positives': 1425, 'false_positives': 1588, 'false_negatives': 3492, 'precision': 0.4729505476269499, 'recall': 0.28981086028065894, 'f1': 0.3593947036569987}\n",
      "Epochs n. 2\n",
      "F1 train: [0.         0.         0.         0.         0.         0.02507837\n",
      " 0.         0.         0.45425868 0.         0.98473291 0.\n",
      " 0.60033167 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9529170082042079\n",
      "identification {'true_positives': 1039, 'false_positives': 238, 'false_negatives': 1121, 'precision': 0.8136256851996868, 'recall': 0.4810185185185185, 'f1': 0.6045970322956066}\n",
      "classification_result {'true_positives': 654, 'false_positives': 623, 'false_negatives': 1506, 'precision': 0.5121378230227095, 'recall': 0.30277777777777776, 'f1': 0.3805644457375618}\n",
      "EPOCHS : 2\n",
      "F1 eval : [0.         0.         0.         0.         0.         0.08510638\n",
      " 0.         0.         0.47235611 0.         0.         0.98839038\n",
      " 0.         0.61841607 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9603851768729754\n",
      "identification {'true_positives': 2850, 'false_positives': 540, 'false_negatives': 2044, 'precision': 0.8407079646017699, 'recall': 0.5823457294646506, 'f1': 0.6880733944954128}\n",
      "classification_result {'true_positives': 1679, 'false_positives': 1711, 'false_negatives': 3215, 'precision': 0.4952802359882006, 'recall': 0.34307315079689416, 'f1': 0.40535972959922745}\n",
      "Epochs n. 3\n",
      "F1 train: [0.02083333 0.         0.         0.         0.         0.06153846\n",
      " 0.         0.         0.47641509 0.         0.98594461 0.03174603\n",
      " 0.66554054 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9553718806994993\n",
      "identification {'true_positives': 1097, 'false_positives': 183, 'false_negatives': 1041, 'precision': 0.85703125, 'recall': 0.5130963517305893, 'f1': 0.641895845523698}\n",
      "classification_result {'true_positives': 710, 'false_positives': 570, 'false_negatives': 1428, 'precision': 0.5546875, 'recall': 0.33208606173994387, 'f1': 0.41544763019309533}\n",
      "EPOCHS : 3\n",
      "F1 eval : [0.         0.         0.         0.         0.         0.10466761\n",
      " 0.         0.         0.50744559 0.         0.         0.98940468\n",
      " 0.19767442 0.62452972 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9623892332760388\n",
      "identification {'true_positives': 2989, 'false_positives': 463, 'false_negatives': 1871, 'precision': 0.865874855156431, 'recall': 0.6150205761316873, 'f1': 0.7192011549566891}\n",
      "classification_result {'true_positives': 1787, 'false_positives': 1665, 'false_negatives': 3073, 'precision': 0.5176709154113557, 'recall': 0.3676954732510288, 'f1': 0.4299807507218479}\n",
      "Epochs n. 4\n",
      "F1 train: [0.01980198 0.         0.         0.         0.         0.14835165\n",
      " 0.         0.         0.49092344 0.         0.98769816 0.23129252\n",
      " 0.71567044 0.         0.01626016 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9589902660301024\n",
      "identification {'true_positives': 1239, 'false_positives': 169, 'false_negatives': 898, 'precision': 0.8799715909090909, 'recall': 0.5797847449695835, 'f1': 0.6990126939351199}\n",
      "classification_result {'true_positives': 800, 'false_positives': 608, 'false_negatives': 1337, 'precision': 0.5681818181818182, 'recall': 0.37435657463734207, 'f1': 0.4513399153737659}\n",
      "EPOCHS : 4\n",
      "F1 eval : [0.11764706 0.         0.         0.         0.         0.10084034\n",
      " 0.         0.         0.51652016 0.         0.         0.98944327\n",
      " 0.24109589 0.65471924 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.963153888605081\n",
      "identification {'true_positives': 2843, 'false_positives': 300, 'false_negatives': 2038, 'precision': 0.9045497931912185, 'recall': 0.5824626101208769, 'f1': 0.7086241276171485}\n",
      "classification_result {'true_positives': 1766, 'false_positives': 1377, 'false_negatives': 3115, 'precision': 0.5618835507476933, 'recall': 0.36181110428190943, 'f1': 0.44017946161515453}\n",
      "Epochs n. 5\n",
      "F1 train: [0.22033898 0.         0.         0.         0.         0.22058824\n",
      " 0.         0.         0.52607362 0.         0.98845329 0.3803681\n",
      " 0.74821287 0.         0.14814815 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9624650511658759\n",
      "identification {'true_positives': 1337, 'false_positives': 194, 'false_negatives': 823, 'precision': 0.8732854343566296, 'recall': 0.6189814814814815, 'f1': 0.7244649146572745}\n",
      "classification_result {'true_positives': 913, 'false_positives': 618, 'false_negatives': 1247, 'precision': 0.5963422599608099, 'recall': 0.42268518518518516, 'f1': 0.4947168788946085}\n",
      "EPOCHS : 5\n",
      "F1 eval : [0.27433628 0.         0.         0.         0.         0.12396694\n",
      " 0.         0.         0.51751208 0.         0.         0.98959303\n",
      " 0.28927681 0.64575363 0.         0.15116279 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9641459592349468\n",
      "identification {'true_positives': 2964, 'false_positives': 380, 'false_negatives': 1924, 'precision': 0.8863636363636364, 'recall': 0.6063829787234043, 'f1': 0.7201166180758017}\n",
      "classification_result {'true_positives': 1861, 'false_positives': 1483, 'false_negatives': 3027, 'precision': 0.5565191387559809, 'recall': 0.38072831423895254, 'f1': 0.4521379980563654}\n",
      "Epochs n. 6\n",
      "F1 train: [0.26666667 0.         0.         0.         0.         0.35318275\n",
      " 0.         0.         0.52265372 0.         0.98950155 0.34444444\n",
      " 0.76183088 0.         0.16551724 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9646280225355381\n",
      "identification {'true_positives': 1418, 'false_positives': 188, 'false_negatives': 740, 'precision': 0.8829389788293898, 'recall': 0.6570898980537535, 'f1': 0.7534537725823592}\n",
      "classification_result {'true_positives': 959, 'false_positives': 647, 'false_negatives': 1199, 'precision': 0.5971357409713575, 'recall': 0.44439295644114923, 'f1': 0.5095642933049948}\n",
      "EPOCHS : 6\n",
      "F1 eval : [0.30769231 0.         0.         0.         0.         0.20432692\n",
      " 0.         0.         0.51781628 0.         0.         0.9896911\n",
      " 0.31325301 0.68061263 0.         0.23655914 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9654625627559774\n",
      "identification {'true_positives': 2964, 'false_positives': 365, 'false_negatives': 1910, 'precision': 0.8903574647041154, 'recall': 0.6081247435371359, 'f1': 0.7226624405705231}\n",
      "classification_result {'true_positives': 1933, 'false_positives': 1396, 'false_negatives': 2941, 'precision': 0.5806548513066987, 'recall': 0.3965941731637259, 'f1': 0.4712909911008168}\n",
      "Epochs n. 7\n",
      "F1 train: [0.33093525 0.         0.         0.         0.         0.39543726\n",
      " 0.         0.         0.5675446  0.         0.99024407 0.44444444\n",
      " 0.76995305 0.         0.23809524 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9666490394740247\n",
      "identification {'true_positives': 1465, 'false_positives': 173, 'false_negatives': 678, 'precision': 0.8943833943833944, 'recall': 0.6836210919272049, 'f1': 0.77492726791854}\n",
      "classification_result {'true_positives': 1015, 'false_positives': 623, 'false_negatives': 1128, 'precision': 0.6196581196581197, 'recall': 0.47363509099393375, 'f1': 0.5368950013224015}\n",
      "EPOCHS : 7\n",
      "F1 eval : [0.33472803 0.         0.         0.         0.         0.21621622\n",
      " 0.         0.         0.52810047 0.         0.         0.98997963\n",
      " 0.33688699 0.67350158 0.         0.23684211 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9659324834123993\n",
      "identification {'true_positives': 3030, 'false_positives': 362, 'false_negatives': 1842, 'precision': 0.8932783018867925, 'recall': 0.6219211822660099, 'f1': 0.7333010648596322}\n",
      "classification_result {'true_positives': 1951, 'false_positives': 1441, 'false_negatives': 2921, 'precision': 0.5751768867924528, 'recall': 0.4004515599343186, 'f1': 0.47216844143272024}\n",
      "Epochs n. 8\n",
      "F1 train: [0.39705882 0.05555556 0.         0.         0.         0.44883303\n",
      " 0.         0.         0.58985383 0.         0.99114249 0.42990654\n",
      " 0.78434382 0.         0.38888889 0.         0.         0.\n",
      " 0.         0.         0.         0.0952381  0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9688954144139268\n",
      "identification {'true_positives': 1546, 'false_positives': 172, 'false_negatives': 597, 'precision': 0.89988358556461, 'recall': 0.7214185720951937, 'f1': 0.8008288008288008}\n",
      "classification_result {'true_positives': 1090, 'false_positives': 628, 'false_negatives': 1053, 'precision': 0.6344586728754366, 'recall': 0.5086327578161456, 'f1': 0.5646205646205646}\n",
      "Early stopping at epoch :  8\n",
      "F1 eval : [0.34944238 0.         0.         0.         0.         0.2973822\n",
      " 0.         0.         0.52483221 0.         0.         0.98978872\n",
      " 0.2955665  0.68241279 0.         0.32828283 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9665973989197582\n"
     ]
    }
   ],
   "source": [
    "model = Arg_Classifier(cfg).cuda()\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "currentDateAndTime = datetime.now()\n",
    "_id = str(currentDateAndTime.year)+\"_\"+str(currentDateAndTime.month)+\"_\"+str(currentDateAndTime.day)+\"_\"+str(currentDateAndTime.hour)+\"_\"+str(currentDateAndTime.minute)+\"_\"+str(currentDateAndTime.second)+\"WT\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#optimizer = torch.optim.Adam(model.parameters(),lr = 0.000005)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "\n",
    "logSotfMax = torch.nn.LogSoftmax(dim=1)\n",
    "nll_loss = torch.nn.NLLLoss()\n",
    "\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "dataloader_dev = DataLoader(dev_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "mapping = dataloader_train.dataset.args_roles\n",
    "\n",
    "auto_model.eval()\n",
    "auto_model.cuda()\n",
    "\n",
    "EPOCHS = 200\n",
    "patience_counter = 0\n",
    "patience = 5\n",
    "max_val_loss = 9999\n",
    "f1_score_max = 0\n",
    "output_path = \"hw2/stud/saved\"\n",
    "model_name = \"model_\"+_id+\".pth\"\n",
    "PATH = os.path.join(output_path,model_name)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    #TRAINING\n",
    "    p = []\n",
    "    g = []\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_train):\n",
    "        #print(sample_batched)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "       \n",
    "        #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        #sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx_bis\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "        #------------------FILTERING SUB-WORDS----------------------\n",
    "        subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "        word_emebedding = []\n",
    "        for i in range(n):\n",
    "            subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "            flag = subtoken_mask[0,i,0]\n",
    "            if flag :\n",
    "                continue\n",
    "            else :\n",
    "                word_emebedding.append(subwords_embedding)\n",
    "        word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "        #-------------------------FORWARD/BACKWARD----------------------------------\n",
    "        x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "            perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "            predicate_index = sample_batched[\"predicate_index\"],\n",
    "            pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "            predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])        \n",
    "        b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "        loss = nll_loss(logSotfMax(x),gt)\n",
    "        total_loss = total_loss + loss\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "        counter += 1 \n",
    "            \n",
    "\n",
    "        #-------------------------RESULT STORING----------------------------------\n",
    "        predicted = torch.argmax(x, dim=1)\n",
    "        p += predicted.tolist()\n",
    "        g += gt.tolist()\n",
    "    \n",
    "\n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    scheduler.step()\n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    print(\"Epochs n.\", epoch)\n",
    "    print(\"F1 train:\",f1)\n",
    "    print(\"F1 avg train:\",f1_avg)\n",
    "    \n",
    "    avg_train_loss = total_loss/counter\n",
    "    writer.add_scalar(\"Loss_ES/train\", avg_train_loss, epoch)\n",
    "\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"Train_ES/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"Train_ES/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "    #EVALUATION\n",
    "    p = []\n",
    "    g = []\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_dev):\n",
    "    \n",
    "      #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        #sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx_bis\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "            #------------------FILTERING SUB-WORDS----------------------\n",
    "            subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "            word_emebedding = []\n",
    "            for i in range(n): \n",
    "                subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "                flag = subtoken_mask[0,i,0]\n",
    "                if flag :\n",
    "                    continue\n",
    "                else :\n",
    "                    word_emebedding.append(subwords_embedding)\n",
    "            word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "            #-------------------------FORWARD----------------------------------\n",
    "            x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "                        perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "                        predicate_index = sample_batched[\"predicate_index\"],\n",
    "                        pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "                        predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])   \n",
    "\n",
    "\n",
    "            b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "            loss = nll_loss(logSotfMax(x),gt)\n",
    "            total_loss = total_loss + loss\n",
    "            #-------------------------RESULT STORING----------------------------------\n",
    "            predicted = torch.argmax(x, dim=1)\n",
    "            p += predicted.tolist()\n",
    "            g += gt.tolist()\n",
    "            counter += 1 \n",
    "    \n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    avg_eval_loss = total_loss/counter\n",
    "\n",
    "    if avg_eval_loss < max_val_loss:\n",
    "        max_val_loss = avg_eval_loss\n",
    "    else :\n",
    "        patience_counter += 1\n",
    "    \n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    if patience_counter >= patience :  \n",
    "\n",
    "\n",
    "        print(\"Early stopping at epoch : \",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 avg eval :\",f1_avg)\n",
    "        break\n",
    "    else :\n",
    "        print(\"EPOCHS :\",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 avg eval :\",f1_avg)\n",
    "    \n",
    "\n",
    "    writer.add_scalar(\"Loss_ES/validation\", avg_eval_loss, epoch)\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"Eval_ES/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"Eval_ES/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if f1_avg > f1_score_max:\n",
    "        f1_score_max = f1_avg\n",
    "        #print(\"SAVED :\",PATH)\n",
    "        #torch.save(model.state_dict(),PATH)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### French"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New French Dataset\n",
    "bert-base-multilingual-cased\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import random\n",
    "from typing import Dict\n",
    "\n",
    "class SRL(Dataset):\n",
    " \n",
    "    def __init__(self,language,tokenizer,path,args_roles = None,pos_list = None,predicate_dis = None) -> None:\n",
    "        #train\n",
    "        #self.path_root = 'data'\n",
    "        #inference \n",
    "        self.path_root = 'hw2/stud/data'\n",
    "        #self.path_root = 'stud/data'\n",
    "        self.load_data(language,path)\n",
    "        if args_roles is None :\n",
    "            self.args_roles,self.list_broken_id = self.list_arg_roles()\n",
    "            self.args_roles.append(\"UNK\")\n",
    "        else : \n",
    "            self.args_roles = args_roles\n",
    "            _,self.list_broken_id = self.list_arg_roles()\n",
    "        \n",
    "\n",
    "        if pos_list is None :\n",
    "            self.pos_list,_ = self.list_pos()\n",
    "            self.pos_list.append(\"Nothing\")\n",
    "            self.pos_list.append(\"UNK\")\n",
    "        else : \n",
    "            self.pos_list = pos_list\n",
    "        \n",
    "\n",
    "\n",
    "        if predicate_dis is None :\n",
    "            self.predicate_dis,_ = self.list_predicate_roles()\n",
    "            self.predicate_dis.append(\"Nothing\")\n",
    "            self.predicate_dis.append(\"UNK\")\n",
    "        else : \n",
    "            self.predicate_dis = predicate_dis\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def load_data(self,language,mode):\n",
    "        \n",
    "        mode = mode+\".json\"\n",
    "        path = os.path.join(self.path_root,language,mode)\n",
    "        data_file = open(path)\n",
    "       \n",
    "        data_ = json.load(data_file)\n",
    "\n",
    "        list_data = []\n",
    "\n",
    "        for data in data_:\n",
    "            list_data.append(data_[data])\n",
    "        \n",
    "\n",
    "        self.data = list_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, id : int):\n",
    "\n",
    "        flag = False\n",
    "        if id in self.list_broken_id :\n",
    "            flag = True\n",
    "            while flag == True:\n",
    "\n",
    "                rand_id = random.randint(0, len(self.data)-1)\n",
    "                \n",
    "                if rand_id in self.list_broken_id :\n",
    "                    pass\n",
    "                else :\n",
    "                    flag = False\n",
    "                    id = rand_id        \n",
    "\n",
    "\n",
    "        data = self.pre_processing(self.data[id])\n",
    "        data = self.processig(data)\n",
    "        return data\n",
    "        \n",
    "    def pre_processing(self, data:dict):\n",
    "        data_list = []\n",
    "        for role in data[\"roles\"]:\n",
    "            dictionary = dict()\n",
    "            dictionary[\"words\"] = data[\"words\"]\n",
    "            dictionary[\"role\"] = data[\"roles\"][role]\n",
    "            dictionary[\"pre_idx\"] = role\n",
    "            dictionary[\"pos_tags\"] = data[\"pos_tags\"]\n",
    "            dictionary[\"predicate_meaning\"] = data[\"predicates\"]\n",
    "            data_list.append(dictionary)    \n",
    "        return data_list\n",
    "    \n",
    "    def processig(self,data_list:list):\n",
    "        \n",
    "        for dictionary in data_list:\n",
    "\n",
    "            #dictionary[\"words\"] = data[\"words\"]\n",
    "            dictionary[\"gt_arg_identification\"] = self.arg_id(dictionary[\"role\"])\n",
    "            dictionary[\"gt_arg_classification\"] = self.arg_class(dictionary[\"role\"])\n",
    "            dictionary[\"pos_idx\"] = self.pos_idx(dictionary[\"pos_tags\"])\n",
    "            dictionary[\"predicate_meaning_idx\"] = self.predicate_meaning_idx(dictionary[\"predicate_meaning\"])\n",
    "        \n",
    "        return data_list\n",
    "   \n",
    "    def list_arg_roles(self):\n",
    "        list_roles = []\n",
    "        list_broken_id = []\n",
    "        for i,element in enumerate(self.data):\n",
    "            flag = True\n",
    "            try : roles = element[\"roles\"]\n",
    "            except : flag = False\n",
    "            if flag :\n",
    "                for e in roles:\n",
    "                    sentence = element[\"roles\"][e]\n",
    "\n",
    "                    for word in sentence:\n",
    "                        \n",
    "                        list_roles.append(word)\n",
    "                list_roles = list(set(list_roles))\n",
    "            else : \n",
    "                list_broken_id.append(i)\n",
    "        return list_roles,list_broken_id\n",
    "\n",
    "    def list_predicate_roles(self):\n",
    "        list_predicate_roles = []\n",
    "        list_broken_id = []\n",
    "        for i,element in enumerate(self.data):\n",
    "            flag = True\n",
    "            try : predicates = element[\"predicates\"]\n",
    "            except : flag = False\n",
    "            if flag :\n",
    "                for pre in predicates:\n",
    "                    list_predicate_roles.append(pre)\n",
    "                list_predicate_roles = list(set(list_predicate_roles))\n",
    "            else : \n",
    "                list_broken_id.append(i)\n",
    "        return list_predicate_roles,list_broken_id\n",
    "\n",
    "    def list_pos(self):\n",
    "        list_pos = []\n",
    "        list_broken_id = []\n",
    "        for i,element in enumerate(self.data):\n",
    "            flag = True\n",
    "            try : pos = element[\"pos_tags\"]\n",
    "            except : flag = False\n",
    "            if flag :\n",
    "                for e in pos:\n",
    "                    list_pos.append(e)\n",
    "                list_pos = list(set(list_pos))\n",
    "            else : \n",
    "                list_broken_id.append(i)\n",
    "        return list_pos,list_broken_id\n",
    "  \n",
    "    def arg_class(self,role:list):\n",
    "        list_idxs = []\n",
    "        for element in role:\n",
    "            try : list_idxs.append(self.args_roles.index(element))\n",
    "            except : list_idxs.append(self.args_roles.index(\"UNK\"))\n",
    "        \n",
    "\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "\n",
    "    def arg_id(self,role:dict):\n",
    "        list_idxs = []\n",
    "        for element in role:\n",
    "            if element == \"_\":\n",
    "                list_idxs.append(0)\n",
    "            else :\n",
    "                list_idxs.append(1)\n",
    "\n",
    "        \n",
    "\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "\n",
    "    def pos_idx(self,pos_tags:dict):\n",
    "        list_idxs = []\n",
    "        list_idxs.append(self.pos_list.index(\"Nothing\"))\n",
    "\n",
    "        for element in pos_tags:\n",
    "            try :list_idxs.append(self.pos_list.index(element))\n",
    "            except :list_idxs.append(self.pos_list.index(\"UNK\"))\n",
    "        \n",
    "        list_idxs.append(self.pos_list.index(\"Nothing\"))\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "    \n",
    "    def predicate_meaning_idx(self,predicate_meaning_tags:dict):\n",
    "        list_idxs = []\n",
    "        list_idxs.append(self.predicate_dis.index(\"Nothing\"))\n",
    "\n",
    "        for element in predicate_meaning_tags:\n",
    "            try : list_idxs.append(self.predicate_dis.index(element))\n",
    "            except : list_idxs.append(self.predicate_dis.index(\"UNK\"))\n",
    "            \n",
    "        \n",
    "        list_idxs.append(self.predicate_dis.index(\"Nothing\"))\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "   \n",
    "    def role_gen(self,sentence):\n",
    "\n",
    "        base = [\"_\"]*len(sentence[\"predicates\"])\n",
    "        roles_dict = dict()\n",
    "        counter = 0\n",
    "        for i,item in enumerate(sentence[\"predicates\"]):\n",
    "\n",
    "            if item != \"_\":\n",
    "                base = [\"_\"]*len(sentence[\"predicates\"])\n",
    "                sentence[\"roles\"] = 10\n",
    "                roles_dict[str(i)] = base\n",
    "                counter += 1\n",
    "        \n",
    "        if counter == 0:\n",
    "            sentence[\"roles\"] = { }\n",
    "            flag = False\n",
    "            \n",
    "                \n",
    "        else :\n",
    "            sentence[\"roles\"] = roles_dict\n",
    "            flag = True\n",
    "\n",
    "        return sentence,flag\n",
    "        \n",
    "    def prepare_batch(self,sentence):\n",
    "\n",
    "        sentence,flag = self.role_gen(sentence)\n",
    "        \n",
    "        if flag :\n",
    "\n",
    "            data = self.pre_processing(sentence)\n",
    "            data = self.processig(data)\n",
    "            data = [data]\n",
    "            \n",
    "            \n",
    "            input = dict() \n",
    "            gt = dict()\n",
    "            batch_sentence = [] \n",
    "            \n",
    "            for period in data:\n",
    "                for sentence in period :\n",
    "\n",
    "                    \n",
    "                \n",
    "                    #print(len(sentence[0][\"words\"]))\n",
    "                    pre_idx = int(sentence[\"pre_idx\"])\n",
    "                    \n",
    "\n",
    "                    predicate = sentence[\"words\"][pre_idx]\n",
    "\n",
    "                    text = \" \".join(sentence[\"words\"])\n",
    "                    tokens: list[str] = text.split()\n",
    "                    predicate: list[str] = predicate.split()\n",
    "\n",
    "                    #text = sentence[0][\"words\"]\n",
    "                    \n",
    "                    t = (tokens,predicate)\n",
    "\n",
    "                    batch_sentence.append(t)\n",
    "                \n",
    "                \n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "            batch_output = self.tokenizer.batch_encode_plus(batch_sentence,padding=True,is_split_into_words=True, truncation=True,return_offsets_mapping=True, return_tensors=\"pt\")\n",
    "            \n",
    "\n",
    "\n",
    "            for period in data:\n",
    "\n",
    "                list_positional_predicate_encoding = []\n",
    "                list_predicate_index = [] \n",
    "                list_pos_index = [] \n",
    "                list_arg_gt = []\n",
    "                list_predicate_meaning_index = []\n",
    "                list_meaning_predicate_encoding = []\n",
    "\n",
    "                for sentence in period:\n",
    "                    #positional_encoding\n",
    "                    #+2 per il CLS iniziale ad SEP finale\n",
    "                    sentence_words_lenght =  len(sentence[\"words\"])\n",
    "                    positional_predicate_encoding = torch.zeros(1,sentence_words_lenght+2)\n",
    "                    #+1 per il CLS iniziale\n",
    "                    pre_idx = int(sentence[\"pre_idx\"])\n",
    "                    positional_predicate_encoding[:,pre_idx+1] = 1\n",
    "                    list_positional_predicate_encoding.append(positional_predicate_encoding)\n",
    "                    #print(\"positional_prefix_encoding\",positional_predicate_encoding)\n",
    "                    list_predicate_index.append(pre_idx)\n",
    "\n",
    "                    meaning_predicate_encoding = torch.zeros(1,sentence_words_lenght+2)\n",
    "                    pre_idx = int(sentence[\"pre_idx\"])\n",
    "                    #rather then set the flag 0,1 set with class verb\n",
    "                    meaning_predicate_encoding[:,pre_idx+1] = sentence[\"predicate_meaning_idx\"][pre_idx+1]\n",
    "                    list_meaning_predicate_encoding.append(meaning_predicate_encoding)\n",
    "                    \n",
    "\n",
    "                    pos = torch.unsqueeze(sentence[\"pos_idx\"],dim = 0)\n",
    "                    list_pos_index.append(pos)\n",
    "                    predicate_meaning_idxs = torch.unsqueeze(sentence[\"predicate_meaning_idx\"],dim = 0)\n",
    "                    list_predicate_meaning_index.append(predicate_meaning_idxs)\n",
    "\n",
    "\n",
    "                    arg_gt = torch.unsqueeze(sentence[\"gt_arg_classification\"],dim = 0)\n",
    "                    list_arg_gt.append(arg_gt)\n",
    "\n",
    "\n",
    "            list_arg_gt = torch.cat(list_arg_gt,dim = 0)\n",
    "            list_pos_index = torch.cat(list_pos_index,dim = 0)\n",
    "            list_predicate_meaning_index = torch.cat(list_predicate_meaning_index,dim = 0)\n",
    "            list_positional_predicate_encoding = torch.cat(list_positional_predicate_encoding,dim = 0)\n",
    "            list_predicate_meaning_index_bis = torch.cat(list_meaning_predicate_encoding,dim = 0)\n",
    "            gt[\"arg_gt\"] = list_arg_gt\n",
    "            input[\"predicate_index\"] = list_predicate_index\n",
    "            input[\"pos_index\"] = list_pos_index.long()\n",
    "            input[\"predicate_meaning_idx\"] = list_predicate_meaning_index.long()\n",
    "            input[\"predicate_meaning_idx_bis\"] = list_predicate_meaning_index_bis.long()\n",
    "            offset = batch_output.pop(\"offset_mapping\")\n",
    "            input[\"BERT_input\"] = batch_output\n",
    "            input[\"positional_encoding\"] = list_positional_predicate_encoding.long()\n",
    "            input[\"offset_mapping\"] = offset\n",
    "            input[\"gt\"] = gt\n",
    "        \n",
    "        else :\n",
    "            input = sentence\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return input,flag\n",
    "    \n",
    "# here we define our collate function\n",
    "def collate_fn(batch) -> Dict[str, torch.Tensor]:\n",
    "    #print(batch)\n",
    "    input = dict() \n",
    "    batch_sentence = [] \n",
    "    #print(len(batch))\n",
    "    for period in batch:\n",
    "        for sentence in period :\n",
    "        \n",
    "            #print(len(sentence[0][\"words\"]))\n",
    "            pre_idx = int(sentence[\"pre_idx\"])\n",
    "            \n",
    "\n",
    "            predicate = sentence[\"words\"][pre_idx]\n",
    "\n",
    "            text = \" \".join(sentence[\"words\"])\n",
    "            tokens: list[str] = text.split()\n",
    "            predicate: list[str] = predicate.split()\n",
    "\n",
    "            #text = sentence[0][\"words\"]\n",
    "            \n",
    "            t = (tokens,predicate)\n",
    "\n",
    "            batch_sentence.append(t)\n",
    "            #print(batch_sentence)\n",
    "\n",
    "    batch_output = tokenizer.batch_encode_plus(batch_sentence,padding=True,is_split_into_words=True, truncation=True,return_offsets_mapping=True, return_tensors=\"pt\")\n",
    "    #print(batch_output.keys())\n",
    "\n",
    "\n",
    "    gt = dict()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    for period in batch:\n",
    "\n",
    "        list_positional_predicate_encoding = []\n",
    "        list_arg_gt = []\n",
    "        list_predicate_index = [] \n",
    "        list_pos_index = [] \n",
    "        list_predicate_meaning_index = []\n",
    "        list_meaning_predicate_encoding = []\n",
    "\n",
    "        for sentence in period:\n",
    "            #positional_encoding\n",
    "            #+2 per il CLS iniziale ad SEP finale\n",
    "            sentence_words_lenght =  len(sentence[\"words\"])\n",
    "            positional_predicate_encoding = torch.zeros(1,sentence_words_lenght+2)\n",
    "            #+1 per il CLS iniziale\n",
    "            pre_idx = int(sentence[\"pre_idx\"])\n",
    "            positional_predicate_encoding[:,pre_idx+1] = 1\n",
    "            list_positional_predicate_encoding.append(positional_predicate_encoding)\n",
    "            #print(\"positional_prefix_encoding\",positional_predicate_encoding)\n",
    "            list_predicate_index.append(pre_idx)\n",
    "\n",
    "\n",
    "            meaning_predicate_encoding = torch.zeros(1,sentence_words_lenght+2)\n",
    "            pre_idx = int(sentence[\"pre_idx\"])\n",
    "            #rather then set the flag 0,1 set with class verb\n",
    "            meaning_predicate_encoding[:,pre_idx+1] = sentence[\"predicate_meaning_idx\"][pre_idx+1]\n",
    "            list_meaning_predicate_encoding.append(meaning_predicate_encoding)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            pos = torch.unsqueeze(sentence[\"pos_idx\"],dim = 0)\n",
    "            list_pos_index.append(pos)\n",
    "            predicate_meaning_idxs = torch.unsqueeze(sentence[\"predicate_meaning_idx\"],dim = 0)\n",
    "            list_predicate_meaning_index.append(predicate_meaning_idxs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #note CLS and SEP are discharder after Bi-LSTM, the Classifier takes in input only wokrds hidden state embedding\n",
    "            arg_gt = torch.unsqueeze(sentence[\"gt_arg_classification\"],dim = 0)\n",
    "            list_arg_gt.append(arg_gt)\n",
    "        \n",
    "\n",
    "    list_arg_gt = torch.cat(list_arg_gt,dim = 0)\n",
    "    list_pos_index = torch.cat(list_pos_index,dim = 0)\n",
    "    list_predicate_meaning_index = torch.cat(list_predicate_meaning_index,dim = 0)\n",
    "    list_predicate_meaning_index_bis = torch.cat(list_meaning_predicate_encoding,dim = 0)\n",
    "    list_positional_predicate_encoding = torch.cat(list_positional_predicate_encoding,dim = 0)\n",
    "    gt[\"arg_gt\"] = list_arg_gt\n",
    "    input[\"predicate_index\"] = list_predicate_index\n",
    "    input[\"pos_index\"] = list_pos_index.long()\n",
    "    input[\"predicate_meaning_idx\"] = list_predicate_meaning_index.long()\n",
    "    input[\"predicate_meaning_idx_bis\"] = list_predicate_meaning_index_bis.long()\n",
    "    offset = batch_output.pop(\"offset_mapping\")\n",
    "    input[\"BERT_input\"] = batch_output\n",
    "    input[\"positional_encoding\"] = list_positional_predicate_encoding.long()\n",
    "    input[\"offset_mapping\"] = offset\n",
    "    input[\"gt\"] = gt\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    return input\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#train_dataset = SRL(\"EN\",\"train\")\n",
    "\n",
    "#note here we are directly loading args_roles mapping as computed before the the dasetet where we have perfomerd \n",
    "#EN and ES dataset should have the same consistency in generating \n",
    "train_dataset = SRL(\"FR\",tokenizer,\"train\",train_dataset.args_roles,train_dataset.pos_list,train_dataset.predicate_dis)\n",
    "#same mapping should be used in both the dataset\n",
    "dev_dataset = SRL(\"FR\",tokenizer,\"dev\",train_dataset.args_roles,train_dataset.pos_list,train_dataset.predicate_dis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English-French attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Arg_Classifier(\n",
       "  (bi_lstm_portable): LSTM(132, 50, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (embedding_predicate_flag): Embedding(2, 32, max_norm=True)\n",
       "  (embedding_predicate): Embedding(305, 50, max_norm=True)\n",
       "  (embedding_pos): Embedding(19, 100, max_norm=True)\n",
       "  (bi_lstm): LSTM(950, 50, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (dropout_language_constraint): Dropout(p=0.6, inplace=False)\n",
       "  (dropout_in_classifier): Dropout(p=0.4, inplace=False)\n",
       "  (Relu): ReLU()\n",
       "  (Sigmoid): Sigmoid()\n",
       "  (linear0): Linear(in_features=300, out_features=700, bias=True)\n",
       "  (linear1): Linear(in_features=700, out_features=140, bias=True)\n",
       "  (linear2): Linear(in_features=140, out_features=28, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#note that parameter EN is only used for tracking on which dataset was/is trained, and activate loading of the pretrained head\n",
    "#load the fine-tuned model over english\n",
    "model = Arg_Classifier(cfg)\n",
    "model.load_state_dict(torch.load(PATH_FINE))\n",
    "model.train().cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language constained training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezed layer : bi_lstm_portable and embedding \n"
     ]
    }
   ],
   "source": [
    "model.freeze_parts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs n. 0\n",
      "F1 train: [0.         0.0952381  0.         0.         0.         0.34222222\n",
      " 0.         0.         0.43516874 0.         0.         0.98402296\n",
      " 0.         0.41909385 0.         0.20408163 0.         0.\n",
      " 0.         0.06451613 0.         0.         0.08163265 0.\n",
      " 0.         0.         0.        ]\n",
      "F1 avg train: 0.9544972524727422\n",
      "identification {'true_positives': 961, 'false_positives': 285, 'false_negatives': 1324, 'precision': 0.7712680577849117, 'recall': 0.4205689277899344, 'f1': 0.5443217218918154}\n",
      "classification_result {'true_positives': 601, 'false_positives': 645, 'false_negatives': 1684, 'precision': 0.4823434991974318, 'recall': 0.263019693654267, 'f1': 0.3404134806003965}\n",
      "EPOCHS : 0\n",
      "F1 eval : [0.         0.         0.         0.         0.         0.57250471\n",
      " 0.         0.         0.52765058 0.         0.         0.98735714\n",
      " 0.         0.4934688  0.         0.16066482 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "F1 avg eval : 0.9628367196675093\n",
      "identification {'true_positives': 2536, 'false_positives': 524, 'false_negatives': 2558, 'precision': 0.8287581699346406, 'recall': 0.4978405967805261, 'f1': 0.6220259995094433}\n",
      "classification_result {'true_positives': 1762, 'false_positives': 1298, 'false_negatives': 3332, 'precision': 0.5758169934640522, 'recall': 0.3458971338829996, 'f1': 0.43218052489575665}\n",
      "SAVED : saved/model_2022_12_25_18_24_22.pth\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'saved/model_2022_12_25_18_24_22.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 238\u001b[0m\n\u001b[1;32m    236\u001b[0m f1_score_max \u001b[38;5;241m=\u001b[39m f1_avg\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSAVED :\u001b[39m\u001b[38;5;124m\"\u001b[39m,PATH)\n\u001b[0;32m--> 238\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mPATH\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/serialization.py:376\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/serialization.py?line=339'>340</a>\u001b[0m \u001b[39m\"\"\"save(obj, f, pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL, _use_new_zipfile_serialization=True)\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/serialization.py?line=340'>341</a>\u001b[0m \n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/serialization.py?line=341'>342</a>\u001b[0m \u001b[39mSaves an object to a disk file.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/serialization.py?line=371'>372</a>\u001b[0m \u001b[39m    >>> torch.save(x, buffer)\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/serialization.py?line=372'>373</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/serialization.py?line=373'>374</a>\u001b[0m _check_dill_version(pickle_module)\n\u001b[0;32m--> <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/serialization.py?line=375'>376</a>\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mwb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/serialization.py?line=376'>377</a>\u001b[0m     \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/serialization.py?line=377'>378</a>\u001b[0m         \u001b[39mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[39mas\u001b[39;00m opened_zipfile:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/serialization.py?line=227'>228</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/serialization.py?line=228'>229</a>\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/serialization.py?line=229'>230</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/serialization.py?line=230'>231</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/serialization.py?line=231'>232</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/serialization.py?line=209'>210</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/serialization.py?line=210'>211</a>\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'saved/model_2022_12_25_18_24_22.pth'"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "currentDateAndTime = datetime.now()\n",
    "_id = str(currentDateAndTime.year)+\"_\"+str(currentDateAndTime.month)+\"_\"+str(currentDateAndTime.day)+\"_\"+str(currentDateAndTime.hour)+\"_\"+str(currentDateAndTime.minute)+\"_\"+str(currentDateAndTime.second)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#optimizer = torch.optim.Adam(model.parameters(),lr = 0.000005)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "\n",
    "logSotfMax = torch.nn.LogSoftmax(dim=1)\n",
    "nll_loss = torch.nn.NLLLoss()\n",
    "\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "dataloader_dev = DataLoader(dev_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "mapping = dataloader_train.dataset.args_roles\n",
    "\n",
    "auto_model.eval()\n",
    "auto_model.cuda()\n",
    "\n",
    "EPOCHS = 200\n",
    "patience_counter = 0\n",
    "patience = 5\n",
    "max_val_loss = 9999\n",
    "f1_score_max = 0\n",
    "output_path = \"hw2/stud/saved\"\n",
    "model_name = \"model_\"+_id+\".pth\"\n",
    "PATH = os.path.join(output_path,model_name)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    #TRAINING\n",
    "    p = []\n",
    "    g = []\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_train):\n",
    "        #print(sample_batched)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "       \n",
    "        #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        #sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx_bis\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "        #------------------FILTERING SUB-WORDS----------------------\n",
    "        subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "        word_emebedding = []\n",
    "        for i in range(n):\n",
    "            subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "            flag = subtoken_mask[0,i,0]\n",
    "            if flag :\n",
    "                continue\n",
    "            else :\n",
    "                word_emebedding.append(subwords_embedding)\n",
    "        word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "        #-------------------------FORWARD/BACKWARD----------------------------------\n",
    "        x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "            perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "            predicate_index = sample_batched[\"predicate_index\"],\n",
    "            pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "            predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])        \n",
    "        b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "        loss = nll_loss(logSotfMax(x),gt)\n",
    "        total_loss = total_loss + loss\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "        counter += 1 \n",
    "            \n",
    "\n",
    "        #-------------------------RESULT STORING----------------------------------\n",
    "        predicted = torch.argmax(x, dim=1)\n",
    "        p += predicted.tolist()\n",
    "        g += gt.tolist()\n",
    "    \n",
    "\n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    scheduler.step()\n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    print(\"Epochs n.\", epoch)\n",
    "    print(\"F1 train:\",f1)\n",
    "    print(\"F1 avg train:\",f1_avg)\n",
    "    \n",
    "    avg_train_loss = total_loss/counter\n",
    "    writer.add_scalar(\"EN_Loss_ES/train\", avg_train_loss, epoch)\n",
    "\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"EN_Train_ES/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"EN_Train_ES/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "    #EVALUATION\n",
    "    p = []\n",
    "    g = []\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_dev):\n",
    "    \n",
    "      #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        #sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx_bis\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "            #------------------FILTERING SUB-WORDS----------------------\n",
    "            subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "            word_emebedding = []\n",
    "            for i in range(n): \n",
    "                subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "                flag = subtoken_mask[0,i,0]\n",
    "                if flag :\n",
    "                    continue\n",
    "                else :\n",
    "                    word_emebedding.append(subwords_embedding)\n",
    "            word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "            #-------------------------FORWARD----------------------------------\n",
    "            x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "                        perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "                        predicate_index = sample_batched[\"predicate_index\"],\n",
    "                        pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "                        predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])   \n",
    "\n",
    "\n",
    "            b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "            loss = nll_loss(logSotfMax(x),gt)\n",
    "            total_loss = total_loss + loss\n",
    "            #-------------------------RESULT STORING----------------------------------\n",
    "            predicted = torch.argmax(x, dim=1)\n",
    "            p += predicted.tolist()\n",
    "            g += gt.tolist()\n",
    "            counter += 1 \n",
    "    \n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    avg_eval_loss = total_loss/counter\n",
    "\n",
    "    if avg_eval_loss < max_val_loss:\n",
    "        max_val_loss = avg_eval_loss\n",
    "    else :\n",
    "        patience_counter += 1\n",
    "    \n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    if patience_counter >= patience :  \n",
    "\n",
    "\n",
    "        print(\"Early stopping at epoch : \",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 avg eval :\",f1_avg)\n",
    "        break\n",
    "    else :\n",
    "        print(\"EPOCHS :\",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 avg eval :\",f1_avg)\n",
    "    \n",
    "\n",
    "    writer.add_scalar(\"EN_Loss_ES/validation\", avg_eval_loss, epoch)\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"EN_Eval_ES/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"EN_Eval_ES/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if f1_avg > f1_score_max:\n",
    "        f1_score_max = f1_avg\n",
    "        print(\"SAVED :\",PATH)\n",
    "        torch.save(model.state_dict(),PATH)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare without Transfert Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs n. 0\n",
      "F1 train: [0.         0.19946809 0.         0.9768764  0.         0.0295421\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9336015019908596\n",
      "identification {'true_positives': 172, 'false_positives': 71, 'false_negatives': 1987, 'precision': 0.7078189300411523, 'recall': 0.07966651227420102, 'f1': 0.14321398834304747}\n",
      "classification_result {'true_positives': 85, 'false_positives': 158, 'false_negatives': 2074, 'precision': 0.3497942386831276, 'recall': 0.03937007874015748, 'f1': 0.070774354704413}\n",
      "EPOCHS : 0\n",
      "F1 eval : [0.         0.23416618 0.         0.9777718  0.00265252 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9389493973815146\n",
      "identification {'true_positives': 1030, 'false_positives': 1096, 'false_negatives': 3858, 'precision': 0.48447789275634995, 'recall': 0.2107201309328969, 'f1': 0.2936983176504135}\n",
      "classification_result {'true_positives': 405, 'false_positives': 1721, 'false_negatives': 4483, 'precision': 0.1904985888993415, 'recall': 0.08285597381342062, 'f1': 0.11548331907613345}\n",
      "SAVED : saved/model_2022_12_18_22_2_58.pth\n",
      "Epochs n. 1\n",
      "F1 train: [0.         0.3872549  0.         0.98172752 0.         0.42610365\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9464882649310143\n",
      "identification {'true_positives': 798, 'false_positives': 268, 'false_negatives': 1337, 'precision': 0.7485928705440901, 'recall': 0.3737704918032787, 'f1': 0.4985941893158388}\n",
      "classification_result {'true_positives': 459, 'false_positives': 607, 'false_negatives': 1676, 'precision': 0.43058161350844276, 'recall': 0.21498829039812647, 'f1': 0.2867853795688848}\n",
      "EPOCHS : 1\n",
      "F1 eval : [0.         0.23666396 0.         0.97658545 0.00478469 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9376621220958927\n",
      "identification {'true_positives': 1127, 'false_positives': 1421, 'false_negatives': 3759, 'precision': 0.4423076923076923, 'recall': 0.23065902578796563, 'f1': 0.3032015065913371}\n",
      "classification_result {'true_positives': 441, 'false_positives': 2107, 'false_negatives': 4445, 'precision': 0.17307692307692307, 'recall': 0.09025787965616046, 'f1': 0.11864406779661017}\n",
      "Epochs n. 2\n",
      "F1 train: [0.         0.44530046 0.         0.98438053 0.         0.60050463\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.952145225902215\n",
      "identification {'true_positives': 1022, 'false_positives': 243, 'false_negatives': 1124, 'precision': 0.807905138339921, 'recall': 0.4762348555452004, 'f1': 0.5992377601876282}\n",
      "classification_result {'true_positives': 646, 'false_positives': 619, 'false_negatives': 1500, 'precision': 0.5106719367588933, 'recall': 0.30102516309412863, 'f1': 0.37877455291703316}\n",
      "EPOCHS : 2\n",
      "F1 eval : [0.         0.26013195 0.         0.9780147  0.0071599  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9395047264553521\n",
      "identification {'true_positives': 1022, 'false_positives': 1031, 'false_negatives': 3848, 'precision': 0.4978080857282026, 'recall': 0.20985626283367556, 'f1': 0.2952477249747219}\n",
      "classification_result {'true_positives': 420, 'false_positives': 1633, 'false_negatives': 4450, 'precision': 0.20457866536775451, 'recall': 0.08624229979466119, 'f1': 0.12133468149646107}\n",
      "SAVED : saved/model_2022_12_18_22_2_58.pth\n",
      "Epochs n. 3\n",
      "F1 train: [0.         0.47011952 0.02555911 0.98541674 0.         0.63305785\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9541615111910255\n",
      "identification {'true_positives': 1065, 'false_positives': 200, 'false_negatives': 1089, 'precision': 0.841897233201581, 'recall': 0.49442896935933145, 'f1': 0.6229891781222578}\n",
      "classification_result {'true_positives': 682, 'false_positives': 583, 'false_negatives': 1472, 'precision': 0.5391304347826087, 'recall': 0.3166202414113278, 'f1': 0.39894706054401874}\n",
      "EPOCHS : 3\n",
      "F1 eval : [0.         0.28145455 0.02816901 0.97774314 0.00762112 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9396751709187108\n",
      "identification {'true_positives': 871, 'false_positives': 950, 'false_negatives': 4019, 'precision': 0.4783086216364635, 'recall': 0.17811860940695295, 'f1': 0.2595738340038742}\n",
      "classification_result {'true_positives': 404, 'false_positives': 1417, 'false_negatives': 4486, 'precision': 0.22185612300933552, 'recall': 0.08261758691206544, 'f1': 0.12039934436000595}\n",
      "SAVED : saved/model_2022_12_18_22_2_58.pth\n",
      "Epochs n. 4\n",
      "F1 train: [0.         0.51370399 0.05590062 0.98651971 0.         0.69414674\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.07843137 0.\n",
      " 0.         0.         0.03333333 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9570333876174468\n",
      "identification {'true_positives': 1139, 'false_positives': 185, 'false_negatives': 994, 'precision': 0.8602719033232629, 'recall': 0.5339896858884201, 'f1': 0.658952849291293}\n",
      "classification_result {'true_positives': 764, 'false_positives': 560, 'false_negatives': 1369, 'precision': 0.5770392749244713, 'recall': 0.35818096577590247, 'f1': 0.4420017356089094}\n",
      "EPOCHS : 4\n",
      "F1 eval : [0.         0.27561328 0.01457726 0.9769951  0.00616333 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9388430347981687\n",
      "identification {'true_positives': 843, 'false_positives': 1092, 'false_negatives': 4038, 'precision': 0.4356589147286822, 'recall': 0.1727105101413645, 'f1': 0.24735915492957747}\n",
      "classification_result {'true_positives': 393, 'false_positives': 1542, 'false_negatives': 4488, 'precision': 0.20310077519379846, 'recall': 0.08051628764597418, 'f1': 0.11531690140845072}\n",
      "Epochs n. 5\n",
      "F1 train: [0.         0.51257862 0.12105263 0.98804271 0.         0.72871917\n",
      " 0.         0.         0.         0.         0.0620155  0.\n",
      " 0.         0.         0.         0.         0.12612613 0.\n",
      " 0.         0.         0.01652893 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg train: 0.9595282512313569\n",
      "identification {'true_positives': 1263, 'false_positives': 167, 'false_negatives': 879, 'precision': 0.8832167832167832, 'recall': 0.5896358543417367, 'f1': 0.7071668533034715}\n",
      "classification_result {'true_positives': 819, 'false_positives': 611, 'false_negatives': 1323, 'precision': 0.5727272727272728, 'recall': 0.38235294117647056, 'f1': 0.4585666293393057}\n",
      "Early stopping at epoch :  5\n",
      "F1 eval : [0.         0.30775194 0.03133903 0.97723285 0.00687623 0.\n",
      " 0.         0.         0.         0.04590164 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "F1 avg eval : 0.9394300496737596\n"
     ]
    }
   ],
   "source": [
    "model = Arg_Classifier(\"FR\",cfg).cuda()\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "_id = str(currentDateAndTime.year)+\"_\"+str(currentDateAndTime.month)+\"_\"+str(currentDateAndTime.day)+\"_\"+str(currentDateAndTime.hour)+\"_\"+str(currentDateAndTime.minute)+\"_\"+str(currentDateAndTime.second)\n",
    "_id =  _id +\"FR\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#optimizer = torch.optim.Adam(model.parameters(),lr = 0.000005)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "\n",
    "logSotfMax = torch.nn.LogSoftmax(dim=1)\n",
    "nll_loss = torch.nn.NLLLoss()\n",
    "\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "dataloader_dev = DataLoader(dev_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "mapping = dataloader_train.dataset.args_roles\n",
    "\n",
    "auto_model.eval()\n",
    "auto_model.cuda()\n",
    "\n",
    "EPOCHS = 200\n",
    "patience_counter = 0\n",
    "patience = 5\n",
    "max_val_loss = 9999\n",
    "f1_score_max = 0\n",
    "output_path = \"hw2/stud/saved\"\n",
    "model_name = \"model_\"+_id+\".pth\"\n",
    "PATH = os.path.join(output_path,model_name)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    #TRAINING\n",
    "    p = []\n",
    "    g = []\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_train):\n",
    "        #print(sample_batched)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "       \n",
    "        #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        #sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx_bis\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "        #------------------FILTERING SUB-WORDS----------------------\n",
    "        subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "        word_emebedding = []\n",
    "        for i in range(n):\n",
    "            subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "            flag = subtoken_mask[0,i,0]\n",
    "            if flag :\n",
    "                continue\n",
    "            else :\n",
    "                word_emebedding.append(subwords_embedding)\n",
    "        word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "        #-------------------------FORWARD/BACKWARD----------------------------------\n",
    "        x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "            perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "            predicate_index = sample_batched[\"predicate_index\"],\n",
    "            pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "            predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])        \n",
    "        b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "        loss = nll_loss(logSotfMax(x),gt)\n",
    "        total_loss = total_loss + loss\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "        counter += 1 \n",
    "            \n",
    "\n",
    "        #-------------------------RESULT STORING----------------------------------\n",
    "        predicted = torch.argmax(x, dim=1)\n",
    "        p += predicted.tolist()\n",
    "        g += gt.tolist()\n",
    "    \n",
    "\n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    scheduler.step()\n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    print(\"Epochs n.\", epoch)\n",
    "    print(\"F1 train:\",f1)\n",
    "    print(\"F1 avg train:\",f1_avg)\n",
    "    \n",
    "    avg_train_loss = total_loss/counter\n",
    "    writer.add_scalar(\"Loss_ES/train\", avg_train_loss, epoch)\n",
    "\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"Train_ES/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"Train_ES/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "    #EVALUATION\n",
    "    p = []\n",
    "    g = []\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_dev):\n",
    "    \n",
    "      #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda()\n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda()\n",
    "        #sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx\"].cuda()\n",
    "        sample_batched[\"predicate_meaning_idx\"] = sample_batched[\"predicate_meaning_idx_bis\"].cuda()\n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "            #------------------FILTERING SUB-WORDS----------------------\n",
    "            subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "            word_emebedding = []\n",
    "            for i in range(n): \n",
    "                subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "                flag = subtoken_mask[0,i,0]\n",
    "                if flag :\n",
    "                    continue\n",
    "                else :\n",
    "                    word_emebedding.append(subwords_embedding)\n",
    "            word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "            #-------------------------FORWARD----------------------------------\n",
    "            x = model.forward(subwords_embeddings = output_hidden_states_sum,\n",
    "                        perdicate_positional_encoding = sample_batched[\"positional_encoding\"],\n",
    "                        predicate_index = sample_batched[\"predicate_index\"],\n",
    "                        pos_index_encoding = sample_batched[\"pos_index\"],\n",
    "                        predicate_meaning_encoding = sample_batched[\"predicate_meaning_idx\"])   \n",
    "\n",
    "\n",
    "            b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "            loss = nll_loss(logSotfMax(x),gt)\n",
    "            total_loss = total_loss + loss\n",
    "            #-------------------------RESULT STORING----------------------------------\n",
    "            predicted = torch.argmax(x, dim=1)\n",
    "            p += predicted.tolist()\n",
    "            g += gt.tolist()\n",
    "            counter += 1 \n",
    "    \n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    avg_eval_loss = total_loss/counter\n",
    "\n",
    "    if avg_eval_loss < max_val_loss:\n",
    "        max_val_loss = avg_eval_loss\n",
    "    else :\n",
    "        patience_counter += 1\n",
    "    \n",
    "\n",
    "    f1 = f1_score(g, p, average=None)\n",
    "    f1_avg = f1_score(g, p, average=\"weighted\")\n",
    "\n",
    "    if patience_counter >= patience :  \n",
    "\n",
    "\n",
    "        print(\"Early stopping at epoch : \",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 avg eval :\",f1_avg)\n",
    "        break\n",
    "    else :\n",
    "        print(\"EPOCHS :\",epoch)\n",
    "        print(\"F1 eval :\",f1)\n",
    "        print(\"F1 avg eval :\",f1_avg)\n",
    "    \n",
    "\n",
    "    writer.add_scalar(\"Loss_ES/validation\", avg_eval_loss, epoch)\n",
    "\n",
    "    g,p = mapping_args(g,p,mapping)\n",
    "\n",
    "    identification_result,classification_result = metrics(g,p)\n",
    "    print(\"identification\",identification_result)\n",
    "    print(\"classification_result\",classification_result)\n",
    "\n",
    "    writer.add_scalar(\"Eval_ES/identification\", identification_result[\"f1\"], epoch)\n",
    "    writer.add_scalar(\"Eval_ES/classification\", classification_result[\"f1\"], epoch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if f1_avg > f1_score_max:\n",
    "        f1_score_max = f1_avg\n",
    "        #print(\"SAVED :\",PATH)\n",
    "        #torch.save(model.state_dict(),PATH)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('nlp2022-hw3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ea0fd027ae2f380e05c4e3293ede0f85a80d4d466dd9309da3a748502c958571"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
