{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT and TOKENIZER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model class is      : <class 'transformers.models.bert.modeling_bert.BertModel'>\n",
      "\n",
      "model class is      : <class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel,AutoTokenizer\n",
    "\n",
    "auto_model = AutoModel.from_pretrained(\"bert-base-cased\",output_hidden_states=True)\n",
    "\n",
    "print(f\"\\nmodel class is      : {type(auto_model)}\")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "print(f\"\\nmodel class is      : {type(tokenizer)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Role Labelling Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import random\n",
    "from typing import Dict\n",
    "\n",
    "class SRL(Dataset):\n",
    " \n",
    "    def __init__(self,language,path) -> None:\n",
    "\n",
    "        self.path_root = 'data'\n",
    "        self.load_data(language,path)\n",
    "        self.args_roles,self.list_broken_id = self.list_arg_roles()\n",
    "        self.pos_list,_ = self.list_pos()\n",
    "        self.device = device \n",
    "\n",
    "    def load_data(self,language,mode):\n",
    "        \n",
    "        mode = mode+\".json\"\n",
    "        path = os.path.join(self.path_root,language,mode)\n",
    "        data_file = open(path)\n",
    "       \n",
    "        data_ = json.load(data_file)\n",
    "\n",
    "        list_data = []\n",
    "\n",
    "        for data in data_:\n",
    "            list_data.append(data_[data])\n",
    "        \n",
    "\n",
    "        self.data = list_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, id : int):\n",
    "\n",
    "        flag = False\n",
    "        if id in self.list_broken_id :\n",
    "            flag = True\n",
    "            while flag == True:\n",
    "\n",
    "                rand_id = random.randint(0, len(self.data))\n",
    "                \n",
    "                if rand_id in self.list_broken_id :\n",
    "                    pass\n",
    "                else :\n",
    "                    flag = False\n",
    "                    id = rand_id        \n",
    "\n",
    "\n",
    "        data = self.pre_processing(self.data[id])\n",
    "        data = self.processig(data)\n",
    "        return data\n",
    "        \n",
    "    def pre_processing(self, data:dict):\n",
    "        data_list = []\n",
    "        for role in data[\"roles\"]:\n",
    "            dictionary = dict()\n",
    "            dictionary[\"words\"] = data[\"words\"]\n",
    "            dictionary[\"role\"] = data[\"roles\"][role]\n",
    "            dictionary[\"pre_idx\"] = role\n",
    "            \n",
    "            dictionary[\"pos_tags\"] = data[\"pos_tags\"]\n",
    "            data_list.append(dictionary)    \n",
    "        return data_list\n",
    "    \n",
    "    def processig(self,data_list:list):\n",
    "        \n",
    "        for dictionary in data_list:\n",
    "\n",
    "            #dictionary[\"words\"] = data[\"words\"]\n",
    "            dictionary[\"gt_arg_identification\"] = self.arg_id(dictionary[\"role\"])\n",
    "            dictionary[\"gt_arg_classification\"] = self.arg_class(dictionary[\"role\"])\n",
    "            dictionary[\"pos_idx\"] = self.pos_idx(dictionary[\"pos_tags\"])\n",
    "        \n",
    "        return data_list\n",
    "   \n",
    "    def list_arg_roles(self):\n",
    "        list_roles = []\n",
    "        list_broken_id = []\n",
    "        for i,element in enumerate(self.data):\n",
    "            flag = True\n",
    "            try : roles = element[\"roles\"]\n",
    "            except : flag = False\n",
    "            if flag :\n",
    "                for e in roles:\n",
    "                    sentence = element[\"roles\"][e]\n",
    "\n",
    "                    for word in sentence:\n",
    "                        \n",
    "                        list_roles.append(word)\n",
    "                list_roles = list(set(list_roles))\n",
    "            else : \n",
    "                list_broken_id.append(i)\n",
    "        return list_roles,list_broken_id\n",
    "\n",
    "    def list_pos(self):\n",
    "        list_pos = []\n",
    "        list_broken_id = []\n",
    "        for i,element in enumerate(self.data):\n",
    "            flag = True\n",
    "            try : pos = element[\"pos_tags\"]\n",
    "            except : flag = False\n",
    "            if flag :\n",
    "                for e in pos:\n",
    "                    list_pos.append(e)\n",
    "                list_pos = list(set(list_pos))\n",
    "            else : \n",
    "                list_broken_id.append(i)\n",
    "        return list_pos,list_broken_id\n",
    "  \n",
    "    def arg_class(self,role:list):\n",
    "        list_idxs = []\n",
    "        for element in role:\n",
    "            list_idxs.append(self.args_roles.index(element))\n",
    "        \n",
    "\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "\n",
    "    def arg_id(self,role:dict):\n",
    "        list_idxs = []\n",
    "        for element in role:\n",
    "            if element == \"_\":\n",
    "                list_idxs.append(0)\n",
    "            else :\n",
    "                list_idxs.append(1)\n",
    "\n",
    "        \n",
    "\n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "\n",
    "    def pos_idx(self,pos_tags:dict):\n",
    "        list_idxs = []\n",
    "        for element in pos_tags:\n",
    "            list_idxs.append(self.pos_list.index(element))\n",
    "    \n",
    "        return torch.tensor(list_idxs, dtype=torch.int64)\n",
    "    \n",
    "# here we define our collate function\n",
    "def collate_fn(batch) -> Dict[str, torch.Tensor]:\n",
    "    #print(batch)\n",
    "    input = dict() \n",
    "    batch_sentence = [] \n",
    "    #print(len(batch))\n",
    "    for period in batch:\n",
    "        for sentence in period :\n",
    "        \n",
    "            #print(len(sentence[0][\"words\"]))\n",
    "            pre_idx = int(sentence[\"pre_idx\"])\n",
    "            \n",
    "\n",
    "            predicate = sentence[\"words\"][pre_idx]\n",
    "\n",
    "            text = \" \".join(sentence[\"words\"])\n",
    "            tokens: list[str] = text.split()\n",
    "            predicate: list[str] = predicate.split()\n",
    "\n",
    "            #text = sentence[0][\"words\"]\n",
    "            \n",
    "            t = (tokens,predicate)\n",
    "\n",
    "            batch_sentence.append(t)\n",
    "            #print(batch_sentence)\n",
    "\n",
    "    batch_output = tokenizer.batch_encode_plus(batch_sentence,padding=True,is_split_into_words=True, truncation=True,return_offsets_mapping=True, return_tensors=\"pt\")\n",
    "    #print(batch_output.keys())\n",
    "\n",
    "\n",
    "    gt = dict()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    for period in batch:\n",
    "\n",
    "        list_positional_predicate_encoding = []\n",
    "        list_arg_gt = []\n",
    "        list_predicate_index = [] \n",
    "        list_pos_index = [] \n",
    "\n",
    "        for sentence in period:\n",
    "            #positional_encoding\n",
    "            #+2 per il CLS iniziale ad SEP finale\n",
    "            sentence_words_lenght =  len(sentence[\"words\"])\n",
    "            positional_predicate_encoding = torch.zeros(1,sentence_words_lenght+2)\n",
    "            #+1 per il CLS iniziale\n",
    "            pre_idx = int(sentence[\"pre_idx\"])\n",
    "            positional_predicate_encoding[:,pre_idx+1] = 1\n",
    "            list_positional_predicate_encoding.append(positional_predicate_encoding)\n",
    "            #print(\"positional_prefix_encoding\",positional_predicate_encoding)\n",
    "            list_predicate_index.append(pre_idx)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            pos = torch.unsqueeze(sentence[\"pos_idx\"],dim = 0)\n",
    "            list_pos_index.append(pos)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #note CLS and SEP are discharder after Bi-LSTM, the Classifier takes in input only wokrds hidden state embedding\n",
    "            arg_gt = torch.unsqueeze(sentence[\"gt_arg_classification\"],dim = 0)\n",
    "            list_arg_gt.append(arg_gt)\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    list_arg_gt = torch.cat(list_arg_gt,dim = 0)\n",
    "    list_pos_index = torch.cat(list_pos_index,dim = 0)\n",
    "    list_positional_predicate_encoding = torch.cat(list_positional_predicate_encoding,dim = 0)\n",
    "\n",
    "\n",
    "\n",
    "    gt[\"arg_gt\"] = list_arg_gt\n",
    "    input[\"predicate_index\"] = list_predicate_index\n",
    "    input[\"pos_index\"] = list_pos_index.long()\n",
    "\n",
    "\n",
    "    offset = batch_output.pop(\"offset_mapping\")\n",
    "\n",
    "\n",
    "    input[\"BERT_input\"] = batch_output\n",
    "    input[\"positional_encoding\"] = list_positional_predicate_encoding.long()\n",
    "    input[\"offset_mapping\"] = offset\n",
    "    input[\"gt\"] = gt\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    return input\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Argument Identification and Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs n. 0\n",
      "F1 train: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.46126188 0.         0.03106009 0.         0.         0.\n",
      " 0.         0.08500401 0.         0.         0.         0.97514247\n",
      " 0.         0.         0.02791625]\n",
      "F1 dev: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.59618209 0.         0.         0.         0.         0.\n",
      " 0.         0.26600496 0.         0.         0.         0.9814664\n",
      " 0.         0.         0.1106383 ]\n",
      "Epochs n. 1\n",
      "F1 train: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.07119021 0.         0.         0.06365372\n",
      " 0.65973435 0.         0.36798438 0.         0.         0.\n",
      " 0.         0.2863485  0.         0.01008403 0.         0.98086138\n",
      " 0.         0.         0.15438596]\n",
      "F1 dev: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.16931217 0.         0.         0.16494845\n",
      " 0.67598566 0.         0.37155963 0.         0.         0.\n",
      " 0.         0.32426989 0.         0.         0.         0.98312135\n",
      " 0.         0.         0.21875   ]\n",
      "Epochs n. 2\n",
      "F1 train: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.226451   0.         0.         0.15780446\n",
      " 0.71402629 0.         0.46840149 0.         0.         0.\n",
      " 0.         0.38191882 0.         0.03257329 0.         0.98304111\n",
      " 0.         0.         0.26405229]\n",
      "F1 dev: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.25365854 0.         0.         0.24038462\n",
      " 0.72271791 0.         0.46153846 0.         0.         0.\n",
      " 0.         0.41123275 0.         0.         0.         0.98491733\n",
      " 0.         0.         0.38638858]\n",
      "Epochs n. 3\n",
      "F1 train: [0.         0.         0.         0.         0.         0.\n",
      " 0.01413428 0.         0.35927152 0.         0.         0.23957219\n",
      " 0.74185464 0.         0.49909747 0.         0.         0.\n",
      " 0.         0.44487736 0.         0.07751938 0.         0.98451669\n",
      " 0.         0.         0.40703971]\n",
      "F1 dev: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.45783133 0.         0.         0.26535627\n",
      " 0.73811865 0.         0.57088847 0.         0.         0.\n",
      " 0.         0.47513321 0.         0.         0.         0.98608744\n",
      " 0.         0.         0.44957983]\n",
      "Epochs n. 4\n",
      "F1 train: [0.02006689 0.         0.         0.         0.         0.\n",
      " 0.07792208 0.         0.47040498 0.         0.         0.28914405\n",
      " 0.76499053 0.00985222 0.54997779 0.         0.         0.\n",
      " 0.         0.51245974 0.         0.12573099 0.         0.98584538\n",
      " 0.         0.         0.49514563]\n",
      "F1 dev: [0.         0.         0.         0.         0.         0.\n",
      " 0.04255319 0.         0.406639   0.         0.         0.33882353\n",
      " 0.75431332 0.         0.59550562 0.         0.         0.\n",
      " 0.         0.51868132 0.         0.         0.         0.98700938\n",
      " 0.         0.         0.47639035]\n",
      "Epochs n. 5\n",
      "F1 train: [0.06493506 0.         0.         0.         0.00851064 0.\n",
      " 0.14164306 0.         0.51731761 0.00766284 0.00533333 0.35218254\n",
      " 0.78100264 0.02870813 0.58419244 0.         0.00609756 0.\n",
      " 0.         0.54452422 0.00662252 0.17974322 0.         0.98672759\n",
      " 0.         0.         0.53850843]\n",
      "F1 dev: [0.07142857 0.         0.         0.         0.         0.\n",
      " 0.03846154 0.         0.57664234 0.         0.         0.3956044\n",
      " 0.76095361 0.         0.60628466 0.         0.         0.\n",
      " 0.         0.54680412 0.02898551 0.         0.         0.98770984\n",
      " 0.         0.         0.5528613 ]\n",
      "Epochs n. 6\n",
      "F1 train: [0.1337386  0.         0.         0.         0.07968127 0.\n",
      " 0.1091954  0.         0.53519062 0.03717472 0.08888889 0.39848557\n",
      " 0.79307729 0.06278027 0.61429759 0.         0.04216867 0.\n",
      " 0.         0.57753576 0.07430341 0.19519095 0.         0.98763568\n",
      " 0.         0.         0.58754407]\n",
      "F1 dev: [0.07142857 0.         0.         0.         0.0952381  0.\n",
      " 0.10714286 0.         0.54873646 0.         0.06349206 0.42173913\n",
      " 0.76490066 0.         0.66323024 0.         0.         0.\n",
      " 0.         0.55592935 0.05714286 0.         0.         0.98787739\n",
      " 0.         0.         0.57469077]\n",
      "Epochs n. 7\n",
      "F1 train: [0.19263456 0.         0.         0.         0.12686567 0.\n",
      " 0.19947507 0.         0.57991513 0.08275862 0.20935412 0.42227378\n",
      " 0.80561836 0.08888889 0.64150943 0.         0.22739018 0.\n",
      " 0.         0.60200291 0.13642961 0.23783784 0.         0.98819785\n",
      " 0.         0.         0.61497018]\n",
      "F1 dev: [0.13559322 0.         0.         0.         0.20833333 0.\n",
      " 0.11111111 0.         0.56603774 0.04347826 0.17910448 0.42436975\n",
      " 0.77505713 0.         0.66666667 0.         0.14925373 0.\n",
      " 0.         0.58489033 0.16993464 0.         0.         0.98850445\n",
      " 0.         0.         0.63366337]\n",
      "Epochs n. 8\n",
      "F1 train: [0.19498607 0.         0.03225806 0.         0.18367347 0.\n",
      " 0.18469657 0.         0.62219101 0.13636364 0.2993763  0.45818505\n",
      " 0.81351536 0.1300813  0.66529266 0.         0.29268293 0.\n",
      " 0.         0.62494058 0.20939597 0.26558266 0.         0.98883848\n",
      " 0.         0.         0.65028143]\n",
      "F1 dev: [0.19354839 0.         0.         0.         0.24489796 0.\n",
      " 0.1754386  0.         0.60583942 0.04       0.19444444 0.48459959\n",
      " 0.78363339 0.         0.6969697  0.         0.25       0.\n",
      " 0.06060606 0.59231722 0.24418605 0.20183486 0.         0.98884801\n",
      " 0.         0.         0.63950399]\n",
      "Epochs n. 9\n",
      "F1 train: [0.25063939 0.05555556 0.03076923 0.         0.275      0.06779661\n",
      " 0.27777778 0.         0.62978142 0.22089552 0.35616438 0.47324924\n",
      " 0.82180635 0.1322314  0.6725735  0.         0.35266821 0.\n",
      " 0.01351351 0.64689131 0.28244275 0.30395913 0.         0.98934796\n",
      " 0.         0.         0.67058824]\n",
      "F1 dev: [0.27272727 0.         0.         0.         0.20833333 0.\n",
      " 0.16666667 0.         0.58909091 0.2295082  0.28947368 0.47933884\n",
      " 0.7824119  0.05882353 0.67944251 0.         0.32098765 0.\n",
      " 0.         0.61003861 0.25       0.21818182 0.         0.98905521\n",
      " 0.         0.         0.64458371]\n",
      "Epochs n. 10\n",
      "F1 train: [0.34398034 0.         0.         0.         0.34402332 0.16129032\n",
      " 0.24578313 0.         0.66172507 0.26966292 0.44402985 0.51928021\n",
      " 0.83235433 0.21960784 0.70764381 0.         0.38752784 0.\n",
      " 0.11180124 0.6681691  0.25967541 0.34727503 0.         0.98997626\n",
      " 0.         0.         0.68772961]\n",
      "F1 dev: [0.21875    0.         0.         0.         0.25454545 0.5\n",
      " 0.2295082  0.         0.61428571 0.21538462 0.3373494  0.49287169\n",
      " 0.77788492 0.05714286 0.71917808 0.         0.27848101 0.\n",
      " 0.         0.61068111 0.29585799 0.2        0.         0.98905539\n",
      " 0.         0.         0.640955  ]\n",
      "Epochs n. 11\n",
      "F1 train: [0.39344262 0.         0.06060606 0.         0.42458101 0.25714286\n",
      " 0.2939759  0.         0.67927076 0.30851064 0.49023091 0.53972257\n",
      " 0.84136263 0.25179856 0.72340426 0.         0.4742268  0.\n",
      " 0.11180124 0.68262548 0.3144208  0.37858032 0.         0.9905019\n",
      " 0.         0.         0.71873293]\n",
      "F1 dev: [0.25       0.         0.         0.         0.26415094 0.5\n",
      " 0.16949153 0.         0.63799283 0.22222222 0.34090909 0.50406504\n",
      " 0.78787879 0.05263158 0.70784641 0.         0.3373494  0.\n",
      " 0.16666667 0.61338742 0.30601093 0.34710744 0.         0.98932513\n",
      " 0.         0.         0.65689655]\n",
      "Epochs n. 12\n",
      "F1 train: [0.37383178 0.1025641  0.09230769 0.         0.45125348 0.31325301\n",
      " 0.33632287 0.         0.6835781  0.30238727 0.52686308 0.55588112\n",
      " 0.8514347  0.28679245 0.73123298 0.03636364 0.54693878 0.\n",
      " 0.17777778 0.70590055 0.35157159 0.39756098 0.         0.99097225\n",
      " 0.         0.         0.72576079]\n",
      "F1 dev: [0.22857143 0.         0.         0.         0.24137931 0.5\n",
      " 0.1971831  0.         0.62837838 0.30769231 0.35294118 0.54158607\n",
      " 0.78721404 0.15384615 0.72402597 0.         0.3373494  0.\n",
      " 0.11111111 0.61933998 0.31958763 0.40322581 0.         0.98957077\n",
      " 0.         0.         0.65862069]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 111\u001b[0m\n\u001b[1;32m    108\u001b[0m gt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(sample_batched[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgt\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_gt\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m    110\u001b[0m loss \u001b[38;5;241m=\u001b[39m nll_loss(m(x),gt)\n\u001b[0;32m--> 111\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m#print(loss)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/_tensor.py?line=386'>387</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/_tensor.py?line=387'>388</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/_tensor.py?line=388'>389</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/_tensor.py?line=389'>390</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/_tensor.py?line=393'>394</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/_tensor.py?line=394'>395</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/_tensor.py?line=395'>396</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from hw2.stud.arg import Arg_Classifier \n",
    "from sklearn.metrics import f1_score\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "model = Arg_Classifier(\"EN\").cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "train_dataset = SRL(\"EN\",\"train\")\n",
    "dev_dataset = SRL(\"EN\",\"dev\")\n",
    "logSotfMax = torch.nn.LogSoftmax(dim=1)\n",
    "nll_loss = torch.nn.NLLLoss()\n",
    "\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "dataloader_dev = DataLoader(dev_dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate_fn,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "\n",
    "auto_model.eval()\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    #TRAINING\n",
    "    p = []\n",
    "    g = []\n",
    "    model.train()\n",
    "    for i_batch, sample_batched in enumerate(dataloader_train):\n",
    "        optimizer.zero_grad()\n",
    "       \n",
    "        #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda() \n",
    "        sample_batched[\"pos_index\"] = sample_batched[\"pos_index\"].cuda() \n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "        #------------------FILTERING SUB-WORDS----------------------\n",
    "        subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "        word_emebedding = []\n",
    "        for i in range(n): \n",
    "            subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "            flag = subtoken_mask[0,i,0]\n",
    "            if flag :\n",
    "                continue\n",
    "            else :\n",
    "                word_emebedding.append(subwords_embedding)\n",
    "        word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "        #-------------------------FORWARD/BACKWARD----------------------------------\n",
    "        x = model.forward(subwords_embeddings = output_hidden_states_sum,perdicate_positional_encoding = sample_batched[\"positional_encoding\"], predicate_index = sample_batched[\"predicate_index\"],pos_index_encoding = sample_batched[\"pos_index\"])        \n",
    "        b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "        loss = nll_loss(logSotfMax(x),gt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #-------------------------RESULT STORING----------------------------------\n",
    "        predicted = torch.argmax(x, dim=1)\n",
    "        p += predicted.tolist()\n",
    "        g += gt.tolist()\n",
    "    \n",
    "\n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    print(\"Epochs n.\", epoch)\n",
    "    print(\"F1 train:\",f1_score(g, p, average=None))\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "    #EVALUATION\n",
    "    p = []\n",
    "    g = []\n",
    "    model.eval()\n",
    "    for i_batch, sample_batched in enumerate(dataloader_dev):\n",
    "    \n",
    "      #----------------------PREPARE INPUT/OUTPUT-------------------------------\n",
    "        input_bert = sample_batched[\"BERT_input\"]\n",
    "        input_bert['input_ids'] = input_bert['input_ids'].cuda()\n",
    "        input_bert['token_type_ids'] = input_bert['token_type_ids'].cuda()\n",
    "        input_bert['attention_mask'] = input_bert['attention_mask'].cuda()\n",
    "        sample_batched[\"positional_encoding\"] = sample_batched[\"positional_encoding\"].cuda() \n",
    "        #prepare gt\n",
    "        gt = torch.flatten(sample_batched[\"gt\"][\"arg_gt\"]).cuda()\n",
    "        offset = sample_batched[\"offset_mapping\"]\n",
    "        #-----------------BERT EMBEDDING---------------------------\n",
    "        with torch.no_grad():\n",
    "            output = auto_model(**input_bert)\n",
    "            output_hidden_states_sum = torch.stack(output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "            b,n,h = output_hidden_states_sum.size()\n",
    "    \n",
    "        #------------------FILTERING SUB-WORDS----------------------\n",
    "        subtoken_mask = torch.unsqueeze(offset[:,:, 0] != 0,dim =-1)\n",
    "        word_emebedding = []\n",
    "        for i in range(n): \n",
    "            subwords_embedding = torch.unsqueeze(output_hidden_states_sum[:,i,:],dim = 1)\n",
    "            flag = subtoken_mask[0,i,0]\n",
    "            if flag :\n",
    "                continue\n",
    "            else :\n",
    "                word_emebedding.append(subwords_embedding)\n",
    "        word_emebedding = torch.cat(word_emebedding,dim = 1)\n",
    "        #-------------------------FORWARD----------------------------------\n",
    "        x = model.forward(subwords_embeddings = output_hidden_states_sum,perdicate_positional_encoding = sample_batched[\"positional_encoding\"], predicate_index = sample_batched[\"predicate_index\"])        \n",
    "        b,n = sample_batched[\"gt\"][\"arg_gt\"].size()\n",
    "        #-------------------------RESULT STORING----------------------------------\n",
    "        predicted = torch.argmax(x, dim=1)\n",
    "        p += predicted.tolist()\n",
    "        g += gt.tolist()\n",
    "    \n",
    "    #-------------------------RESULTS----------------------------------\n",
    "    print(\"F1 train:\",f1_score(g, p, average=None))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "# we will use with Distil-BERT\n",
    "language_model_name = \"distilbert-base-uncased\"\n",
    "# this GPU should be enough for this task to handle 32 samples per batch\n",
    "batch_size = 32\n",
    "# we keep num_workers = min(4 * number of GPUs, number of cores)\n",
    "# tells the data loader how many sub-processes to use for data loading\n",
    "num_workers = 2\n",
    "# optim\n",
    "learning_rate = 2e-4\n",
    "weight_decay = 0.0\n",
    "transformer_learning_rate = 1e-5\n",
    "transformer_weight_decay = 0.0\n",
    "# training\n",
    "epochs = 3\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# load our dataset\n",
    "ner_dataset = load_dataset(\"conll2003\")\n",
    "\n",
    "# let's instantiate the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(language_model_name)\n",
    "\n",
    "# here we define a vocab dict to map the labels to int (and vice versa)\n",
    "label_list = ner_dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "label_to_id = {n: i for i, n in enumerate(ner_dataset[\"train\"].features[\"ner_tags\"].feature.names)}\n",
    "id_to_label = {i: n for n, i in label_to_id.items()}\n",
    "\n",
    "# here we define our collate function\n",
    "def collate_fn(batch) -> Dict[str, torch.Tensor]:\n",
    "    batch_out = tokenizer(\n",
    "        [sentence[\"tokens\"] for sentence in batch],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        # We use this argument because the texts in our dataset are lists of words.\n",
    "        is_split_into_words=True,\n",
    "    )\n",
    "    labels = []\n",
    "    ner_tags = [sentence[\"ner_tags\"] for sentence in batch]\n",
    "    for i, label in enumerate(ner_tags):\n",
    "      # obtains the word_ids of the i-th sentence\n",
    "      word_ids = batch_out.word_ids(batch_index=i)\n",
    "      previous_word_idx = None\n",
    "      label_ids = []\n",
    "      for word_idx in word_ids:\n",
    "        # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "        # ignored in the loss function.\n",
    "        if word_idx is None:\n",
    "          label_ids.append(-100)\n",
    "        # We set the label for the first token of each word.\n",
    "        elif word_idx != previous_word_idx:\n",
    "          label_ids.append(label[word_idx])\n",
    "        # For the other tokens in a word, we set the label to -100 so they are automatically\n",
    "        # ignored in the loss function.\n",
    "        else:\n",
    "          label_ids.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "      labels.append(label_ids)\n",
    "    \n",
    "    # pad the labels with -100\n",
    "    batch_max_length = len(max(labels, key=len))\n",
    "    labels = [l + ([-100] * abs(batch_max_length - len(l))) for l in labels]\n",
    "    batch_out[\"labels\"] = torch.as_tensor(labels)\n",
    "    return batch_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('nlp2022-hw2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "396663868a68a40d4aad1f83db3cf05f3bd24b2a6e9304aa6d34e745f3941ef3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
